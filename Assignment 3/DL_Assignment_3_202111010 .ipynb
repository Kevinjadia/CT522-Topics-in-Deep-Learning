{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSycngCiCIji",
        "outputId": "94cb40ae-541c-4dd1-850f-6c8719d866c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVFeQBx-CspM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5408ad3a-87f6-44ab-80ed-719c036439ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘Data’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir Data\n",
        "!cp \"/content/drive/MyDrive/DL_LAB/ASG3/heart.csv\" \"./Data\"\n",
        "!cp \"/content/drive/MyDrive/DL_LAB/ASG3/Mobile_price.csv\" \"./Data\"\n",
        "!cp \"/content/drive/MyDrive/DL_LAB/ASG1/50_Startups.csv\" \"./Data\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piwmYTSBMl1M"
      },
      "source": [
        "Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qs6sOqOtDFpb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import math\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import OneHotEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "heart_df=pd.read_csv('./Data/heart.csv')\n",
        "mobile_df=pd.read_csv('./Data/Mobile_price.csv')\n",
        "startups_df=pd.read_csv('./Data/50_Startups.csv')"
      ],
      "metadata": {
        "id": "3A6XPlHFwjhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UAws8PX3Edwn",
        "outputId": "9cec8c69-7783-4668-abf3-b49a73b9bf8e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    R&D Spend  Administration  Marketing Spend       State     Profit\n",
              "0   165349.20       136897.80        471784.10    New York  192261.83\n",
              "1   162597.70       151377.59        443898.53  California  191792.06\n",
              "2   153441.51       101145.55        407934.54     Florida  191050.39\n",
              "3   144372.41       118671.85        383199.62    New York  182901.99\n",
              "4   142107.34        91391.77        366168.42     Florida  166187.94\n",
              "5   131876.90        99814.71        362861.36    New York  156991.12\n",
              "6   134615.46       147198.87        127716.82  California  156122.51\n",
              "7   130298.13       145530.06        323876.68     Florida  155752.60\n",
              "8   120542.52       148718.95        311613.29    New York  152211.77\n",
              "9   123334.88       108679.17        304981.62  California  149759.96\n",
              "10  101913.08       110594.11        229160.95     Florida  146121.95\n",
              "11  100671.96        91790.61        249744.55  California  144259.40\n",
              "12   93863.75       127320.38        249839.44     Florida  141585.52\n",
              "13   91992.39       135495.07        252664.93  California  134307.35\n",
              "14  119943.24       156547.42        256512.92     Florida  132602.65\n",
              "15  114523.61       122616.84        261776.23    New York  129917.04\n",
              "16   78013.11       121597.55        264346.06  California  126992.93\n",
              "17   94657.16       145077.58        282574.31    New York  125370.37\n",
              "18   91749.16       114175.79        294919.57     Florida  124266.90\n",
              "19   86419.70       153514.11             0.00    New York  122776.86\n",
              "20   76253.86       113867.30        298664.47  California  118474.03\n",
              "21   78389.47       153773.43        299737.29    New York  111313.02\n",
              "22   73994.56       122782.75        303319.26     Florida  110352.25\n",
              "23   67532.53       105751.03        304768.73     Florida  108733.99\n",
              "24   77044.01        99281.34        140574.81    New York  108552.04\n",
              "25   64664.71       139553.16        137962.62  California  107404.34\n",
              "26   75328.87       144135.98        134050.07     Florida  105733.54\n",
              "27   72107.60       127864.55        353183.81    New York  105008.31\n",
              "28   66051.52       182645.56        118148.20     Florida  103282.38\n",
              "29   65605.48       153032.06        107138.38    New York  101004.64\n",
              "30   61994.48       115641.28         91131.24     Florida   99937.59\n",
              "31   61136.38       152701.92         88218.23    New York   97483.56\n",
              "32   63408.86       129219.61         46085.25  California   97427.84\n",
              "33   55493.95       103057.49        214634.81     Florida   96778.92\n",
              "34   46426.07       157693.92        210797.67  California   96712.80\n",
              "35   46014.02        85047.44        205517.64    New York   96479.51\n",
              "36   28663.76       127056.21        201126.82     Florida   90708.19\n",
              "37   44069.95        51283.14        197029.42  California   89949.14\n",
              "38   20229.59        65947.93        185265.10    New York   81229.06\n",
              "39   38558.51        82982.09        174999.30  California   81005.76\n",
              "40   28754.33       118546.05        172795.67  California   78239.91\n",
              "41   27892.92        84710.77        164470.71     Florida   77798.83\n",
              "42   23640.93        96189.63        148001.11  California   71498.49\n",
              "43   15505.73       127382.30         35534.17    New York   69758.98\n",
              "44   22177.74       154806.14         28334.72  California   65200.33\n",
              "45    1000.23       124153.04          1903.93    New York   64926.08\n",
              "46    1315.46       115816.21        297114.46     Florida   49490.75\n",
              "47       0.00       135426.92             0.00  California   42559.73\n",
              "48     542.05        51743.15             0.00    New York   35673.41\n",
              "49       0.00       116983.80         45173.06  California   14681.40"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8e5cb9a8-19fa-4844-a2e9-4dac05ebc764\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>R&amp;D Spend</th>\n",
              "      <th>Administration</th>\n",
              "      <th>Marketing Spend</th>\n",
              "      <th>State</th>\n",
              "      <th>Profit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>165349.20</td>\n",
              "      <td>136897.80</td>\n",
              "      <td>471784.10</td>\n",
              "      <td>New York</td>\n",
              "      <td>192261.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>162597.70</td>\n",
              "      <td>151377.59</td>\n",
              "      <td>443898.53</td>\n",
              "      <td>California</td>\n",
              "      <td>191792.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>153441.51</td>\n",
              "      <td>101145.55</td>\n",
              "      <td>407934.54</td>\n",
              "      <td>Florida</td>\n",
              "      <td>191050.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>144372.41</td>\n",
              "      <td>118671.85</td>\n",
              "      <td>383199.62</td>\n",
              "      <td>New York</td>\n",
              "      <td>182901.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>142107.34</td>\n",
              "      <td>91391.77</td>\n",
              "      <td>366168.42</td>\n",
              "      <td>Florida</td>\n",
              "      <td>166187.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>131876.90</td>\n",
              "      <td>99814.71</td>\n",
              "      <td>362861.36</td>\n",
              "      <td>New York</td>\n",
              "      <td>156991.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>134615.46</td>\n",
              "      <td>147198.87</td>\n",
              "      <td>127716.82</td>\n",
              "      <td>California</td>\n",
              "      <td>156122.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>130298.13</td>\n",
              "      <td>145530.06</td>\n",
              "      <td>323876.68</td>\n",
              "      <td>Florida</td>\n",
              "      <td>155752.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>120542.52</td>\n",
              "      <td>148718.95</td>\n",
              "      <td>311613.29</td>\n",
              "      <td>New York</td>\n",
              "      <td>152211.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>123334.88</td>\n",
              "      <td>108679.17</td>\n",
              "      <td>304981.62</td>\n",
              "      <td>California</td>\n",
              "      <td>149759.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>101913.08</td>\n",
              "      <td>110594.11</td>\n",
              "      <td>229160.95</td>\n",
              "      <td>Florida</td>\n",
              "      <td>146121.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>100671.96</td>\n",
              "      <td>91790.61</td>\n",
              "      <td>249744.55</td>\n",
              "      <td>California</td>\n",
              "      <td>144259.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>93863.75</td>\n",
              "      <td>127320.38</td>\n",
              "      <td>249839.44</td>\n",
              "      <td>Florida</td>\n",
              "      <td>141585.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>91992.39</td>\n",
              "      <td>135495.07</td>\n",
              "      <td>252664.93</td>\n",
              "      <td>California</td>\n",
              "      <td>134307.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>119943.24</td>\n",
              "      <td>156547.42</td>\n",
              "      <td>256512.92</td>\n",
              "      <td>Florida</td>\n",
              "      <td>132602.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>114523.61</td>\n",
              "      <td>122616.84</td>\n",
              "      <td>261776.23</td>\n",
              "      <td>New York</td>\n",
              "      <td>129917.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>78013.11</td>\n",
              "      <td>121597.55</td>\n",
              "      <td>264346.06</td>\n",
              "      <td>California</td>\n",
              "      <td>126992.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>94657.16</td>\n",
              "      <td>145077.58</td>\n",
              "      <td>282574.31</td>\n",
              "      <td>New York</td>\n",
              "      <td>125370.37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>91749.16</td>\n",
              "      <td>114175.79</td>\n",
              "      <td>294919.57</td>\n",
              "      <td>Florida</td>\n",
              "      <td>124266.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>86419.70</td>\n",
              "      <td>153514.11</td>\n",
              "      <td>0.00</td>\n",
              "      <td>New York</td>\n",
              "      <td>122776.86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>76253.86</td>\n",
              "      <td>113867.30</td>\n",
              "      <td>298664.47</td>\n",
              "      <td>California</td>\n",
              "      <td>118474.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>78389.47</td>\n",
              "      <td>153773.43</td>\n",
              "      <td>299737.29</td>\n",
              "      <td>New York</td>\n",
              "      <td>111313.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>73994.56</td>\n",
              "      <td>122782.75</td>\n",
              "      <td>303319.26</td>\n",
              "      <td>Florida</td>\n",
              "      <td>110352.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>67532.53</td>\n",
              "      <td>105751.03</td>\n",
              "      <td>304768.73</td>\n",
              "      <td>Florida</td>\n",
              "      <td>108733.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>77044.01</td>\n",
              "      <td>99281.34</td>\n",
              "      <td>140574.81</td>\n",
              "      <td>New York</td>\n",
              "      <td>108552.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>64664.71</td>\n",
              "      <td>139553.16</td>\n",
              "      <td>137962.62</td>\n",
              "      <td>California</td>\n",
              "      <td>107404.34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>75328.87</td>\n",
              "      <td>144135.98</td>\n",
              "      <td>134050.07</td>\n",
              "      <td>Florida</td>\n",
              "      <td>105733.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>72107.60</td>\n",
              "      <td>127864.55</td>\n",
              "      <td>353183.81</td>\n",
              "      <td>New York</td>\n",
              "      <td>105008.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>66051.52</td>\n",
              "      <td>182645.56</td>\n",
              "      <td>118148.20</td>\n",
              "      <td>Florida</td>\n",
              "      <td>103282.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>65605.48</td>\n",
              "      <td>153032.06</td>\n",
              "      <td>107138.38</td>\n",
              "      <td>New York</td>\n",
              "      <td>101004.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>61994.48</td>\n",
              "      <td>115641.28</td>\n",
              "      <td>91131.24</td>\n",
              "      <td>Florida</td>\n",
              "      <td>99937.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>61136.38</td>\n",
              "      <td>152701.92</td>\n",
              "      <td>88218.23</td>\n",
              "      <td>New York</td>\n",
              "      <td>97483.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>63408.86</td>\n",
              "      <td>129219.61</td>\n",
              "      <td>46085.25</td>\n",
              "      <td>California</td>\n",
              "      <td>97427.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>55493.95</td>\n",
              "      <td>103057.49</td>\n",
              "      <td>214634.81</td>\n",
              "      <td>Florida</td>\n",
              "      <td>96778.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>46426.07</td>\n",
              "      <td>157693.92</td>\n",
              "      <td>210797.67</td>\n",
              "      <td>California</td>\n",
              "      <td>96712.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>46014.02</td>\n",
              "      <td>85047.44</td>\n",
              "      <td>205517.64</td>\n",
              "      <td>New York</td>\n",
              "      <td>96479.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>28663.76</td>\n",
              "      <td>127056.21</td>\n",
              "      <td>201126.82</td>\n",
              "      <td>Florida</td>\n",
              "      <td>90708.19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>44069.95</td>\n",
              "      <td>51283.14</td>\n",
              "      <td>197029.42</td>\n",
              "      <td>California</td>\n",
              "      <td>89949.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>20229.59</td>\n",
              "      <td>65947.93</td>\n",
              "      <td>185265.10</td>\n",
              "      <td>New York</td>\n",
              "      <td>81229.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>38558.51</td>\n",
              "      <td>82982.09</td>\n",
              "      <td>174999.30</td>\n",
              "      <td>California</td>\n",
              "      <td>81005.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>28754.33</td>\n",
              "      <td>118546.05</td>\n",
              "      <td>172795.67</td>\n",
              "      <td>California</td>\n",
              "      <td>78239.91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>27892.92</td>\n",
              "      <td>84710.77</td>\n",
              "      <td>164470.71</td>\n",
              "      <td>Florida</td>\n",
              "      <td>77798.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>23640.93</td>\n",
              "      <td>96189.63</td>\n",
              "      <td>148001.11</td>\n",
              "      <td>California</td>\n",
              "      <td>71498.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>15505.73</td>\n",
              "      <td>127382.30</td>\n",
              "      <td>35534.17</td>\n",
              "      <td>New York</td>\n",
              "      <td>69758.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>22177.74</td>\n",
              "      <td>154806.14</td>\n",
              "      <td>28334.72</td>\n",
              "      <td>California</td>\n",
              "      <td>65200.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>1000.23</td>\n",
              "      <td>124153.04</td>\n",
              "      <td>1903.93</td>\n",
              "      <td>New York</td>\n",
              "      <td>64926.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>1315.46</td>\n",
              "      <td>115816.21</td>\n",
              "      <td>297114.46</td>\n",
              "      <td>Florida</td>\n",
              "      <td>49490.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.00</td>\n",
              "      <td>135426.92</td>\n",
              "      <td>0.00</td>\n",
              "      <td>California</td>\n",
              "      <td>42559.73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>542.05</td>\n",
              "      <td>51743.15</td>\n",
              "      <td>0.00</td>\n",
              "      <td>New York</td>\n",
              "      <td>35673.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.00</td>\n",
              "      <td>116983.80</td>\n",
              "      <td>45173.06</td>\n",
              "      <td>California</td>\n",
              "      <td>14681.40</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e5cb9a8-19fa-4844-a2e9-4dac05ebc764')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8e5cb9a8-19fa-4844-a2e9-4dac05ebc764 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8e5cb9a8-19fa-4844-a2e9-4dac05ebc764');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "startups_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAmH9c7zD7Eu"
      },
      "source": [
        "**1. Implement ANN model for classification where you classify between chance of heart attack or not.\n",
        "You will use first 13 attributes to as input the network and later try to implement the model with several layers and output layer will comprise of 2 neurons so model reduce from 13 to 2 with few hidden layers.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "6BhUWI9qDJIB",
        "outputId": "9841d422-ba25-41ea-d184-d20934749ee0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
              "0     63    1   3       145   233    1        0      150      0      2.3   \n",
              "1     37    1   2       130   250    0        1      187      0      3.5   \n",
              "2     41    0   1       130   204    0        0      172      0      1.4   \n",
              "3     56    1   1       120   236    0        1      178      0      0.8   \n",
              "4     57    0   0       120   354    0        1      163      1      0.6   \n",
              "..   ...  ...  ..       ...   ...  ...      ...      ...    ...      ...   \n",
              "298   57    0   0       140   241    0        1      123      1      0.2   \n",
              "299   45    1   3       110   264    0        1      132      0      1.2   \n",
              "300   68    1   0       144   193    1        1      141      0      3.4   \n",
              "301   57    1   0       130   131    0        1      115      1      1.2   \n",
              "302   57    0   1       130   236    0        0      174      0      0.0   \n",
              "\n",
              "     slope  ca  thal  target  \n",
              "0        0   0     1       1  \n",
              "1        0   0     2       1  \n",
              "2        2   0     2       1  \n",
              "3        2   0     2       1  \n",
              "4        2   0     2       1  \n",
              "..     ...  ..   ...     ...  \n",
              "298      1   0     3       0  \n",
              "299      1   0     3       0  \n",
              "300      1   2     3       0  \n",
              "301      1   1     3       0  \n",
              "302      1   1     2       0  \n",
              "\n",
              "[303 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9c376de0-afba-456c-a180-b3d1a00d1a62\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>63</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>145</td>\n",
              "      <td>233</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130</td>\n",
              "      <td>250</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>187</td>\n",
              "      <td>0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>130</td>\n",
              "      <td>204</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>236</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>354</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>163</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>140</td>\n",
              "      <td>241</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>123</td>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>45</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>110</td>\n",
              "      <td>264</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>132</td>\n",
              "      <td>0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>68</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>144</td>\n",
              "      <td>193</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>141</td>\n",
              "      <td>0</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>301</th>\n",
              "      <td>57</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>130</td>\n",
              "      <td>131</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>115</td>\n",
              "      <td>1</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>302</th>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>130</td>\n",
              "      <td>236</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>174</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>303 rows × 14 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9c376de0-afba-456c-a180-b3d1a00d1a62')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9c376de0-afba-456c-a180-b3d1a00d1a62 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9c376de0-afba-456c-a180-b3d1a00d1a62');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "heart_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XfSCB94Yl_h"
      },
      "outputs": [],
      "source": [
        "# dividing data into train set and test set\n",
        "train_data1, test_data1 = train_test_split(heart_df, test_size=0.2, random_state=25)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7rAFZISWEzP"
      },
      "outputs": [],
      "source": [
        "# Seperating Features and Target values\n",
        "x_train1, y_train1 = train_data1.drop('target', axis=1), train_data1['target']\n",
        "x_test1, y_test1 = test_data1.drop('target', axis=1), test_data1['target']\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#one hot encoding on target values based on no of class\n",
        "ohe = OneHotEncoder()\n",
        "\n",
        "y_train1=np.array(y_train1).reshape(-1,1)\n",
        "y_train1 = ohe.fit_transform(y_train1).toarray()\n",
        "\n",
        "y_test1=np.array(y_test1).reshape(-1,1)\n",
        "y_test1 = ohe.fit_transform(y_test1).toarray()"
      ],
      "metadata": {
        "id": "FP9C5AbglmcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVzcVbElFDHH",
        "outputId": "6eeee7db-413e-41e4-afe3-c8a1c14744ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLz2iZi0WDR9"
      },
      "outputs": [],
      "source": [
        "#function to normalize the data\n",
        "\n",
        "def scale_datasets(x_train, x_test):\n",
        "  standard_scaler = StandardScaler()\n",
        "  x_train_scaled = pd.DataFrame(standard_scaler.fit_transform(x_train),columns=x_train.columns\n",
        "  )\n",
        "  x_test_scaled = pd.DataFrame(\n",
        "      standard_scaler.transform(x_test),\n",
        "      columns = x_test.columns\n",
        "  )\n",
        "  return x_train_scaled, x_test_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NA-zn4HLWXsT"
      },
      "outputs": [],
      "source": [
        "# normalizing the Features of trainset and testset\n",
        "x_train_scaled_all1, x_test_scaled_all1 = scale_datasets(x_train1, x_test1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JEap1qHZV3H",
        "outputId": "39fbd35d-ca4b-4b70-b183-c3a1403f4e8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(193, 13)\n",
            "(193, 2)\n",
            "(49, 13)\n"
          ]
        }
      ],
      "source": [
        "# Dividing scalled train data into train and validation data\n",
        "\n",
        "X_train_scaled1, X_val_scaled1, Y_train1, Y_val1 = train_test_split(x_train_scaled_all1, y_train1, test_size=0.20, random_state=40)\n",
        "print(X_train_scaled1.shape)\n",
        "print(Y_train1.shape)\n",
        "print(X_val_scaled1.shape)\n",
        "#validation data is not test data...\n",
        "#validation data is to check whether during training whether our training is happening successfully or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zfz_TtrFZZSw"
      },
      "outputs": [],
      "source": [
        "# Defining model for classification\n",
        "model1 = Sequential()\n",
        "model1.add(Dense(activation = \"relu\", input_dim = 13, \n",
        "                     units = 10, kernel_initializer = \"uniform\"))\n",
        "model1.add(Dense(activation = \"relu\", units = 9, \n",
        "                     kernel_initializer = \"uniform\"))\n",
        "model1.add(Dense(activation = \"sigmoid\", units = 2, \n",
        "                     kernel_initializer = \"uniform\"))\n",
        "model1.compile(optimizer = 'adam' , loss = 'binary_crossentropy', \n",
        "                   metrics = ['accuracy'] )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJS4qt7hAMPY",
        "outputId": "9fc5122c-de32-46b2-e168-fd03aa06204d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_6 (Dense)             (None, 10)                140       \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 9)                 99        \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 2)                 20        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 259\n",
            "Trainable params: 259\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-NDwtw-ZkEh",
        "outputId": "60c80dc4-9be6-4dc1-cdfd-83e0c1e67328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.5596\n",
            "Epoch 2/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.6910 - accuracy: 0.6425\n",
            "Epoch 3/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.6829 - accuracy: 0.7824\n",
            "Epoch 4/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.6589 - accuracy: 0.8290\n",
            "Epoch 5/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.6126 - accuracy: 0.8497\n",
            "Epoch 6/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.5454 - accuracy: 0.8601\n",
            "Epoch 7/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.4712 - accuracy: 0.8705\n",
            "Epoch 8/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.4102 - accuracy: 0.8860\n",
            "Epoch 9/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.3677 - accuracy: 0.8808\n",
            "Epoch 10/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.3421 - accuracy: 0.8808\n",
            "Epoch 11/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.3273 - accuracy: 0.8860\n",
            "Epoch 12/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.3158 - accuracy: 0.8860\n",
            "Epoch 13/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.3107 - accuracy: 0.8860\n",
            "Epoch 14/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.3052 - accuracy: 0.8860\n",
            "Epoch 15/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.3009 - accuracy: 0.8860\n",
            "Epoch 16/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2980 - accuracy: 0.8964\n",
            "Epoch 17/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2960 - accuracy: 0.9016\n",
            "Epoch 18/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2929 - accuracy: 0.9067\n",
            "Epoch 19/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2907 - accuracy: 0.9067\n",
            "Epoch 20/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2893 - accuracy: 0.9067\n",
            "Epoch 21/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2867 - accuracy: 0.9067\n",
            "Epoch 22/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2865 - accuracy: 0.9119\n",
            "Epoch 23/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2841 - accuracy: 0.9119\n",
            "Epoch 24/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2825 - accuracy: 0.9119\n",
            "Epoch 25/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2808 - accuracy: 0.9119\n",
            "Epoch 26/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2797 - accuracy: 0.9119\n",
            "Epoch 27/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2788 - accuracy: 0.9119\n",
            "Epoch 28/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2762 - accuracy: 0.9171\n",
            "Epoch 29/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2761 - accuracy: 0.9171\n",
            "Epoch 30/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2743 - accuracy: 0.9171\n",
            "Epoch 31/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2733 - accuracy: 0.9171\n",
            "Epoch 32/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2718 - accuracy: 0.9223\n",
            "Epoch 33/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2708 - accuracy: 0.9223\n",
            "Epoch 34/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2692 - accuracy: 0.9223\n",
            "Epoch 35/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2676 - accuracy: 0.9223\n",
            "Epoch 36/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2689 - accuracy: 0.9223\n",
            "Epoch 37/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2668 - accuracy: 0.9275\n",
            "Epoch 38/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2661 - accuracy: 0.9275\n",
            "Epoch 39/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2649 - accuracy: 0.9275\n",
            "Epoch 40/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2645 - accuracy: 0.9275\n",
            "Epoch 41/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2627 - accuracy: 0.9275\n",
            "Epoch 42/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2628 - accuracy: 0.9275\n",
            "Epoch 43/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2617 - accuracy: 0.9275\n",
            "Epoch 44/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2609 - accuracy: 0.9275\n",
            "Epoch 45/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2636 - accuracy: 0.9275\n",
            "Epoch 46/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2604 - accuracy: 0.9275\n",
            "Epoch 47/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2583 - accuracy: 0.9275\n",
            "Epoch 48/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2580 - accuracy: 0.9275\n",
            "Epoch 49/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2564 - accuracy: 0.9275\n",
            "Epoch 50/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2563 - accuracy: 0.9275\n",
            "Epoch 51/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2549 - accuracy: 0.9275\n",
            "Epoch 52/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2545 - accuracy: 0.9275\n",
            "Epoch 53/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2541 - accuracy: 0.9275\n",
            "Epoch 54/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2537 - accuracy: 0.9275\n",
            "Epoch 55/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2522 - accuracy: 0.9275\n",
            "Epoch 56/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2518 - accuracy: 0.9275\n",
            "Epoch 57/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2502 - accuracy: 0.9275\n",
            "Epoch 58/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2491 - accuracy: 0.9275\n",
            "Epoch 59/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2487 - accuracy: 0.9275\n",
            "Epoch 60/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2488 - accuracy: 0.9275\n",
            "Epoch 61/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2479 - accuracy: 0.9275\n",
            "Epoch 62/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2472 - accuracy: 0.9275\n",
            "Epoch 63/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2454 - accuracy: 0.9275\n",
            "Epoch 64/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2448 - accuracy: 0.9275\n",
            "Epoch 65/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2446 - accuracy: 0.9275\n",
            "Epoch 66/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2437 - accuracy: 0.9275\n",
            "Epoch 67/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2424 - accuracy: 0.9275\n",
            "Epoch 68/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2411 - accuracy: 0.9275\n",
            "Epoch 69/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2401 - accuracy: 0.9275\n",
            "Epoch 70/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2407 - accuracy: 0.9275\n",
            "Epoch 71/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2390 - accuracy: 0.9275\n",
            "Epoch 72/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2385 - accuracy: 0.9275\n",
            "Epoch 73/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2375 - accuracy: 0.9275\n",
            "Epoch 74/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2362 - accuracy: 0.9275\n",
            "Epoch 75/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2352 - accuracy: 0.9275\n",
            "Epoch 76/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2340 - accuracy: 0.9275\n",
            "Epoch 77/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2328 - accuracy: 0.9275\n",
            "Epoch 78/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2318 - accuracy: 0.9275\n",
            "Epoch 79/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2312 - accuracy: 0.9275\n",
            "Epoch 80/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2305 - accuracy: 0.9275\n",
            "Epoch 81/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2285 - accuracy: 0.9275\n",
            "Epoch 82/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2296 - accuracy: 0.9275\n",
            "Epoch 83/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2287 - accuracy: 0.9275\n",
            "Epoch 84/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2266 - accuracy: 0.9275\n",
            "Epoch 85/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2280 - accuracy: 0.9326\n",
            "Epoch 86/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2256 - accuracy: 0.9378\n",
            "Epoch 87/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2238 - accuracy: 0.9378\n",
            "Epoch 88/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2229 - accuracy: 0.9326\n",
            "Epoch 89/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2230 - accuracy: 0.9326\n",
            "Epoch 90/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2206 - accuracy: 0.9326\n",
            "Epoch 91/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2204 - accuracy: 0.9326\n",
            "Epoch 92/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2188 - accuracy: 0.9326\n",
            "Epoch 93/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.2182 - accuracy: 0.9326\n",
            "Epoch 94/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2166 - accuracy: 0.9326\n",
            "Epoch 95/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.2153 - accuracy: 0.9378\n",
            "Epoch 96/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2138 - accuracy: 0.9326\n",
            "Epoch 97/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2141 - accuracy: 0.9326\n",
            "Epoch 98/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2172 - accuracy: 0.9378\n",
            "Epoch 99/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2154 - accuracy: 0.9430\n",
            "Epoch 100/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2107 - accuracy: 0.9430\n",
            "Epoch 101/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2098 - accuracy: 0.9430\n",
            "Epoch 102/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2080 - accuracy: 0.9430\n",
            "Epoch 103/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.2070 - accuracy: 0.9430\n",
            "Epoch 104/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2058 - accuracy: 0.9430\n",
            "Epoch 105/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2052 - accuracy: 0.9378\n",
            "Epoch 106/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.2020 - accuracy: 0.9430\n",
            "Epoch 107/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.2043 - accuracy: 0.9482\n",
            "Epoch 108/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.2033 - accuracy: 0.9482\n",
            "Epoch 109/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2013 - accuracy: 0.9430\n",
            "Epoch 110/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2008 - accuracy: 0.9430\n",
            "Epoch 111/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1991 - accuracy: 0.9430\n",
            "Epoch 112/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1981 - accuracy: 0.9430\n",
            "Epoch 113/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1968 - accuracy: 0.9430\n",
            "Epoch 114/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1953 - accuracy: 0.9430\n",
            "Epoch 115/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1946 - accuracy: 0.9430\n",
            "Epoch 116/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1937 - accuracy: 0.9482\n",
            "Epoch 117/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1946 - accuracy: 0.9482\n",
            "Epoch 118/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.1920 - accuracy: 0.9482\n",
            "Epoch 119/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1908 - accuracy: 0.9482\n",
            "Epoch 120/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1907 - accuracy: 0.9482\n",
            "Epoch 121/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1890 - accuracy: 0.9482\n",
            "Epoch 122/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.1878 - accuracy: 0.9482\n",
            "Epoch 123/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1866 - accuracy: 0.9482\n",
            "Epoch 124/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1856 - accuracy: 0.9482\n",
            "Epoch 125/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1844 - accuracy: 0.9482\n",
            "Epoch 126/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1838 - accuracy: 0.9482\n",
            "Epoch 127/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1832 - accuracy: 0.9482\n",
            "Epoch 128/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1820 - accuracy: 0.9482\n",
            "Epoch 129/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1811 - accuracy: 0.9482\n",
            "Epoch 130/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1803 - accuracy: 0.9482\n",
            "Epoch 131/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.1798 - accuracy: 0.9482\n",
            "Epoch 132/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1788 - accuracy: 0.9482\n",
            "Epoch 133/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1780 - accuracy: 0.9482\n",
            "Epoch 134/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1777 - accuracy: 0.9482\n",
            "Epoch 135/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.1765 - accuracy: 0.9482\n",
            "Epoch 136/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.1753 - accuracy: 0.9482\n",
            "Epoch 137/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.1753 - accuracy: 0.9482\n",
            "Epoch 138/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.1730 - accuracy: 0.9482\n",
            "Epoch 139/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1736 - accuracy: 0.9430\n",
            "Epoch 140/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.1719 - accuracy: 0.9482\n",
            "Epoch 141/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.1704 - accuracy: 0.9534\n",
            "Epoch 142/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1696 - accuracy: 0.9534\n",
            "Epoch 143/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1691 - accuracy: 0.9534\n",
            "Epoch 144/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.1675 - accuracy: 0.9534\n",
            "Epoch 145/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1668 - accuracy: 0.9534\n",
            "Epoch 146/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.1657 - accuracy: 0.9534\n",
            "Epoch 147/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.1648 - accuracy: 0.9482\n",
            "Epoch 148/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.1645 - accuracy: 0.9482\n",
            "Epoch 149/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.1627 - accuracy: 0.9534\n",
            "Epoch 150/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.1614 - accuracy: 0.9534\n",
            "Epoch 151/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.1602 - accuracy: 0.9534\n",
            "Epoch 152/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.1603 - accuracy: 0.9534\n",
            "Epoch 153/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.1586 - accuracy: 0.9534\n",
            "Epoch 154/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.1580 - accuracy: 0.9534\n",
            "Epoch 155/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.1575 - accuracy: 0.9585\n",
            "Epoch 156/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.1568 - accuracy: 0.9534\n",
            "Epoch 157/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.1574 - accuracy: 0.9585\n",
            "Epoch 158/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1602 - accuracy: 0.9534\n",
            "Epoch 159/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.1553 - accuracy: 0.9585\n",
            "Epoch 160/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.1534 - accuracy: 0.9585\n",
            "Epoch 161/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1526 - accuracy: 0.9585\n",
            "Epoch 162/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1518 - accuracy: 0.9585\n",
            "Epoch 163/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.1512 - accuracy: 0.9585\n",
            "Epoch 164/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.1521 - accuracy: 0.9585\n",
            "Epoch 165/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.1503 - accuracy: 0.9585\n",
            "Epoch 166/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1483 - accuracy: 0.9585\n",
            "Epoch 167/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1487 - accuracy: 0.9585\n",
            "Epoch 168/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1478 - accuracy: 0.9585\n",
            "Epoch 169/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1470 - accuracy: 0.9585\n",
            "Epoch 170/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1465 - accuracy: 0.9585\n",
            "Epoch 171/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1457 - accuracy: 0.9585\n",
            "Epoch 172/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1457 - accuracy: 0.9585\n",
            "Epoch 173/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1448 - accuracy: 0.9585\n",
            "Epoch 174/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1437 - accuracy: 0.9585\n",
            "Epoch 175/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.1430 - accuracy: 0.9585\n",
            "Epoch 176/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1427 - accuracy: 0.9585\n",
            "Epoch 177/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.1421 - accuracy: 0.9585\n",
            "Epoch 178/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.1408 - accuracy: 0.9585\n",
            "Epoch 179/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.1408 - accuracy: 0.9585\n",
            "Epoch 180/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.1408 - accuracy: 0.9585\n",
            "Epoch 181/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1400 - accuracy: 0.9585\n",
            "Epoch 182/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1390 - accuracy: 0.9585\n",
            "Epoch 183/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1390 - accuracy: 0.9585\n",
            "Epoch 184/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1379 - accuracy: 0.9585\n",
            "Epoch 185/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1372 - accuracy: 0.9585\n",
            "Epoch 186/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1367 - accuracy: 0.9585\n",
            "Epoch 187/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1359 - accuracy: 0.9585\n",
            "Epoch 188/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1360 - accuracy: 0.9585\n",
            "Epoch 189/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.1346 - accuracy: 0.9585\n",
            "Epoch 190/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1341 - accuracy: 0.9585\n",
            "Epoch 191/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1344 - accuracy: 0.9585\n",
            "Epoch 192/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1329 - accuracy: 0.9637\n",
            "Epoch 193/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.1322 - accuracy: 0.9637\n",
            "Epoch 194/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1325 - accuracy: 0.9585\n",
            "Epoch 195/200\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.1310 - accuracy: 0.9637\n",
            "Epoch 196/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.1309 - accuracy: 0.9637\n",
            "Epoch 197/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1303 - accuracy: 0.9585\n",
            "Epoch 198/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1292 - accuracy: 0.9637\n",
            "Epoch 199/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.1286 - accuracy: 0.9637\n",
            "Epoch 200/200\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1277 - accuracy: 0.9637\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa619831d90>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "model1.fit(X_train_scaled1 , Y_train1 , batch_size = 8 ,epochs = 200 )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOnrfnFobRtp",
        "outputId": "ed145d12-e172-4815-e3a7-ab5c61cb4b23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on training data: 96.37305736541748% \n",
            "Error on training data: 3.6269426345825195\n"
          ]
        }
      ],
      "source": [
        "# prediction on scalled train data\n",
        "pred_train1= model1.predict(X_train_scaled1)\n",
        "scores = model1.evaluate(X_train_scaled1, Y_train1, verbose=0)\n",
        "print('Accuracy on training data: {}% \\nError on training data: {}'.format(scores[1]*100, (1 - scores[1])*100))   \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction on scalled test data\n",
        "pred_test1= model1.predict(x_test_scaled_all1)\n",
        "\n",
        "scores1 = model1.evaluate(x_test_scaled_all1, y_test1, verbose=0)\n",
        "print('Accuracy on test data: {}% \\nError on test data: {}'.format(scores1[1]*100, (1 - scores1[1])*100))    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wu7NzVduR4px",
        "outputId": "629f5f35-e149-435f-ef7c-377eb33c27c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test data: 78.68852615356445% \n",
            "Error on test data: 21.311473846435547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting predictions to label(reversing onehot encoding operation)\n",
        "pred1 = list()\n",
        "for i in range(len(pred_test1)):\n",
        "    pred1.append(np.argmax(pred_test1[i])) # returns index of maximum value"
      ],
      "metadata": {
        "id": "TY0vsuPlmo5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting predictions to label(reversing onehot encoding operation) \n",
        "test1 = list()\n",
        "for i in range(len(y_test1)):\n",
        "    test1.append(np.argmax(y_test1[i])) # returns index of maximum value\n"
      ],
      "metadata": {
        "id": "hCxVl_Rgu1tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#comparing test and prediction\n",
        "pd.DataFrame({'Test':test1,'Pred':pred1}).head(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "wZkSy0M_uxvf",
        "outputId": "3a150700-033a-4de9-d2c6-cef349e11823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Test  Pred\n",
              "0      0     1\n",
              "1      1     1\n",
              "2      1     1\n",
              "3      0     0\n",
              "4      1     1\n",
              "5      1     1\n",
              "6      0     1\n",
              "7      0     0\n",
              "8      1     1\n",
              "9      1     1\n",
              "10     0     0\n",
              "11     1     0\n",
              "12     0     0\n",
              "13     0     0\n",
              "14     0     1\n",
              "15     1     1\n",
              "16     0     0\n",
              "17     0     0\n",
              "18     1     1\n",
              "19     1     0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-309fe72a-39b9-4e55-873d-92eab679688a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Test</th>\n",
              "      <th>Pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-309fe72a-39b9-4e55-873d-92eab679688a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-309fe72a-39b9-4e55-873d-92eab679688a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-309fe72a-39b9-4e55-873d-92eab679688a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD1cIeOQEA7I"
      },
      "source": [
        "**2. Implement ANN Model for multi-classification where you will classify mobiles into various price range based on multiple features. You have to train the network for all total 20 attributes.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "B0-G1qbdDYhV",
        "outputId": "50281bb3-e623-4e5a-f277-50bacd4da311"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      battery_power  blue  clock_speed  dual_sim  fc  four_g  int_memory  \\\n",
              "0               842     0          2.2         0   1       0           7   \n",
              "1              1021     1          0.5         1   0       1          53   \n",
              "2               563     1          0.5         1   2       1          41   \n",
              "3               615     1          2.5         0   0       0          10   \n",
              "4              1821     1          1.2         0  13       1          44   \n",
              "...             ...   ...          ...       ...  ..     ...         ...   \n",
              "1995            794     1          0.5         1   0       1           2   \n",
              "1996           1965     1          2.6         1   0       0          39   \n",
              "1997           1911     0          0.9         1   1       1          36   \n",
              "1998           1512     0          0.9         0   4       1          46   \n",
              "1999            510     1          2.0         1   5       1          45   \n",
              "\n",
              "      m_dep  mobile_wt  n_cores  ...  px_height  px_width   ram  sc_h  sc_w  \\\n",
              "0       0.6        188        2  ...         20       756  2549     9     7   \n",
              "1       0.7        136        3  ...        905      1988  2631    17     3   \n",
              "2       0.9        145        5  ...       1263      1716  2603    11     2   \n",
              "3       0.8        131        6  ...       1216      1786  2769    16     8   \n",
              "4       0.6        141        2  ...       1208      1212  1411     8     2   \n",
              "...     ...        ...      ...  ...        ...       ...   ...   ...   ...   \n",
              "1995    0.8        106        6  ...       1222      1890   668    13     4   \n",
              "1996    0.2        187        4  ...        915      1965  2032    11    10   \n",
              "1997    0.7        108        8  ...        868      1632  3057     9     1   \n",
              "1998    0.1        145        5  ...        336       670   869    18    10   \n",
              "1999    0.9        168        6  ...        483       754  3919    19     4   \n",
              "\n",
              "      talk_time  three_g  touch_screen  wifi  price_range  \n",
              "0            19        0             0     1            1  \n",
              "1             7        1             1     0            2  \n",
              "2             9        1             1     0            2  \n",
              "3            11        1             0     0            2  \n",
              "4            15        1             1     0            1  \n",
              "...         ...      ...           ...   ...          ...  \n",
              "1995         19        1             1     0            0  \n",
              "1996         16        1             1     1            2  \n",
              "1997          5        1             1     0            3  \n",
              "1998         19        1             1     1            0  \n",
              "1999          2        1             1     1            3  \n",
              "\n",
              "[2000 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4d0921fe-1b6e-41a3-9543-077408a1c977\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>battery_power</th>\n",
              "      <th>blue</th>\n",
              "      <th>clock_speed</th>\n",
              "      <th>dual_sim</th>\n",
              "      <th>fc</th>\n",
              "      <th>four_g</th>\n",
              "      <th>int_memory</th>\n",
              "      <th>m_dep</th>\n",
              "      <th>mobile_wt</th>\n",
              "      <th>n_cores</th>\n",
              "      <th>...</th>\n",
              "      <th>px_height</th>\n",
              "      <th>px_width</th>\n",
              "      <th>ram</th>\n",
              "      <th>sc_h</th>\n",
              "      <th>sc_w</th>\n",
              "      <th>talk_time</th>\n",
              "      <th>three_g</th>\n",
              "      <th>touch_screen</th>\n",
              "      <th>wifi</th>\n",
              "      <th>price_range</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842</td>\n",
              "      <td>0</td>\n",
              "      <td>2.2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0.6</td>\n",
              "      <td>188</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>756</td>\n",
              "      <td>2549</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1021</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>53</td>\n",
              "      <td>0.7</td>\n",
              "      <td>136</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>905</td>\n",
              "      <td>1988</td>\n",
              "      <td>2631</td>\n",
              "      <td>17</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>563</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>41</td>\n",
              "      <td>0.9</td>\n",
              "      <td>145</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>1263</td>\n",
              "      <td>1716</td>\n",
              "      <td>2603</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>615</td>\n",
              "      <td>1</td>\n",
              "      <td>2.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>0.8</td>\n",
              "      <td>131</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>1216</td>\n",
              "      <td>1786</td>\n",
              "      <td>2769</td>\n",
              "      <td>16</td>\n",
              "      <td>8</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1821</td>\n",
              "      <td>1</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>44</td>\n",
              "      <td>0.6</td>\n",
              "      <td>141</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>1208</td>\n",
              "      <td>1212</td>\n",
              "      <td>1411</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>794</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.8</td>\n",
              "      <td>106</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>1222</td>\n",
              "      <td>1890</td>\n",
              "      <td>668</td>\n",
              "      <td>13</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>1965</td>\n",
              "      <td>1</td>\n",
              "      <td>2.6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>0.2</td>\n",
              "      <td>187</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>915</td>\n",
              "      <td>1965</td>\n",
              "      <td>2032</td>\n",
              "      <td>11</td>\n",
              "      <td>10</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>1911</td>\n",
              "      <td>0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>36</td>\n",
              "      <td>0.7</td>\n",
              "      <td>108</td>\n",
              "      <td>8</td>\n",
              "      <td>...</td>\n",
              "      <td>868</td>\n",
              "      <td>1632</td>\n",
              "      <td>3057</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>1512</td>\n",
              "      <td>0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>46</td>\n",
              "      <td>0.1</td>\n",
              "      <td>145</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>336</td>\n",
              "      <td>670</td>\n",
              "      <td>869</td>\n",
              "      <td>18</td>\n",
              "      <td>10</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>510</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>45</td>\n",
              "      <td>0.9</td>\n",
              "      <td>168</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>483</td>\n",
              "      <td>754</td>\n",
              "      <td>3919</td>\n",
              "      <td>19</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4d0921fe-1b6e-41a3-9543-077408a1c977')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4d0921fe-1b6e-41a3-9543-077408a1c977 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4d0921fe-1b6e-41a3-9543-077408a1c977');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "mobile_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uniqueValues = mobile_df['price_range'].unique()\n",
        "uniqueValues"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV30t8BFS_ks",
        "outputId": "435473de-56cc-45d3-886d-a481701694ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 3, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69Lz8MNsrqHO"
      },
      "outputs": [],
      "source": [
        "# dividing data into train set and test set\n",
        "train_data2, test_data2 = train_test_split(mobile_df, test_size=0.2, random_state=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83XoSYNBrqHQ"
      },
      "outputs": [],
      "source": [
        "# spliting original data into trainset and test set\n",
        "x_train2, y_train2 = train_data2.drop('price_range', axis=1), train_data2['price_range']\n",
        "x_test2, y_test2 = test_data2.drop('price_range', axis=1), test_data2['price_range']\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#one hot encoding on target values based on no of class\n",
        "ohe = OneHotEncoder()\n",
        "\n",
        "y_train2=np.array(y_train2).reshape(-1,1)\n",
        "y_train2 = ohe.fit_transform(y_train2).toarray()\n",
        "\n",
        "y_test2=np.array(y_test2).reshape(-1,1)\n",
        "y_test2 = ohe.fit_transform(y_test2).toarray()"
      ],
      "metadata": {
        "id": "I_Ywrcg2rqHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkaTMhB8rqHR"
      },
      "outputs": [],
      "source": [
        "#function to normalize the data\n",
        "\n",
        "def scale_datasets(x_train, x_test):\n",
        "  standard_scaler = StandardScaler()\n",
        "  x_train_scaled = pd.DataFrame(\n",
        "      standard_scaler.fit_transform(x_train),\n",
        "      columns=x_train.columns\n",
        "  )\n",
        "  x_test_scaled = pd.DataFrame(\n",
        "      standard_scaler.transform(x_test),\n",
        "      columns = x_test.columns\n",
        "  )\n",
        "  return x_train_scaled, x_test_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_oFTxQhrqHS"
      },
      "outputs": [],
      "source": [
        "# normalizing the original data\n",
        "x_train_scaled_all2, x_test_scaled_all2 = scale_datasets(x_train2, x_test2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aee9f49d-9204-4220-9f12-70e64f5f3774",
        "id": "DxZMOhJQrqHT"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1280, 20)\n",
            "(1280, 4)\n",
            "(320, 20)\n"
          ]
        }
      ],
      "source": [
        "# Dividing scalled train data into train and validation data\n",
        "\n",
        "X_train_scaled2, X_val_scaled2, Y_train2, Y_val2 = train_test_split(x_train_scaled_all2, y_train2, test_size=0.20, random_state=40)\n",
        "print(X_train_scaled2.shape)\n",
        "print(Y_train2.shape)\n",
        "print(X_val_scaled2.shape)\n",
        "#validation data is not test data...\n",
        "#validation data is to check whether during training whether our training is happening successfully or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guT7KTWhSSKO"
      },
      "outputs": [],
      "source": [
        "model2 = Sequential()\n",
        "model2.add(Dense(activation = \"relu\", input_dim = 20, \n",
        "                     units = 40, kernel_initializer = \"uniform\"))\n",
        "model2.add(Dense(activation = \"relu\", units = 20, \n",
        "                     kernel_initializer = \"uniform\"))\n",
        "model2.add(Dense(activation = \"softmax\", units = 4, \n",
        "                     kernel_initializer = \"uniform\"))\n",
        "model2.compile(optimizer = 'adam' , loss = 'categorical_crossentropy', \n",
        "                   metrics = ['accuracy'] )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6rq8ODkzpWO",
        "outputId": "ae0698d6-ec16-4657-c478-454039050d89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 40)                840       \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 20)                820       \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 4)                 84        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,744\n",
            "Trainable params: 1,744\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "551f57b3-6f3d-46ff-af03-2fe577928c39",
        "id": "kiAvveKPsTVA"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "160/160 [==============================] - 1s 2ms/step - loss: 1.2363 - accuracy: 0.4430\n",
            "Epoch 2/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.5834 - accuracy: 0.7570\n",
            "Epoch 3/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.2952 - accuracy: 0.9266\n",
            "Epoch 4/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.1895 - accuracy: 0.9516\n",
            "Epoch 5/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.1426 - accuracy: 0.9656\n",
            "Epoch 6/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.1203 - accuracy: 0.9695\n",
            "Epoch 7/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.1074 - accuracy: 0.9672\n",
            "Epoch 8/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0889 - accuracy: 0.9750\n",
            "Epoch 9/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0796 - accuracy: 0.9781\n",
            "Epoch 10/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0739 - accuracy: 0.9781\n",
            "Epoch 11/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0644 - accuracy: 0.9820\n",
            "Epoch 12/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0629 - accuracy: 0.9812\n",
            "Epoch 13/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0562 - accuracy: 0.9844\n",
            "Epoch 14/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0552 - accuracy: 0.9836\n",
            "Epoch 15/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0480 - accuracy: 0.9875\n",
            "Epoch 16/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0469 - accuracy: 0.9906\n",
            "Epoch 17/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0389 - accuracy: 0.9922\n",
            "Epoch 18/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0458 - accuracy: 0.9852\n",
            "Epoch 19/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0326 - accuracy: 0.9922\n",
            "Epoch 20/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0331 - accuracy: 0.9930\n",
            "Epoch 21/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0313 - accuracy: 0.9914\n",
            "Epoch 22/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0293 - accuracy: 0.9937\n",
            "Epoch 23/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0285 - accuracy: 0.9914\n",
            "Epoch 24/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0276 - accuracy: 0.9930\n",
            "Epoch 25/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0225 - accuracy: 0.9961\n",
            "Epoch 26/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0195 - accuracy: 0.9984\n",
            "Epoch 27/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0247 - accuracy: 0.9945\n",
            "Epoch 28/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0189 - accuracy: 0.9977\n",
            "Epoch 29/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0154 - accuracy: 0.9984\n",
            "Epoch 30/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0276 - accuracy: 0.9906\n",
            "Epoch 31/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0195 - accuracy: 0.9945\n",
            "Epoch 32/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0136 - accuracy: 0.9977\n",
            "Epoch 33/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0198 - accuracy: 0.9953\n",
            "Epoch 34/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0130 - accuracy: 0.9992\n",
            "Epoch 35/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0085 - accuracy: 1.0000\n",
            "Epoch 36/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 0.9992\n",
            "Epoch 37/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0105 - accuracy: 0.9977\n",
            "Epoch 38/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0216 - accuracy: 0.9937\n",
            "Epoch 39/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0135 - accuracy: 0.9984\n",
            "Epoch 40/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0076 - accuracy: 0.9992\n",
            "Epoch 41/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0089 - accuracy: 0.9992\n",
            "Epoch 42/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0067 - accuracy: 0.9992\n",
            "Epoch 43/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0115 - accuracy: 0.9969\n",
            "Epoch 44/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0107 - accuracy: 0.9969\n",
            "Epoch 45/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 46/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0108 - accuracy: 0.9977\n",
            "Epoch 47/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0086 - accuracy: 0.9984\n",
            "Epoch 48/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0143 - accuracy: 0.9961\n",
            "Epoch 49/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0090 - accuracy: 0.9977\n",
            "Epoch 50/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0057 - accuracy: 1.0000\n",
            "Epoch 51/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 52/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0050 - accuracy: 0.9984\n",
            "Epoch 53/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0123 - accuracy: 0.9961\n",
            "Epoch 54/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0078 - accuracy: 1.0000\n",
            "Epoch 55/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 56/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 57/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 58/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 59/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 60/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 61/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 62/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 63/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 0.9992\n",
            "Epoch 64/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0555 - accuracy: 0.9773\n",
            "Epoch 65/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0168 - accuracy: 0.9945\n",
            "Epoch 66/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0060 - accuracy: 0.9969\n",
            "Epoch 67/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0083 - accuracy: 0.9969\n",
            "Epoch 68/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 69/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 9.9132e-04 - accuracy: 1.0000\n",
            "Epoch 70/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 8.9172e-04 - accuracy: 1.0000\n",
            "Epoch 71/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 8.3749e-04 - accuracy: 1.0000\n",
            "Epoch 72/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 7.7343e-04 - accuracy: 1.0000\n",
            "Epoch 73/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 7.5604e-04 - accuracy: 1.0000\n",
            "Epoch 74/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 7.0490e-04 - accuracy: 1.0000\n",
            "Epoch 75/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 6.9582e-04 - accuracy: 1.0000\n",
            "Epoch 76/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 6.5523e-04 - accuracy: 1.0000\n",
            "Epoch 77/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 6.3295e-04 - accuracy: 1.0000\n",
            "Epoch 78/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 5.8345e-04 - accuracy: 1.0000\n",
            "Epoch 79/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 5.8324e-04 - accuracy: 1.0000\n",
            "Epoch 80/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 5.5358e-04 - accuracy: 1.0000\n",
            "Epoch 81/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 5.3533e-04 - accuracy: 1.0000\n",
            "Epoch 82/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 4.9780e-04 - accuracy: 1.0000\n",
            "Epoch 83/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 4.9097e-04 - accuracy: 1.0000\n",
            "Epoch 84/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 4.3863e-04 - accuracy: 1.0000\n",
            "Epoch 85/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 5.3134e-04 - accuracy: 1.0000\n",
            "Epoch 86/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 4.7945e-04 - accuracy: 1.0000\n",
            "Epoch 87/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 3.9066e-04 - accuracy: 1.0000\n",
            "Epoch 88/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 3.6740e-04 - accuracy: 1.0000\n",
            "Epoch 89/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 4.2058e-04 - accuracy: 1.0000\n",
            "Epoch 90/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 4.7293e-04 - accuracy: 1.0000\n",
            "Epoch 91/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0811 - accuracy: 0.9750\n",
            "Epoch 92/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0647 - accuracy: 0.9789\n",
            "Epoch 93/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 0.9977\n",
            "Epoch 94/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0062 - accuracy: 0.9969\n",
            "Epoch 95/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 9.6494e-04 - accuracy: 1.0000\n",
            "Epoch 96/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 6.8962e-04 - accuracy: 1.0000\n",
            "Epoch 97/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 5.4555e-04 - accuracy: 1.0000\n",
            "Epoch 98/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 5.0719e-04 - accuracy: 1.0000\n",
            "Epoch 99/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 4.7691e-04 - accuracy: 1.0000\n",
            "Epoch 100/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 4.4825e-04 - accuracy: 1.0000\n",
            "Epoch 101/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 4.2839e-04 - accuracy: 1.0000\n",
            "Epoch 102/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 4.0838e-04 - accuracy: 1.0000\n",
            "Epoch 103/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 3.8853e-04 - accuracy: 1.0000\n",
            "Epoch 104/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 3.7278e-04 - accuracy: 1.0000\n",
            "Epoch 105/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 3.5203e-04 - accuracy: 1.0000\n",
            "Epoch 106/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 3.3983e-04 - accuracy: 1.0000\n",
            "Epoch 107/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 3.3046e-04 - accuracy: 1.0000\n",
            "Epoch 108/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 3.1624e-04 - accuracy: 1.0000\n",
            "Epoch 109/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 2.9913e-04 - accuracy: 1.0000\n",
            "Epoch 110/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 2.8815e-04 - accuracy: 1.0000\n",
            "Epoch 111/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 2.7802e-04 - accuracy: 1.0000\n",
            "Epoch 112/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 2.6965e-04 - accuracy: 1.0000\n",
            "Epoch 113/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 2.6298e-04 - accuracy: 1.0000\n",
            "Epoch 114/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 2.4688e-04 - accuracy: 1.0000\n",
            "Epoch 115/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 2.3568e-04 - accuracy: 1.0000\n",
            "Epoch 116/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 2.2558e-04 - accuracy: 1.0000\n",
            "Epoch 117/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 2.1597e-04 - accuracy: 1.0000\n",
            "Epoch 118/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 2.0761e-04 - accuracy: 1.0000\n",
            "Epoch 119/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 2.0161e-04 - accuracy: 1.0000\n",
            "Epoch 120/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.9027e-04 - accuracy: 1.0000\n",
            "Epoch 121/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.9523e-04 - accuracy: 1.0000\n",
            "Epoch 122/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.7740e-04 - accuracy: 1.0000\n",
            "Epoch 123/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.6653e-04 - accuracy: 1.0000\n",
            "Epoch 124/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.6166e-04 - accuracy: 1.0000\n",
            "Epoch 125/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.5782e-04 - accuracy: 1.0000\n",
            "Epoch 126/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.4963e-04 - accuracy: 1.0000\n",
            "Epoch 127/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.4134e-04 - accuracy: 1.0000\n",
            "Epoch 128/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.3616e-04 - accuracy: 1.0000\n",
            "Epoch 129/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.2357e-04 - accuracy: 1.0000\n",
            "Epoch 130/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.2327e-04 - accuracy: 1.0000\n",
            "Epoch 131/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.1256e-04 - accuracy: 1.0000\n",
            "Epoch 132/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.0830e-04 - accuracy: 1.0000\n",
            "Epoch 133/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.2207e-04 - accuracy: 1.0000\n",
            "Epoch 134/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 9.4169e-05 - accuracy: 1.0000\n",
            "Epoch 135/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 8.9784e-05 - accuracy: 1.0000\n",
            "Epoch 136/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 8.8096e-05 - accuracy: 1.0000\n",
            "Epoch 137/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 7.9531e-05 - accuracy: 1.0000\n",
            "Epoch 138/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 8.1172e-05 - accuracy: 1.0000\n",
            "Epoch 139/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 7.9712e-05 - accuracy: 1.0000\n",
            "Epoch 140/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 8.4399e-05 - accuracy: 1.0000\n",
            "Epoch 141/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 6.4044e-05 - accuracy: 1.0000\n",
            "Epoch 142/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 5.8621e-05 - accuracy: 1.0000\n",
            "Epoch 143/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 6.0699e-05 - accuracy: 1.0000\n",
            "Epoch 144/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 5.0580e-05 - accuracy: 1.0000\n",
            "Epoch 145/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 4.6294e-05 - accuracy: 1.0000\n",
            "Epoch 146/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 4.0403e-05 - accuracy: 1.0000\n",
            "Epoch 147/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 3.8728e-05 - accuracy: 1.0000\n",
            "Epoch 148/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 3.7386e-05 - accuracy: 1.0000\n",
            "Epoch 149/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 4.1485e-05 - accuracy: 1.0000\n",
            "Epoch 150/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 3.7111e-05 - accuracy: 1.0000\n",
            "Epoch 151/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 3.2192e-05 - accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 3.0418e-05 - accuracy: 1.0000\n",
            "Epoch 153/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 2.6529e-05 - accuracy: 1.0000\n",
            "Epoch 154/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 2.2645e-05 - accuracy: 1.0000\n",
            "Epoch 155/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 2.2363e-05 - accuracy: 1.0000\n",
            "Epoch 156/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 2.1389e-05 - accuracy: 1.0000\n",
            "Epoch 157/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.8688e-05 - accuracy: 1.0000\n",
            "Epoch 158/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.7838e-05 - accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.5756e-05 - accuracy: 1.0000\n",
            "Epoch 160/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.4899e-05 - accuracy: 1.0000\n",
            "Epoch 161/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.3302e-05 - accuracy: 1.0000\n",
            "Epoch 162/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.2633e-05 - accuracy: 1.0000\n",
            "Epoch 163/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.3017e-05 - accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.1110 - accuracy: 0.9820\n",
            "Epoch 165/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0497 - accuracy: 0.9844\n",
            "Epoch 166/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0064 - accuracy: 0.9992\n",
            "Epoch 167/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 3.2121e-04 - accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 2.2872e-04 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.9436e-04 - accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.7293e-04 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.5833e-04 - accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.4599e-04 - accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.3550e-04 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.2602e-04 - accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.1826e-04 - accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.1117e-04 - accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.0452e-04 - accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 9.8187e-05 - accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 9.3488e-05 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 8.8278e-05 - accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 8.3686e-05 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 7.9366e-05 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 7.5738e-05 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 7.2188e-05 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 6.7932e-05 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 6.4576e-05 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 6.1388e-05 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 5.8450e-05 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 5.5244e-05 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 5.2944e-05 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 5.0048e-05 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 4.7180e-05 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 4.5120e-05 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 4.3424e-05 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 4.0835e-05 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 3.9155e-05 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 3.6851e-05 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 3.5312e-05 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 3.3311e-05 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8146c402d0>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "model2.fit(X_train_scaled2 , Y_train2 , batch_size = 8 ,epochs = 200 )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cda2907-4f58-4f1a-cbeb-a258c1615205",
        "id": "v_A2AzXqsTVB"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on training data: 100.0% \n",
            "Error on training data: 0.0\n"
          ]
        }
      ],
      "source": [
        "pred_train2= model2.predict(X_train_scaled2)\n",
        "scores2 = model2.evaluate(X_train_scaled2, Y_train2, verbose=0)\n",
        "print('Accuracy on training data: {}% \\nError on training data: {}'.format(scores2[1]*100, (1 - scores2[1])*100))   \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_test2= model2.predict(x_test_scaled_all2)\n",
        "scores2 = model2.evaluate(x_test_scaled_all2, y_test2, verbose=0)\n",
        "print('Accuracy on test data: {}% \\nError on test data: {}'.format(scores2[1]*100, (1 - scores2[1])*100))    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bac90a1-55b7-489a-8c8d-9c05ab192d43",
        "id": "peLcMUyCsTVC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test data: 93.50000023841858% \n",
            "Error on test data: 6.499999761581421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting predictions to label\n",
        "pred2 = list()\n",
        "for i in range(len(pred_test2)):\n",
        "    pred2.append(np.argmax(pred_test2[i])) # returns index of maximum value\n",
        "\n"
      ],
      "metadata": {
        "id": "SRpBPVIxsTVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting predictions to label\n",
        "test2 = list()\n",
        "for i in range(len(y_test2)):\n",
        "    test2.append(np.argmax(y_test2[i])) # returns index of maximum value\n"
      ],
      "metadata": {
        "id": "jNUB5k_WtEF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#comparing test and prediction\n",
        "pd.DataFrame({'Test':test2,'Pred':pred2}).head(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "wAgdp-qPta8z",
        "outputId": "4f103666-e312-4ec9-f80f-2c07003f3aa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Test  Pred\n",
              "0      1     1\n",
              "1      1     1\n",
              "2      3     3\n",
              "3      0     0\n",
              "4      1     1\n",
              "5      1     1\n",
              "6      0     0\n",
              "7      2     2\n",
              "8      0     0\n",
              "9      3     3\n",
              "10     1     1\n",
              "11     0     0\n",
              "12     1     1\n",
              "13     1     1\n",
              "14     1     1\n",
              "15     2     2\n",
              "16     1     1\n",
              "17     2     2\n",
              "18     3     3\n",
              "19     0     0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ce047dc8-86f4-40f6-b876-bb0f158b73da\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Test</th>\n",
              "      <th>Pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce047dc8-86f4-40f6-b876-bb0f158b73da')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ce047dc8-86f4-40f6-b876-bb0f158b73da button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ce047dc8-86f4-40f6-b876-bb0f158b73da');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jeb9TDQnEGdW"
      },
      "source": [
        "**3.Implement ANN model for regression to predict battery power based on other mobile features.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mobile_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "LXWbwA1OpNXG",
        "outputId": "40146510-6e11-458f-be59-a7833d8eebca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      battery_power  blue  clock_speed  dual_sim  fc  four_g  int_memory  \\\n",
              "0               842     0          2.2         0   1       0           7   \n",
              "1              1021     1          0.5         1   0       1          53   \n",
              "2               563     1          0.5         1   2       1          41   \n",
              "3               615     1          2.5         0   0       0          10   \n",
              "4              1821     1          1.2         0  13       1          44   \n",
              "...             ...   ...          ...       ...  ..     ...         ...   \n",
              "1995            794     1          0.5         1   0       1           2   \n",
              "1996           1965     1          2.6         1   0       0          39   \n",
              "1997           1911     0          0.9         1   1       1          36   \n",
              "1998           1512     0          0.9         0   4       1          46   \n",
              "1999            510     1          2.0         1   5       1          45   \n",
              "\n",
              "      m_dep  mobile_wt  n_cores  ...  px_height  px_width   ram  sc_h  sc_w  \\\n",
              "0       0.6        188        2  ...         20       756  2549     9     7   \n",
              "1       0.7        136        3  ...        905      1988  2631    17     3   \n",
              "2       0.9        145        5  ...       1263      1716  2603    11     2   \n",
              "3       0.8        131        6  ...       1216      1786  2769    16     8   \n",
              "4       0.6        141        2  ...       1208      1212  1411     8     2   \n",
              "...     ...        ...      ...  ...        ...       ...   ...   ...   ...   \n",
              "1995    0.8        106        6  ...       1222      1890   668    13     4   \n",
              "1996    0.2        187        4  ...        915      1965  2032    11    10   \n",
              "1997    0.7        108        8  ...        868      1632  3057     9     1   \n",
              "1998    0.1        145        5  ...        336       670   869    18    10   \n",
              "1999    0.9        168        6  ...        483       754  3919    19     4   \n",
              "\n",
              "      talk_time  three_g  touch_screen  wifi  price_range  \n",
              "0            19        0             0     1            1  \n",
              "1             7        1             1     0            2  \n",
              "2             9        1             1     0            2  \n",
              "3            11        1             0     0            2  \n",
              "4            15        1             1     0            1  \n",
              "...         ...      ...           ...   ...          ...  \n",
              "1995         19        1             1     0            0  \n",
              "1996         16        1             1     1            2  \n",
              "1997          5        1             1     0            3  \n",
              "1998         19        1             1     1            0  \n",
              "1999          2        1             1     1            3  \n",
              "\n",
              "[2000 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-698cbb3f-078d-4d33-b8fe-9d1dd6c7ecdf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>battery_power</th>\n",
              "      <th>blue</th>\n",
              "      <th>clock_speed</th>\n",
              "      <th>dual_sim</th>\n",
              "      <th>fc</th>\n",
              "      <th>four_g</th>\n",
              "      <th>int_memory</th>\n",
              "      <th>m_dep</th>\n",
              "      <th>mobile_wt</th>\n",
              "      <th>n_cores</th>\n",
              "      <th>...</th>\n",
              "      <th>px_height</th>\n",
              "      <th>px_width</th>\n",
              "      <th>ram</th>\n",
              "      <th>sc_h</th>\n",
              "      <th>sc_w</th>\n",
              "      <th>talk_time</th>\n",
              "      <th>three_g</th>\n",
              "      <th>touch_screen</th>\n",
              "      <th>wifi</th>\n",
              "      <th>price_range</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842</td>\n",
              "      <td>0</td>\n",
              "      <td>2.2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0.6</td>\n",
              "      <td>188</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>756</td>\n",
              "      <td>2549</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1021</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>53</td>\n",
              "      <td>0.7</td>\n",
              "      <td>136</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>905</td>\n",
              "      <td>1988</td>\n",
              "      <td>2631</td>\n",
              "      <td>17</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>563</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>41</td>\n",
              "      <td>0.9</td>\n",
              "      <td>145</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>1263</td>\n",
              "      <td>1716</td>\n",
              "      <td>2603</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>615</td>\n",
              "      <td>1</td>\n",
              "      <td>2.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>0.8</td>\n",
              "      <td>131</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>1216</td>\n",
              "      <td>1786</td>\n",
              "      <td>2769</td>\n",
              "      <td>16</td>\n",
              "      <td>8</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1821</td>\n",
              "      <td>1</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>44</td>\n",
              "      <td>0.6</td>\n",
              "      <td>141</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>1208</td>\n",
              "      <td>1212</td>\n",
              "      <td>1411</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>794</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.8</td>\n",
              "      <td>106</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>1222</td>\n",
              "      <td>1890</td>\n",
              "      <td>668</td>\n",
              "      <td>13</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>1965</td>\n",
              "      <td>1</td>\n",
              "      <td>2.6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>0.2</td>\n",
              "      <td>187</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>915</td>\n",
              "      <td>1965</td>\n",
              "      <td>2032</td>\n",
              "      <td>11</td>\n",
              "      <td>10</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>1911</td>\n",
              "      <td>0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>36</td>\n",
              "      <td>0.7</td>\n",
              "      <td>108</td>\n",
              "      <td>8</td>\n",
              "      <td>...</td>\n",
              "      <td>868</td>\n",
              "      <td>1632</td>\n",
              "      <td>3057</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>1512</td>\n",
              "      <td>0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>46</td>\n",
              "      <td>0.1</td>\n",
              "      <td>145</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>336</td>\n",
              "      <td>670</td>\n",
              "      <td>869</td>\n",
              "      <td>18</td>\n",
              "      <td>10</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>510</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>45</td>\n",
              "      <td>0.9</td>\n",
              "      <td>168</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>483</td>\n",
              "      <td>754</td>\n",
              "      <td>3919</td>\n",
              "      <td>19</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-698cbb3f-078d-4d33-b8fe-9d1dd6c7ecdf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-698cbb3f-078d-4d33-b8fe-9d1dd6c7ecdf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-698cbb3f-078d-4d33-b8fe-9d1dd6c7ecdf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhdGWP9nNVAU"
      },
      "outputs": [],
      "source": [
        "# dividing data into train set and test set\n",
        "train_data, test_data = train_test_split(mobile_df, test_size=0.2, random_state=25)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7h14MM8NVAv"
      },
      "outputs": [],
      "source": [
        "#function to normalize the data\n",
        "def scale_datasets(x_train, x_test):\n",
        "  standard_scaler = StandardScaler()\n",
        "  x_train_scaled = pd.DataFrame(\n",
        "      standard_scaler.fit_transform(x_train),\n",
        "      columns=x_train.columns\n",
        "  )\n",
        "  x_test_scaled = pd.DataFrame(\n",
        "      standard_scaler.transform(x_test),\n",
        "      columns = x_test.columns\n",
        "  )\n",
        "  return x_train_scaled, x_test_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMX7h7A0NVAv"
      },
      "outputs": [],
      "source": [
        "x_train, y_train = train_data.drop('battery_power', axis=1), train_data['battery_power']\n",
        "x_test, y_test = test_data.drop('battery_power', axis=1), test_data['battery_power']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcDRd2JdNVAv"
      },
      "outputs": [],
      "source": [
        "x_train_scaled, x_test_scaled = scale_datasets(x_train, x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlmwMlIENVAw",
        "outputId": "6f571e1d-1855-4c18-db46-14bbee6ab38d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1280, 20)\n",
            "(1280,)\n",
            "(320, 20)\n"
          ]
        }
      ],
      "source": [
        "# Dividing scalled train data into train and validation data\n",
        "\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(x_train_scaled, y_train, test_size=0.20, random_state=40)\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(X_val.shape)\n",
        "#validation data is not test data...\n",
        "#validation data is to check whether during training whether our training is happening successfully or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fc-4GGxbNqZB"
      },
      "outputs": [],
      "source": [
        "hidden_units1 = 200\n",
        "hidden_units2 = 150\n",
        "hidden_units3 = 150\n",
        "# Creating model using the Sequential in tensorflow\n",
        "#Sequential() meaning each layer will be added one after another\n",
        "def build_model_using_sequential():\n",
        "  model = Sequential([\n",
        "    Dense(hidden_units1, input_dim=20, kernel_initializer='normal'),\n",
        "    Dense(hidden_units2, kernel_initializer='normal', activation='relu'),\n",
        "    Dense(hidden_units3, kernel_initializer='normal', activation='relu'),\n",
        "    Dense(1, kernel_initializer='normal',activation='linear')\n",
        "  ])\n",
        "  return model\n",
        "# build the model\n",
        "model = build_model_using_sequential()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRYBVqMeN3KM"
      },
      "outputs": [],
      "source": [
        "#r2\n",
        "\n",
        "model.compile(\n",
        "    loss=MeanSquaredLogarithmicError(), \n",
        "    optimizer=SGD(learning_rate = 0.001), \n",
        "    metrics=[MeanSquaredLogarithmicError()]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ob00A7rCN6NR",
        "outputId": "982a934a-fb0c-46a7-9e49-857cf981cd43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "10/10 [==============================] - 1s 24ms/step - loss: 49.6230 - mean_squared_logarithmic_error: 49.6230 - val_loss: 49.6976 - val_mean_squared_logarithmic_error: 49.8199\n",
            "Epoch 2/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 47.8504 - mean_squared_logarithmic_error: 47.8504 - val_loss: 47.7235 - val_mean_squared_logarithmic_error: 47.8426\n",
            "Epoch 3/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 46.1170 - mean_squared_logarithmic_error: 46.1170 - val_loss: 46.1985 - val_mean_squared_logarithmic_error: 46.3150\n",
            "Epoch 4/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 44.7321 - mean_squared_logarithmic_error: 44.7321 - val_loss: 44.9323 - val_mean_squared_logarithmic_error: 45.0464\n",
            "Epoch 5/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 43.5576 - mean_squared_logarithmic_error: 43.5576 - val_loss: 43.8312 - val_mean_squared_logarithmic_error: 43.9433\n",
            "Epoch 6/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 42.5205 - mean_squared_logarithmic_error: 42.5205 - val_loss: 42.8424 - val_mean_squared_logarithmic_error: 42.9525\n",
            "Epoch 7/200\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 41.5792 - mean_squared_logarithmic_error: 41.5792 - val_loss: 41.9337 - val_mean_squared_logarithmic_error: 42.0420\n",
            "Epoch 8/200\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 40.7076 - mean_squared_logarithmic_error: 40.7076 - val_loss: 41.0843 - val_mean_squared_logarithmic_error: 41.1910\n",
            "Epoch 9/200\n",
            "10/10 [==============================] - 0s 14ms/step - loss: 39.8881 - mean_squared_logarithmic_error: 39.8881 - val_loss: 40.2798 - val_mean_squared_logarithmic_error: 40.3848\n",
            "Epoch 10/200\n",
            "10/10 [==============================] - 0s 15ms/step - loss: 39.1085 - mean_squared_logarithmic_error: 39.1085 - val_loss: 39.5103 - val_mean_squared_logarithmic_error: 39.6138\n",
            "Epoch 11/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 38.3604 - mean_squared_logarithmic_error: 38.3604 - val_loss: 38.7688 - val_mean_squared_logarithmic_error: 38.8708\n",
            "Epoch 12/200\n",
            "10/10 [==============================] - 0s 14ms/step - loss: 37.6378 - mean_squared_logarithmic_error: 37.6378 - val_loss: 38.0504 - val_mean_squared_logarithmic_error: 38.1510\n",
            "Epoch 13/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 36.9363 - mean_squared_logarithmic_error: 36.9363 - val_loss: 37.3513 - val_mean_squared_logarithmic_error: 37.4504\n",
            "Epoch 14/200\n",
            "10/10 [==============================] - 0s 15ms/step - loss: 36.2524 - mean_squared_logarithmic_error: 36.2524 - val_loss: 36.6686 - val_mean_squared_logarithmic_error: 36.7664\n",
            "Epoch 15/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 35.5840 - mean_squared_logarithmic_error: 35.5840 - val_loss: 36.0009 - val_mean_squared_logarithmic_error: 36.0973\n",
            "Epoch 16/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 34.9295 - mean_squared_logarithmic_error: 34.9295 - val_loss: 35.3469 - val_mean_squared_logarithmic_error: 35.4420\n",
            "Epoch 17/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 34.2880 - mean_squared_logarithmic_error: 34.2880 - val_loss: 34.7054 - val_mean_squared_logarithmic_error: 34.7992\n",
            "Epoch 18/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 33.6586 - mean_squared_logarithmic_error: 33.6586 - val_loss: 34.0759 - val_mean_squared_logarithmic_error: 34.1683\n",
            "Epoch 19/200\n",
            "10/10 [==============================] - 0s 18ms/step - loss: 33.0407 - mean_squared_logarithmic_error: 33.0407 - val_loss: 33.4578 - val_mean_squared_logarithmic_error: 33.5490\n",
            "Epoch 20/200\n",
            "10/10 [==============================] - 0s 13ms/step - loss: 32.4340 - mean_squared_logarithmic_error: 32.4340 - val_loss: 32.8512 - val_mean_squared_logarithmic_error: 32.9411\n",
            "Epoch 21/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 31.8386 - mean_squared_logarithmic_error: 31.8386 - val_loss: 32.2557 - val_mean_squared_logarithmic_error: 32.3444\n",
            "Epoch 22/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 31.2539 - mean_squared_logarithmic_error: 31.2539 - val_loss: 31.6711 - val_mean_squared_logarithmic_error: 31.7586\n",
            "Epoch 23/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 30.6800 - mean_squared_logarithmic_error: 30.6800 - val_loss: 31.0974 - val_mean_squared_logarithmic_error: 31.1837\n",
            "Epoch 24/200\n",
            "10/10 [==============================] - 0s 13ms/step - loss: 30.1167 - mean_squared_logarithmic_error: 30.1167 - val_loss: 30.5345 - val_mean_squared_logarithmic_error: 30.6196\n",
            "Epoch 25/200\n",
            "10/10 [==============================] - 0s 18ms/step - loss: 29.5641 - mean_squared_logarithmic_error: 29.5641 - val_loss: 29.9823 - val_mean_squared_logarithmic_error: 30.0662\n",
            "Epoch 26/200\n",
            "10/10 [==============================] - 0s 13ms/step - loss: 29.0222 - mean_squared_logarithmic_error: 29.0222 - val_loss: 29.4408 - val_mean_squared_logarithmic_error: 29.5235\n",
            "Epoch 27/200\n",
            "10/10 [==============================] - 0s 14ms/step - loss: 28.4908 - mean_squared_logarithmic_error: 28.4908 - val_loss: 28.9100 - val_mean_squared_logarithmic_error: 28.9916\n",
            "Epoch 28/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 27.9699 - mean_squared_logarithmic_error: 27.9699 - val_loss: 28.3898 - val_mean_squared_logarithmic_error: 28.4703\n",
            "Epoch 29/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 27.4597 - mean_squared_logarithmic_error: 27.4597 - val_loss: 27.8801 - val_mean_squared_logarithmic_error: 27.9596\n",
            "Epoch 30/200\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 26.9599 - mean_squared_logarithmic_error: 26.9599 - val_loss: 27.3810 - val_mean_squared_logarithmic_error: 27.4594\n",
            "Epoch 31/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 26.4704 - mean_squared_logarithmic_error: 26.4704 - val_loss: 26.8922 - val_mean_squared_logarithmic_error: 26.9696\n",
            "Epoch 32/200\n",
            "10/10 [==============================] - 0s 14ms/step - loss: 25.9913 - mean_squared_logarithmic_error: 25.9913 - val_loss: 26.4135 - val_mean_squared_logarithmic_error: 26.4899\n",
            "Epoch 33/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 25.5220 - mean_squared_logarithmic_error: 25.5220 - val_loss: 25.9449 - val_mean_squared_logarithmic_error: 26.0204\n",
            "Epoch 34/200\n",
            "10/10 [==============================] - 0s 14ms/step - loss: 25.0629 - mean_squared_logarithmic_error: 25.0629 - val_loss: 25.4862 - val_mean_squared_logarithmic_error: 25.5607\n",
            "Epoch 35/200\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 24.6135 - mean_squared_logarithmic_error: 24.6135 - val_loss: 25.0372 - val_mean_squared_logarithmic_error: 25.1108\n",
            "Epoch 36/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 24.1737 - mean_squared_logarithmic_error: 24.1737 - val_loss: 24.5979 - val_mean_squared_logarithmic_error: 24.6705\n",
            "Epoch 37/200\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 23.7435 - mean_squared_logarithmic_error: 23.7435 - val_loss: 24.1679 - val_mean_squared_logarithmic_error: 24.2397\n",
            "Epoch 38/200\n",
            "10/10 [==============================] - 0s 13ms/step - loss: 23.3224 - mean_squared_logarithmic_error: 23.3224 - val_loss: 23.7471 - val_mean_squared_logarithmic_error: 23.8181\n",
            "Epoch 39/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 22.9106 - mean_squared_logarithmic_error: 22.9106 - val_loss: 23.3353 - val_mean_squared_logarithmic_error: 23.4055\n",
            "Epoch 40/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 22.5075 - mean_squared_logarithmic_error: 22.5075 - val_loss: 22.9323 - val_mean_squared_logarithmic_error: 23.0017\n",
            "Epoch 41/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 22.1132 - mean_squared_logarithmic_error: 22.1132 - val_loss: 22.5377 - val_mean_squared_logarithmic_error: 22.6064\n",
            "Epoch 42/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 21.7275 - mean_squared_logarithmic_error: 21.7275 - val_loss: 22.1516 - val_mean_squared_logarithmic_error: 22.2196\n",
            "Epoch 43/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 21.3499 - mean_squared_logarithmic_error: 21.3499 - val_loss: 21.7737 - val_mean_squared_logarithmic_error: 21.8410\n",
            "Epoch 44/200\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 20.9801 - mean_squared_logarithmic_error: 20.9801 - val_loss: 21.4038 - val_mean_squared_logarithmic_error: 21.4705\n",
            "Epoch 45/200\n",
            "10/10 [==============================] - 0s 16ms/step - loss: 20.6185 - mean_squared_logarithmic_error: 20.6185 - val_loss: 21.0417 - val_mean_squared_logarithmic_error: 21.1077\n",
            "Epoch 46/200\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 20.2645 - mean_squared_logarithmic_error: 20.2645 - val_loss: 20.6871 - val_mean_squared_logarithmic_error: 20.7525\n",
            "Epoch 47/200\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 19.9180 - mean_squared_logarithmic_error: 19.9180 - val_loss: 20.3399 - val_mean_squared_logarithmic_error: 20.4047\n",
            "Epoch 48/200\n",
            "10/10 [==============================] - 0s 17ms/step - loss: 19.5785 - mean_squared_logarithmic_error: 19.5785 - val_loss: 19.9999 - val_mean_squared_logarithmic_error: 20.0641\n",
            "Epoch 49/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 19.2461 - mean_squared_logarithmic_error: 19.2461 - val_loss: 19.6669 - val_mean_squared_logarithmic_error: 19.7305\n",
            "Epoch 50/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 18.9206 - mean_squared_logarithmic_error: 18.9206 - val_loss: 19.3408 - val_mean_squared_logarithmic_error: 19.4038\n",
            "Epoch 51/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 18.6018 - mean_squared_logarithmic_error: 18.6018 - val_loss: 19.0212 - val_mean_squared_logarithmic_error: 19.0837\n",
            "Epoch 52/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 18.2894 - mean_squared_logarithmic_error: 18.2894 - val_loss: 18.7082 - val_mean_squared_logarithmic_error: 18.7701\n",
            "Epoch 53/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 17.9835 - mean_squared_logarithmic_error: 17.9835 - val_loss: 18.4014 - val_mean_squared_logarithmic_error: 18.4628\n",
            "Epoch 54/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 17.6839 - mean_squared_logarithmic_error: 17.6839 - val_loss: 18.1007 - val_mean_squared_logarithmic_error: 18.1616\n",
            "Epoch 55/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 17.3901 - mean_squared_logarithmic_error: 17.3901 - val_loss: 17.8061 - val_mean_squared_logarithmic_error: 17.8664\n",
            "Epoch 56/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 17.1025 - mean_squared_logarithmic_error: 17.1025 - val_loss: 17.5173 - val_mean_squared_logarithmic_error: 17.5771\n",
            "Epoch 57/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 16.8205 - mean_squared_logarithmic_error: 16.8205 - val_loss: 17.2340 - val_mean_squared_logarithmic_error: 17.2934\n",
            "Epoch 58/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 16.5443 - mean_squared_logarithmic_error: 16.5443 - val_loss: 16.9563 - val_mean_squared_logarithmic_error: 17.0152\n",
            "Epoch 59/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 16.2733 - mean_squared_logarithmic_error: 16.2733 - val_loss: 16.6841 - val_mean_squared_logarithmic_error: 16.7424\n",
            "Epoch 60/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 16.0078 - mean_squared_logarithmic_error: 16.0078 - val_loss: 16.4170 - val_mean_squared_logarithmic_error: 16.4749\n",
            "Epoch 61/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 15.7476 - mean_squared_logarithmic_error: 15.7476 - val_loss: 16.1552 - val_mean_squared_logarithmic_error: 16.2126\n",
            "Epoch 62/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 15.4925 - mean_squared_logarithmic_error: 15.4925 - val_loss: 15.8984 - val_mean_squared_logarithmic_error: 15.9554\n",
            "Epoch 63/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 15.2421 - mean_squared_logarithmic_error: 15.2421 - val_loss: 15.6465 - val_mean_squared_logarithmic_error: 15.7030\n",
            "Epoch 64/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 14.9965 - mean_squared_logarithmic_error: 14.9965 - val_loss: 15.3993 - val_mean_squared_logarithmic_error: 15.4554\n",
            "Epoch 65/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 14.7560 - mean_squared_logarithmic_error: 14.7560 - val_loss: 15.1569 - val_mean_squared_logarithmic_error: 15.2126\n",
            "Epoch 66/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 14.5198 - mean_squared_logarithmic_error: 14.5198 - val_loss: 14.9190 - val_mean_squared_logarithmic_error: 14.9743\n",
            "Epoch 67/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 14.2880 - mean_squared_logarithmic_error: 14.2880 - val_loss: 14.6855 - val_mean_squared_logarithmic_error: 14.7404\n",
            "Epoch 68/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 14.0607 - mean_squared_logarithmic_error: 14.0607 - val_loss: 14.4565 - val_mean_squared_logarithmic_error: 14.5111\n",
            "Epoch 69/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 13.8377 - mean_squared_logarithmic_error: 13.8377 - val_loss: 14.2319 - val_mean_squared_logarithmic_error: 14.2861\n",
            "Epoch 70/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 13.6190 - mean_squared_logarithmic_error: 13.6190 - val_loss: 14.0115 - val_mean_squared_logarithmic_error: 14.0653\n",
            "Epoch 71/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 13.4043 - mean_squared_logarithmic_error: 13.4043 - val_loss: 13.7953 - val_mean_squared_logarithmic_error: 13.8487\n",
            "Epoch 72/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 13.1939 - mean_squared_logarithmic_error: 13.1939 - val_loss: 13.5832 - val_mean_squared_logarithmic_error: 13.6362\n",
            "Epoch 73/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 12.9873 - mean_squared_logarithmic_error: 12.9873 - val_loss: 13.3749 - val_mean_squared_logarithmic_error: 13.4275\n",
            "Epoch 74/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 12.7848 - mean_squared_logarithmic_error: 12.7848 - val_loss: 13.1705 - val_mean_squared_logarithmic_error: 13.2228\n",
            "Epoch 75/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 12.5861 - mean_squared_logarithmic_error: 12.5861 - val_loss: 12.9699 - val_mean_squared_logarithmic_error: 13.0218\n",
            "Epoch 76/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 12.3911 - mean_squared_logarithmic_error: 12.3911 - val_loss: 12.7730 - val_mean_squared_logarithmic_error: 12.8245\n",
            "Epoch 77/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 12.1999 - mean_squared_logarithmic_error: 12.1999 - val_loss: 12.5798 - val_mean_squared_logarithmic_error: 12.6310\n",
            "Epoch 78/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 12.0122 - mean_squared_logarithmic_error: 12.0122 - val_loss: 12.3902 - val_mean_squared_logarithmic_error: 12.4410\n",
            "Epoch 79/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 11.8280 - mean_squared_logarithmic_error: 11.8280 - val_loss: 12.2041 - val_mean_squared_logarithmic_error: 12.2546\n",
            "Epoch 80/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 11.6474 - mean_squared_logarithmic_error: 11.6474 - val_loss: 12.0215 - val_mean_squared_logarithmic_error: 12.0716\n",
            "Epoch 81/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 11.4701 - mean_squared_logarithmic_error: 11.4701 - val_loss: 11.8422 - val_mean_squared_logarithmic_error: 11.8920\n",
            "Epoch 82/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 11.2962 - mean_squared_logarithmic_error: 11.2962 - val_loss: 11.6664 - val_mean_squared_logarithmic_error: 11.7158\n",
            "Epoch 83/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 11.1255 - mean_squared_logarithmic_error: 11.1255 - val_loss: 11.4938 - val_mean_squared_logarithmic_error: 11.5429\n",
            "Epoch 84/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 10.9580 - mean_squared_logarithmic_error: 10.9580 - val_loss: 11.3244 - val_mean_squared_logarithmic_error: 11.3732\n",
            "Epoch 85/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 10.7937 - mean_squared_logarithmic_error: 10.7937 - val_loss: 11.1581 - val_mean_squared_logarithmic_error: 11.2066\n",
            "Epoch 86/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 10.6326 - mean_squared_logarithmic_error: 10.6326 - val_loss: 10.9949 - val_mean_squared_logarithmic_error: 11.0431\n",
            "Epoch 87/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 10.4743 - mean_squared_logarithmic_error: 10.4743 - val_loss: 10.8347 - val_mean_squared_logarithmic_error: 10.8825\n",
            "Epoch 88/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 10.3192 - mean_squared_logarithmic_error: 10.3192 - val_loss: 10.6775 - val_mean_squared_logarithmic_error: 10.7251\n",
            "Epoch 89/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 10.1667 - mean_squared_logarithmic_error: 10.1667 - val_loss: 10.5233 - val_mean_squared_logarithmic_error: 10.5705\n",
            "Epoch 90/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 10.0175 - mean_squared_logarithmic_error: 10.0175 - val_loss: 10.3719 - val_mean_squared_logarithmic_error: 10.4188\n",
            "Epoch 91/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 9.8709 - mean_squared_logarithmic_error: 9.8709 - val_loss: 10.2234 - val_mean_squared_logarithmic_error: 10.2700\n",
            "Epoch 92/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 9.7271 - mean_squared_logarithmic_error: 9.7271 - val_loss: 10.0777 - val_mean_squared_logarithmic_error: 10.1240\n",
            "Epoch 93/200\n",
            "10/10 [==============================] - 0s 13ms/step - loss: 9.5860 - mean_squared_logarithmic_error: 9.5860 - val_loss: 9.9347 - val_mean_squared_logarithmic_error: 9.9808\n",
            "Epoch 94/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 9.4476 - mean_squared_logarithmic_error: 9.4476 - val_loss: 9.7943 - val_mean_squared_logarithmic_error: 9.8401\n",
            "Epoch 95/200\n",
            "10/10 [==============================] - 0s 14ms/step - loss: 9.3117 - mean_squared_logarithmic_error: 9.3117 - val_loss: 9.6566 - val_mean_squared_logarithmic_error: 9.7021\n",
            "Epoch 96/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 9.1786 - mean_squared_logarithmic_error: 9.1786 - val_loss: 9.5215 - val_mean_squared_logarithmic_error: 9.5667\n",
            "Epoch 97/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.0479 - mean_squared_logarithmic_error: 9.0479 - val_loss: 9.3888 - val_mean_squared_logarithmic_error: 9.4338\n",
            "Epoch 98/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 8.9196 - mean_squared_logarithmic_error: 8.9196 - val_loss: 9.2587 - val_mean_squared_logarithmic_error: 9.3034\n",
            "Epoch 99/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 8.7937 - mean_squared_logarithmic_error: 8.7937 - val_loss: 9.1309 - val_mean_squared_logarithmic_error: 9.1754\n",
            "Epoch 100/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 8.6701 - mean_squared_logarithmic_error: 8.6701 - val_loss: 9.0056 - val_mean_squared_logarithmic_error: 9.0498\n",
            "Epoch 101/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 8.5490 - mean_squared_logarithmic_error: 8.5490 - val_loss: 8.8826 - val_mean_squared_logarithmic_error: 8.9265\n",
            "Epoch 102/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 8.4300 - mean_squared_logarithmic_error: 8.4300 - val_loss: 8.7619 - val_mean_squared_logarithmic_error: 8.8055\n",
            "Epoch 103/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 8.3133 - mean_squared_logarithmic_error: 8.3133 - val_loss: 8.6434 - val_mean_squared_logarithmic_error: 8.6867\n",
            "Epoch 104/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 8.1987 - mean_squared_logarithmic_error: 8.1987 - val_loss: 8.5271 - val_mean_squared_logarithmic_error: 8.5702\n",
            "Epoch 105/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 8.0863 - mean_squared_logarithmic_error: 8.0863 - val_loss: 8.4130 - val_mean_squared_logarithmic_error: 8.4558\n",
            "Epoch 106/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 7.9759 - mean_squared_logarithmic_error: 7.9759 - val_loss: 8.3009 - val_mean_squared_logarithmic_error: 8.3435\n",
            "Epoch 107/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 7.8676 - mean_squared_logarithmic_error: 7.8676 - val_loss: 8.1909 - val_mean_squared_logarithmic_error: 8.2332\n",
            "Epoch 108/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 7.7613 - mean_squared_logarithmic_error: 7.7613 - val_loss: 8.0829 - val_mean_squared_logarithmic_error: 8.1249\n",
            "Epoch 109/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 7.6570 - mean_squared_logarithmic_error: 7.6570 - val_loss: 7.9769 - val_mean_squared_logarithmic_error: 8.0186\n",
            "Epoch 110/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 7.5546 - mean_squared_logarithmic_error: 7.5546 - val_loss: 7.8727 - val_mean_squared_logarithmic_error: 7.9141\n",
            "Epoch 111/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.4541 - mean_squared_logarithmic_error: 7.4541 - val_loss: 7.7704 - val_mean_squared_logarithmic_error: 7.8116\n",
            "Epoch 112/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.3553 - mean_squared_logarithmic_error: 7.3553 - val_loss: 7.6700 - val_mean_squared_logarithmic_error: 7.7109\n",
            "Epoch 113/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 7.2584 - mean_squared_logarithmic_error: 7.2584 - val_loss: 7.5713 - val_mean_squared_logarithmic_error: 7.6120\n",
            "Epoch 114/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.1633 - mean_squared_logarithmic_error: 7.1633 - val_loss: 7.4745 - val_mean_squared_logarithmic_error: 7.5149\n",
            "Epoch 115/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.0698 - mean_squared_logarithmic_error: 7.0698 - val_loss: 7.3793 - val_mean_squared_logarithmic_error: 7.4195\n",
            "Epoch 116/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 6.9781 - mean_squared_logarithmic_error: 6.9781 - val_loss: 7.2859 - val_mean_squared_logarithmic_error: 7.3258\n",
            "Epoch 117/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 6.8880 - mean_squared_logarithmic_error: 6.8880 - val_loss: 7.1942 - val_mean_squared_logarithmic_error: 7.2338\n",
            "Epoch 118/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 6.7995 - mean_squared_logarithmic_error: 6.7995 - val_loss: 7.1040 - val_mean_squared_logarithmic_error: 7.1435\n",
            "Epoch 119/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.7126 - mean_squared_logarithmic_error: 6.7126 - val_loss: 7.0155 - val_mean_squared_logarithmic_error: 7.0547\n",
            "Epoch 120/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.6272 - mean_squared_logarithmic_error: 6.6272 - val_loss: 6.9285 - val_mean_squared_logarithmic_error: 6.9675\n",
            "Epoch 121/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.5434 - mean_squared_logarithmic_error: 6.5434 - val_loss: 6.8430 - val_mean_squared_logarithmic_error: 6.8817\n",
            "Epoch 122/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 6.4610 - mean_squared_logarithmic_error: 6.4610 - val_loss: 6.7590 - val_mean_squared_logarithmic_error: 6.7975\n",
            "Epoch 123/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 6.3801 - mean_squared_logarithmic_error: 6.3801 - val_loss: 6.6765 - val_mean_squared_logarithmic_error: 6.7148\n",
            "Epoch 124/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.3007 - mean_squared_logarithmic_error: 6.3007 - val_loss: 6.5954 - val_mean_squared_logarithmic_error: 6.6335\n",
            "Epoch 125/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 6.2226 - mean_squared_logarithmic_error: 6.2226 - val_loss: 6.5157 - val_mean_squared_logarithmic_error: 6.5535\n",
            "Epoch 126/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.1459 - mean_squared_logarithmic_error: 6.1459 - val_loss: 6.4374 - val_mean_squared_logarithmic_error: 6.4750\n",
            "Epoch 127/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.0704 - mean_squared_logarithmic_error: 6.0704 - val_loss: 6.3604 - val_mean_squared_logarithmic_error: 6.3978\n",
            "Epoch 128/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 5.9963 - mean_squared_logarithmic_error: 5.9963 - val_loss: 6.2848 - val_mean_squared_logarithmic_error: 6.3219\n",
            "Epoch 129/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 5.9235 - mean_squared_logarithmic_error: 5.9235 - val_loss: 6.2104 - val_mean_squared_logarithmic_error: 6.2472\n",
            "Epoch 130/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 5.8519 - mean_squared_logarithmic_error: 5.8519 - val_loss: 6.1372 - val_mean_squared_logarithmic_error: 6.1738\n",
            "Epoch 131/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 5.7815 - mean_squared_logarithmic_error: 5.7815 - val_loss: 6.0653 - val_mean_squared_logarithmic_error: 6.1017\n",
            "Epoch 132/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 5.7123 - mean_squared_logarithmic_error: 5.7123 - val_loss: 5.9946 - val_mean_squared_logarithmic_error: 6.0308\n",
            "Epoch 133/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 5.6443 - mean_squared_logarithmic_error: 5.6443 - val_loss: 5.9250 - val_mean_squared_logarithmic_error: 5.9610\n",
            "Epoch 134/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 5.5774 - mean_squared_logarithmic_error: 5.5774 - val_loss: 5.8566 - val_mean_squared_logarithmic_error: 5.8924\n",
            "Epoch 135/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 5.5116 - mean_squared_logarithmic_error: 5.5116 - val_loss: 5.7894 - val_mean_squared_logarithmic_error: 5.8249\n",
            "Epoch 136/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 5.4470 - mean_squared_logarithmic_error: 5.4470 - val_loss: 5.7232 - val_mean_squared_logarithmic_error: 5.7585\n",
            "Epoch 137/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 5.3833 - mean_squared_logarithmic_error: 5.3833 - val_loss: 5.6581 - val_mean_squared_logarithmic_error: 5.6932\n",
            "Epoch 138/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 5.3208 - mean_squared_logarithmic_error: 5.3208 - val_loss: 5.5941 - val_mean_squared_logarithmic_error: 5.6290\n",
            "Epoch 139/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 5.2592 - mean_squared_logarithmic_error: 5.2592 - val_loss: 5.5311 - val_mean_squared_logarithmic_error: 5.5658\n",
            "Epoch 140/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 5.1987 - mean_squared_logarithmic_error: 5.1987 - val_loss: 5.4691 - val_mean_squared_logarithmic_error: 5.5036\n",
            "Epoch 141/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 5.1392 - mean_squared_logarithmic_error: 5.1392 - val_loss: 5.4082 - val_mean_squared_logarithmic_error: 5.4424\n",
            "Epoch 142/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 5.0806 - mean_squared_logarithmic_error: 5.0806 - val_loss: 5.3482 - val_mean_squared_logarithmic_error: 5.3822\n",
            "Epoch 143/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 5.0230 - mean_squared_logarithmic_error: 5.0230 - val_loss: 5.2891 - val_mean_squared_logarithmic_error: 5.3229\n",
            "Epoch 144/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 4.9662 - mean_squared_logarithmic_error: 4.9662 - val_loss: 5.2310 - val_mean_squared_logarithmic_error: 5.2646\n",
            "Epoch 145/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 4.9104 - mean_squared_logarithmic_error: 4.9104 - val_loss: 5.1738 - val_mean_squared_logarithmic_error: 5.2072\n",
            "Epoch 146/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 4.8555 - mean_squared_logarithmic_error: 4.8555 - val_loss: 5.1176 - val_mean_squared_logarithmic_error: 5.1508\n",
            "Epoch 147/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 4.8014 - mean_squared_logarithmic_error: 4.8014 - val_loss: 5.0621 - val_mean_squared_logarithmic_error: 5.0951\n",
            "Epoch 148/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 4.7482 - mean_squared_logarithmic_error: 4.7482 - val_loss: 5.0076 - val_mean_squared_logarithmic_error: 5.0404\n",
            "Epoch 149/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 4.6958 - mean_squared_logarithmic_error: 4.6958 - val_loss: 4.9539 - val_mean_squared_logarithmic_error: 4.9865\n",
            "Epoch 150/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 4.6442 - mean_squared_logarithmic_error: 4.6442 - val_loss: 4.9010 - val_mean_squared_logarithmic_error: 4.9334\n",
            "Epoch 151/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 4.5935 - mean_squared_logarithmic_error: 4.5935 - val_loss: 4.8489 - val_mean_squared_logarithmic_error: 4.8812\n",
            "Epoch 152/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 4.5435 - mean_squared_logarithmic_error: 4.5435 - val_loss: 4.7977 - val_mean_squared_logarithmic_error: 4.8297\n",
            "Epoch 153/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 4.4943 - mean_squared_logarithmic_error: 4.4943 - val_loss: 4.7472 - val_mean_squared_logarithmic_error: 4.7790\n",
            "Epoch 154/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 4.4458 - mean_squared_logarithmic_error: 4.4458 - val_loss: 4.6974 - val_mean_squared_logarithmic_error: 4.7291\n",
            "Epoch 155/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 4.3981 - mean_squared_logarithmic_error: 4.3981 - val_loss: 4.6484 - val_mean_squared_logarithmic_error: 4.6799\n",
            "Epoch 156/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 4.3511 - mean_squared_logarithmic_error: 4.3511 - val_loss: 4.6002 - val_mean_squared_logarithmic_error: 4.6315\n",
            "Epoch 157/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 4.3049 - mean_squared_logarithmic_error: 4.3049 - val_loss: 4.5526 - val_mean_squared_logarithmic_error: 4.5838\n",
            "Epoch 158/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 4.2593 - mean_squared_logarithmic_error: 4.2593 - val_loss: 4.5058 - val_mean_squared_logarithmic_error: 4.5368\n",
            "Epoch 159/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 4.2144 - mean_squared_logarithmic_error: 4.2144 - val_loss: 4.4597 - val_mean_squared_logarithmic_error: 4.4904\n",
            "Epoch 160/200\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 4.1702 - mean_squared_logarithmic_error: 4.1702 - val_loss: 4.4142 - val_mean_squared_logarithmic_error: 4.4448\n",
            "Epoch 161/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 4.1266 - mean_squared_logarithmic_error: 4.1266 - val_loss: 4.3694 - val_mean_squared_logarithmic_error: 4.3998\n",
            "Epoch 162/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 4.0837 - mean_squared_logarithmic_error: 4.0837 - val_loss: 4.3253 - val_mean_squared_logarithmic_error: 4.3555\n",
            "Epoch 163/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 4.0414 - mean_squared_logarithmic_error: 4.0414 - val_loss: 4.2818 - val_mean_squared_logarithmic_error: 4.3119\n",
            "Epoch 164/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 3.9997 - mean_squared_logarithmic_error: 3.9997 - val_loss: 4.2390 - val_mean_squared_logarithmic_error: 4.2688\n",
            "Epoch 165/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 3.9586 - mean_squared_logarithmic_error: 3.9586 - val_loss: 4.1967 - val_mean_squared_logarithmic_error: 4.2264\n",
            "Epoch 166/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 3.9182 - mean_squared_logarithmic_error: 3.9182 - val_loss: 4.1551 - val_mean_squared_logarithmic_error: 4.1846\n",
            "Epoch 167/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 3.8783 - mean_squared_logarithmic_error: 3.8783 - val_loss: 4.1140 - val_mean_squared_logarithmic_error: 4.1434\n",
            "Epoch 168/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 3.8390 - mean_squared_logarithmic_error: 3.8390 - val_loss: 4.0736 - val_mean_squared_logarithmic_error: 4.1028\n",
            "Epoch 169/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 3.8003 - mean_squared_logarithmic_error: 3.8003 - val_loss: 4.0337 - val_mean_squared_logarithmic_error: 4.0627\n",
            "Epoch 170/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 3.7621 - mean_squared_logarithmic_error: 3.7621 - val_loss: 3.9944 - val_mean_squared_logarithmic_error: 4.0233\n",
            "Epoch 171/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 3.7244 - mean_squared_logarithmic_error: 3.7244 - val_loss: 3.9556 - val_mean_squared_logarithmic_error: 3.9843\n",
            "Epoch 172/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 3.6873 - mean_squared_logarithmic_error: 3.6873 - val_loss: 3.9173 - val_mean_squared_logarithmic_error: 3.9459\n",
            "Epoch 173/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 3.6508 - mean_squared_logarithmic_error: 3.6508 - val_loss: 3.8796 - val_mean_squared_logarithmic_error: 3.9080\n",
            "Epoch 174/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 3.6147 - mean_squared_logarithmic_error: 3.6147 - val_loss: 3.8424 - val_mean_squared_logarithmic_error: 3.8707\n",
            "Epoch 175/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 3.5791 - mean_squared_logarithmic_error: 3.5791 - val_loss: 3.8058 - val_mean_squared_logarithmic_error: 3.8339\n",
            "Epoch 176/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 3.5440 - mean_squared_logarithmic_error: 3.5440 - val_loss: 3.7696 - val_mean_squared_logarithmic_error: 3.7975\n",
            "Epoch 177/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 3.5095 - mean_squared_logarithmic_error: 3.5095 - val_loss: 3.7339 - val_mean_squared_logarithmic_error: 3.7617\n",
            "Epoch 178/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 3.4753 - mean_squared_logarithmic_error: 3.4753 - val_loss: 3.6987 - val_mean_squared_logarithmic_error: 3.7263\n",
            "Epoch 179/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 3.4417 - mean_squared_logarithmic_error: 3.4417 - val_loss: 3.6640 - val_mean_squared_logarithmic_error: 3.6914\n",
            "Epoch 180/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 3.4085 - mean_squared_logarithmic_error: 3.4085 - val_loss: 3.6297 - val_mean_squared_logarithmic_error: 3.6570\n",
            "Epoch 181/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 3.3758 - mean_squared_logarithmic_error: 3.3758 - val_loss: 3.5959 - val_mean_squared_logarithmic_error: 3.6231\n",
            "Epoch 182/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 3.3435 - mean_squared_logarithmic_error: 3.3435 - val_loss: 3.5625 - val_mean_squared_logarithmic_error: 3.5896\n",
            "Epoch 183/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 3.3116 - mean_squared_logarithmic_error: 3.3116 - val_loss: 3.5296 - val_mean_squared_logarithmic_error: 3.5565\n",
            "Epoch 184/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 3.2802 - mean_squared_logarithmic_error: 3.2802 - val_loss: 3.4972 - val_mean_squared_logarithmic_error: 3.5239\n",
            "Epoch 185/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 3.2492 - mean_squared_logarithmic_error: 3.2492 - val_loss: 3.4651 - val_mean_squared_logarithmic_error: 3.4917\n",
            "Epoch 186/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 3.2186 - mean_squared_logarithmic_error: 3.2186 - val_loss: 3.4335 - val_mean_squared_logarithmic_error: 3.4599\n",
            "Epoch 187/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 3.1884 - mean_squared_logarithmic_error: 3.1884 - val_loss: 3.4023 - val_mean_squared_logarithmic_error: 3.4286\n",
            "Epoch 188/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 3.1587 - mean_squared_logarithmic_error: 3.1587 - val_loss: 3.3715 - val_mean_squared_logarithmic_error: 3.3976\n",
            "Epoch 189/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 3.1293 - mean_squared_logarithmic_error: 3.1293 - val_loss: 3.3411 - val_mean_squared_logarithmic_error: 3.3671\n",
            "Epoch 190/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 3.1003 - mean_squared_logarithmic_error: 3.1003 - val_loss: 3.3110 - val_mean_squared_logarithmic_error: 3.3369\n",
            "Epoch 191/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 3.0717 - mean_squared_logarithmic_error: 3.0717 - val_loss: 3.2814 - val_mean_squared_logarithmic_error: 3.3071\n",
            "Epoch 192/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 3.0434 - mean_squared_logarithmic_error: 3.0434 - val_loss: 3.2522 - val_mean_squared_logarithmic_error: 3.2777\n",
            "Epoch 193/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 3.0155 - mean_squared_logarithmic_error: 3.0155 - val_loss: 3.2233 - val_mean_squared_logarithmic_error: 3.2487\n",
            "Epoch 194/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 2.9880 - mean_squared_logarithmic_error: 2.9880 - val_loss: 3.1948 - val_mean_squared_logarithmic_error: 3.2201\n",
            "Epoch 195/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 2.9608 - mean_squared_logarithmic_error: 2.9608 - val_loss: 3.1666 - val_mean_squared_logarithmic_error: 3.1918\n",
            "Epoch 196/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 2.9339 - mean_squared_logarithmic_error: 2.9339 - val_loss: 3.1388 - val_mean_squared_logarithmic_error: 3.1639\n",
            "Epoch 197/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 2.9075 - mean_squared_logarithmic_error: 2.9075 - val_loss: 3.1114 - val_mean_squared_logarithmic_error: 3.1363\n",
            "Epoch 198/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 2.8813 - mean_squared_logarithmic_error: 2.8813 - val_loss: 3.0842 - val_mean_squared_logarithmic_error: 3.1090\n",
            "Epoch 199/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 2.8555 - mean_squared_logarithmic_error: 2.8555 - val_loss: 3.0575 - val_mean_squared_logarithmic_error: 3.0821\n",
            "Epoch 200/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 2.8300 - mean_squared_logarithmic_error: 2.8300 - val_loss: 3.0310 - val_mean_squared_logarithmic_error: 3.0556\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "history = model.fit(\n",
        "    X_train, \n",
        "    Y_train, \n",
        "    epochs=200, \n",
        "    batch_size=128,\n",
        "    validation_data = (X_val,Y_val)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "aiumphEsN6Go",
        "outputId": "1e372d8f-306b-4e4b-fb2f-a584fb841a0f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "def plot_history(history, key):\n",
        "  plt.plot(history.history[key])\n",
        "  plt.plot(history.history['val_'+key])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(key)\n",
        "  plt.legend([key, 'val_'+key])\n",
        "  plt.show()\n",
        "# Plot the history\n",
        "plot_history(history, 'mean_squared_logarithmic_error')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions3 = model.predict(x_test)\n"
      ],
      "metadata": {
        "id": "dXApLqpl-01h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test=np.array(y_test).reshape(-1,1)\n"
      ],
      "metadata": {
        "id": "N6f24Km1AUVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores2 = model.evaluate(x_test_scaled, y_test, verbose=0)\n",
        "print('Accuracy on test data: {}% \\nError on test data: {}'.format(scores2[1]*100, (1 - scores2[1])*100))    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JWlK2htB38z",
        "outputId": "1e400475-4f63-4740-8d2d-bac437f66b44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test data: 12248490.625% \n",
            "Error on test data: -12248390.625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting predictions to label\n",
        "test3 = list()\n",
        "for i in range(len(y_test)):\n",
        "    test3.append(y_test[i]) # returns index of maximum value\n"
      ],
      "metadata": {
        "id": "5TdZTMu-_cHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting predictions to label\n",
        "pred3 = list()\n",
        "for i in range(len(predictions3)):\n",
        "    pred3.append(predictions3[i]) # returns index of maximum value\n"
      ],
      "metadata": {
        "id": "DH_21Kys_6uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#comparing test and prediction\n",
        "pd.DataFrame({'Test':test3,'Pred':pred3}).head(50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9284bf70-0cf1-41a1-cb50-c0dcece8a007",
        "id": "9ufHl0ks_cHg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Test         Pred\n",
              "0    [718]  [1681.0208]\n",
              "1    [936]  [1956.8373]\n",
              "2   [1521]  [3371.0774]\n",
              "3    [763]  [1252.6022]\n",
              "4    [659]  [2022.0675]\n",
              "5   [1176]  [1671.6638]\n",
              "6    [981]  [1439.8887]\n",
              "7   [1647]  [2394.8645]\n",
              "8    [595]  [1547.2317]\n",
              "9   [1312]  [3509.6147]\n",
              "10  [1986]  [1675.3438]\n",
              "11  [1218]  [503.08255]\n",
              "12  [1695]  [1646.8145]\n",
              "13  [1307]  [1496.2985]\n",
              "14  [1905]  [1536.2756]\n",
              "15   [787]  [2816.7153]\n",
              "16   [727]  [1940.4526]\n",
              "17  [1660]  [2416.6257]\n",
              "18  [1275]   [3295.519]\n",
              "19  [1358]   [968.4712]\n",
              "20   [833]  [2129.3435]\n",
              "21  [1936]  [1432.3318]\n",
              "22  [1039]  [2774.2185]\n",
              "23   [686]   [989.1469]\n",
              "24   [857]   [2807.865]\n",
              "25  [1871]   [2736.643]\n",
              "26  [1127]  [1994.3031]\n",
              "27  [1193]  [1045.5658]\n",
              "28  [1095]   [2969.205]\n",
              "29   [544]  [1546.0582]\n",
              "30   [805]  [3464.2524]\n",
              "31  [1569]  [1567.6433]\n",
              "32   [553]  [2882.8801]\n",
              "33   [743]  [993.05414]\n",
              "34   [534]   [2385.168]\n",
              "35  [1564]  [3354.9846]\n",
              "36   [720]  [1232.3024]\n",
              "37   [804]  [1919.4962]\n",
              "38  [1832]  [2659.7307]\n",
              "39   [590]   [1643.167]\n",
              "40   [525]   [1959.514]\n",
              "41   [673]  [2612.8484]\n",
              "42  [1433]   [3714.386]\n",
              "43  [1519]  [2710.5117]\n",
              "44  [1397]   [3149.623]\n",
              "45  [1469]  [1587.9554]\n",
              "46   [797]   [973.5452]\n",
              "47  [1853]  [2773.7268]\n",
              "48  [1547]  [1587.6003]\n",
              "49  [1330]  [2444.1936]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1ee372e5-ee3d-45a5-b3ef-ee764c5390e6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Test</th>\n",
              "      <th>Pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[718]</td>\n",
              "      <td>[1681.0208]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[936]</td>\n",
              "      <td>[1956.8373]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[1521]</td>\n",
              "      <td>[3371.0774]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[763]</td>\n",
              "      <td>[1252.6022]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[659]</td>\n",
              "      <td>[2022.0675]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[1176]</td>\n",
              "      <td>[1671.6638]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[981]</td>\n",
              "      <td>[1439.8887]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[1647]</td>\n",
              "      <td>[2394.8645]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[595]</td>\n",
              "      <td>[1547.2317]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[1312]</td>\n",
              "      <td>[3509.6147]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[1986]</td>\n",
              "      <td>[1675.3438]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[1218]</td>\n",
              "      <td>[503.08255]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[1695]</td>\n",
              "      <td>[1646.8145]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>[1307]</td>\n",
              "      <td>[1496.2985]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>[1905]</td>\n",
              "      <td>[1536.2756]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>[787]</td>\n",
              "      <td>[2816.7153]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>[727]</td>\n",
              "      <td>[1940.4526]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>[1660]</td>\n",
              "      <td>[2416.6257]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>[1275]</td>\n",
              "      <td>[3295.519]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>[1358]</td>\n",
              "      <td>[968.4712]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>[833]</td>\n",
              "      <td>[2129.3435]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>[1936]</td>\n",
              "      <td>[1432.3318]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>[1039]</td>\n",
              "      <td>[2774.2185]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>[686]</td>\n",
              "      <td>[989.1469]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>[857]</td>\n",
              "      <td>[2807.865]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>[1871]</td>\n",
              "      <td>[2736.643]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>[1127]</td>\n",
              "      <td>[1994.3031]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>[1193]</td>\n",
              "      <td>[1045.5658]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>[1095]</td>\n",
              "      <td>[2969.205]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>[544]</td>\n",
              "      <td>[1546.0582]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>[805]</td>\n",
              "      <td>[3464.2524]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>[1569]</td>\n",
              "      <td>[1567.6433]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>[553]</td>\n",
              "      <td>[2882.8801]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>[743]</td>\n",
              "      <td>[993.05414]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>[534]</td>\n",
              "      <td>[2385.168]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>[1564]</td>\n",
              "      <td>[3354.9846]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>[720]</td>\n",
              "      <td>[1232.3024]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>[804]</td>\n",
              "      <td>[1919.4962]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>[1832]</td>\n",
              "      <td>[2659.7307]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>[590]</td>\n",
              "      <td>[1643.167]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>[525]</td>\n",
              "      <td>[1959.514]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>[673]</td>\n",
              "      <td>[2612.8484]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>[1433]</td>\n",
              "      <td>[3714.386]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>[1519]</td>\n",
              "      <td>[2710.5117]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>[1397]</td>\n",
              "      <td>[3149.623]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>[1469]</td>\n",
              "      <td>[1587.9554]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>[797]</td>\n",
              "      <td>[973.5452]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>[1853]</td>\n",
              "      <td>[2773.7268]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>[1547]</td>\n",
              "      <td>[1587.6003]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>[1330]</td>\n",
              "      <td>[2444.1936]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1ee372e5-ee3d-45a5-b3ef-ee764c5390e6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1ee372e5-ee3d-45a5-b3ef-ee764c5390e6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1ee372e5-ee3d-45a5-b3ef-ee764c5390e6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WphfDLvEMsD"
      },
      "source": [
        "**4.Predict the profit values on 50_startups dataset for each company using ANN model.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "startups_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QIJj43koWemq",
        "outputId": "9aef9f42-5f6c-4b22-9ba5-7026acf45b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    R&D Spend  Administration  Marketing Spend       State     Profit\n",
              "0   165349.20       136897.80        471784.10    New York  192261.83\n",
              "1   162597.70       151377.59        443898.53  California  191792.06\n",
              "2   153441.51       101145.55        407934.54     Florida  191050.39\n",
              "3   144372.41       118671.85        383199.62    New York  182901.99\n",
              "4   142107.34        91391.77        366168.42     Florida  166187.94\n",
              "5   131876.90        99814.71        362861.36    New York  156991.12\n",
              "6   134615.46       147198.87        127716.82  California  156122.51\n",
              "7   130298.13       145530.06        323876.68     Florida  155752.60\n",
              "8   120542.52       148718.95        311613.29    New York  152211.77\n",
              "9   123334.88       108679.17        304981.62  California  149759.96\n",
              "10  101913.08       110594.11        229160.95     Florida  146121.95\n",
              "11  100671.96        91790.61        249744.55  California  144259.40\n",
              "12   93863.75       127320.38        249839.44     Florida  141585.52\n",
              "13   91992.39       135495.07        252664.93  California  134307.35\n",
              "14  119943.24       156547.42        256512.92     Florida  132602.65\n",
              "15  114523.61       122616.84        261776.23    New York  129917.04\n",
              "16   78013.11       121597.55        264346.06  California  126992.93\n",
              "17   94657.16       145077.58        282574.31    New York  125370.37\n",
              "18   91749.16       114175.79        294919.57     Florida  124266.90\n",
              "19   86419.70       153514.11             0.00    New York  122776.86\n",
              "20   76253.86       113867.30        298664.47  California  118474.03\n",
              "21   78389.47       153773.43        299737.29    New York  111313.02\n",
              "22   73994.56       122782.75        303319.26     Florida  110352.25\n",
              "23   67532.53       105751.03        304768.73     Florida  108733.99\n",
              "24   77044.01        99281.34        140574.81    New York  108552.04\n",
              "25   64664.71       139553.16        137962.62  California  107404.34\n",
              "26   75328.87       144135.98        134050.07     Florida  105733.54\n",
              "27   72107.60       127864.55        353183.81    New York  105008.31\n",
              "28   66051.52       182645.56        118148.20     Florida  103282.38\n",
              "29   65605.48       153032.06        107138.38    New York  101004.64\n",
              "30   61994.48       115641.28         91131.24     Florida   99937.59\n",
              "31   61136.38       152701.92         88218.23    New York   97483.56\n",
              "32   63408.86       129219.61         46085.25  California   97427.84\n",
              "33   55493.95       103057.49        214634.81     Florida   96778.92\n",
              "34   46426.07       157693.92        210797.67  California   96712.80\n",
              "35   46014.02        85047.44        205517.64    New York   96479.51\n",
              "36   28663.76       127056.21        201126.82     Florida   90708.19\n",
              "37   44069.95        51283.14        197029.42  California   89949.14\n",
              "38   20229.59        65947.93        185265.10    New York   81229.06\n",
              "39   38558.51        82982.09        174999.30  California   81005.76\n",
              "40   28754.33       118546.05        172795.67  California   78239.91\n",
              "41   27892.92        84710.77        164470.71     Florida   77798.83\n",
              "42   23640.93        96189.63        148001.11  California   71498.49\n",
              "43   15505.73       127382.30         35534.17    New York   69758.98\n",
              "44   22177.74       154806.14         28334.72  California   65200.33\n",
              "45    1000.23       124153.04          1903.93    New York   64926.08\n",
              "46    1315.46       115816.21        297114.46     Florida   49490.75\n",
              "47       0.00       135426.92             0.00  California   42559.73\n",
              "48     542.05        51743.15             0.00    New York   35673.41\n",
              "49       0.00       116983.80         45173.06  California   14681.40"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0156762c-88a5-41ae-88b6-881e5779c7e3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>R&amp;D Spend</th>\n",
              "      <th>Administration</th>\n",
              "      <th>Marketing Spend</th>\n",
              "      <th>State</th>\n",
              "      <th>Profit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>165349.20</td>\n",
              "      <td>136897.80</td>\n",
              "      <td>471784.10</td>\n",
              "      <td>New York</td>\n",
              "      <td>192261.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>162597.70</td>\n",
              "      <td>151377.59</td>\n",
              "      <td>443898.53</td>\n",
              "      <td>California</td>\n",
              "      <td>191792.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>153441.51</td>\n",
              "      <td>101145.55</td>\n",
              "      <td>407934.54</td>\n",
              "      <td>Florida</td>\n",
              "      <td>191050.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>144372.41</td>\n",
              "      <td>118671.85</td>\n",
              "      <td>383199.62</td>\n",
              "      <td>New York</td>\n",
              "      <td>182901.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>142107.34</td>\n",
              "      <td>91391.77</td>\n",
              "      <td>366168.42</td>\n",
              "      <td>Florida</td>\n",
              "      <td>166187.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>131876.90</td>\n",
              "      <td>99814.71</td>\n",
              "      <td>362861.36</td>\n",
              "      <td>New York</td>\n",
              "      <td>156991.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>134615.46</td>\n",
              "      <td>147198.87</td>\n",
              "      <td>127716.82</td>\n",
              "      <td>California</td>\n",
              "      <td>156122.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>130298.13</td>\n",
              "      <td>145530.06</td>\n",
              "      <td>323876.68</td>\n",
              "      <td>Florida</td>\n",
              "      <td>155752.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>120542.52</td>\n",
              "      <td>148718.95</td>\n",
              "      <td>311613.29</td>\n",
              "      <td>New York</td>\n",
              "      <td>152211.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>123334.88</td>\n",
              "      <td>108679.17</td>\n",
              "      <td>304981.62</td>\n",
              "      <td>California</td>\n",
              "      <td>149759.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>101913.08</td>\n",
              "      <td>110594.11</td>\n",
              "      <td>229160.95</td>\n",
              "      <td>Florida</td>\n",
              "      <td>146121.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>100671.96</td>\n",
              "      <td>91790.61</td>\n",
              "      <td>249744.55</td>\n",
              "      <td>California</td>\n",
              "      <td>144259.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>93863.75</td>\n",
              "      <td>127320.38</td>\n",
              "      <td>249839.44</td>\n",
              "      <td>Florida</td>\n",
              "      <td>141585.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>91992.39</td>\n",
              "      <td>135495.07</td>\n",
              "      <td>252664.93</td>\n",
              "      <td>California</td>\n",
              "      <td>134307.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>119943.24</td>\n",
              "      <td>156547.42</td>\n",
              "      <td>256512.92</td>\n",
              "      <td>Florida</td>\n",
              "      <td>132602.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>114523.61</td>\n",
              "      <td>122616.84</td>\n",
              "      <td>261776.23</td>\n",
              "      <td>New York</td>\n",
              "      <td>129917.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>78013.11</td>\n",
              "      <td>121597.55</td>\n",
              "      <td>264346.06</td>\n",
              "      <td>California</td>\n",
              "      <td>126992.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>94657.16</td>\n",
              "      <td>145077.58</td>\n",
              "      <td>282574.31</td>\n",
              "      <td>New York</td>\n",
              "      <td>125370.37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>91749.16</td>\n",
              "      <td>114175.79</td>\n",
              "      <td>294919.57</td>\n",
              "      <td>Florida</td>\n",
              "      <td>124266.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>86419.70</td>\n",
              "      <td>153514.11</td>\n",
              "      <td>0.00</td>\n",
              "      <td>New York</td>\n",
              "      <td>122776.86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>76253.86</td>\n",
              "      <td>113867.30</td>\n",
              "      <td>298664.47</td>\n",
              "      <td>California</td>\n",
              "      <td>118474.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>78389.47</td>\n",
              "      <td>153773.43</td>\n",
              "      <td>299737.29</td>\n",
              "      <td>New York</td>\n",
              "      <td>111313.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>73994.56</td>\n",
              "      <td>122782.75</td>\n",
              "      <td>303319.26</td>\n",
              "      <td>Florida</td>\n",
              "      <td>110352.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>67532.53</td>\n",
              "      <td>105751.03</td>\n",
              "      <td>304768.73</td>\n",
              "      <td>Florida</td>\n",
              "      <td>108733.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>77044.01</td>\n",
              "      <td>99281.34</td>\n",
              "      <td>140574.81</td>\n",
              "      <td>New York</td>\n",
              "      <td>108552.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>64664.71</td>\n",
              "      <td>139553.16</td>\n",
              "      <td>137962.62</td>\n",
              "      <td>California</td>\n",
              "      <td>107404.34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>75328.87</td>\n",
              "      <td>144135.98</td>\n",
              "      <td>134050.07</td>\n",
              "      <td>Florida</td>\n",
              "      <td>105733.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>72107.60</td>\n",
              "      <td>127864.55</td>\n",
              "      <td>353183.81</td>\n",
              "      <td>New York</td>\n",
              "      <td>105008.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>66051.52</td>\n",
              "      <td>182645.56</td>\n",
              "      <td>118148.20</td>\n",
              "      <td>Florida</td>\n",
              "      <td>103282.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>65605.48</td>\n",
              "      <td>153032.06</td>\n",
              "      <td>107138.38</td>\n",
              "      <td>New York</td>\n",
              "      <td>101004.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>61994.48</td>\n",
              "      <td>115641.28</td>\n",
              "      <td>91131.24</td>\n",
              "      <td>Florida</td>\n",
              "      <td>99937.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>61136.38</td>\n",
              "      <td>152701.92</td>\n",
              "      <td>88218.23</td>\n",
              "      <td>New York</td>\n",
              "      <td>97483.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>63408.86</td>\n",
              "      <td>129219.61</td>\n",
              "      <td>46085.25</td>\n",
              "      <td>California</td>\n",
              "      <td>97427.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>55493.95</td>\n",
              "      <td>103057.49</td>\n",
              "      <td>214634.81</td>\n",
              "      <td>Florida</td>\n",
              "      <td>96778.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>46426.07</td>\n",
              "      <td>157693.92</td>\n",
              "      <td>210797.67</td>\n",
              "      <td>California</td>\n",
              "      <td>96712.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>46014.02</td>\n",
              "      <td>85047.44</td>\n",
              "      <td>205517.64</td>\n",
              "      <td>New York</td>\n",
              "      <td>96479.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>28663.76</td>\n",
              "      <td>127056.21</td>\n",
              "      <td>201126.82</td>\n",
              "      <td>Florida</td>\n",
              "      <td>90708.19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>44069.95</td>\n",
              "      <td>51283.14</td>\n",
              "      <td>197029.42</td>\n",
              "      <td>California</td>\n",
              "      <td>89949.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>20229.59</td>\n",
              "      <td>65947.93</td>\n",
              "      <td>185265.10</td>\n",
              "      <td>New York</td>\n",
              "      <td>81229.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>38558.51</td>\n",
              "      <td>82982.09</td>\n",
              "      <td>174999.30</td>\n",
              "      <td>California</td>\n",
              "      <td>81005.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>28754.33</td>\n",
              "      <td>118546.05</td>\n",
              "      <td>172795.67</td>\n",
              "      <td>California</td>\n",
              "      <td>78239.91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>27892.92</td>\n",
              "      <td>84710.77</td>\n",
              "      <td>164470.71</td>\n",
              "      <td>Florida</td>\n",
              "      <td>77798.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>23640.93</td>\n",
              "      <td>96189.63</td>\n",
              "      <td>148001.11</td>\n",
              "      <td>California</td>\n",
              "      <td>71498.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>15505.73</td>\n",
              "      <td>127382.30</td>\n",
              "      <td>35534.17</td>\n",
              "      <td>New York</td>\n",
              "      <td>69758.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>22177.74</td>\n",
              "      <td>154806.14</td>\n",
              "      <td>28334.72</td>\n",
              "      <td>California</td>\n",
              "      <td>65200.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>1000.23</td>\n",
              "      <td>124153.04</td>\n",
              "      <td>1903.93</td>\n",
              "      <td>New York</td>\n",
              "      <td>64926.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>1315.46</td>\n",
              "      <td>115816.21</td>\n",
              "      <td>297114.46</td>\n",
              "      <td>Florida</td>\n",
              "      <td>49490.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.00</td>\n",
              "      <td>135426.92</td>\n",
              "      <td>0.00</td>\n",
              "      <td>California</td>\n",
              "      <td>42559.73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>542.05</td>\n",
              "      <td>51743.15</td>\n",
              "      <td>0.00</td>\n",
              "      <td>New York</td>\n",
              "      <td>35673.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.00</td>\n",
              "      <td>116983.80</td>\n",
              "      <td>45173.06</td>\n",
              "      <td>California</td>\n",
              "      <td>14681.40</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0156762c-88a5-41ae-88b6-881e5779c7e3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0156762c-88a5-41ae-88b6-881e5779c7e3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0156762c-88a5-41ae-88b6-881e5779c7e3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvukHQAcRs8b"
      },
      "outputs": [],
      "source": [
        "# dividing data into train set and test set\n",
        "train_data4 ,test_data4 = train_test_split(startups_df, test_size=0.2, random_state=25)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0tC2XwhRs8e"
      },
      "outputs": [],
      "source": [
        "#function to normalize the data\n",
        "def scale_datasets(x_train, x_test):\n",
        "  standard_scaler = StandardScaler()\n",
        "\n",
        "  x_train_scaled = pd.DataFrame(standard_scaler.fit_transform(x_train),columns=x_train.columns)\n",
        "  x_test_scaled = pd.DataFrame(standard_scaler.transform(x_test),columns = x_test.columns)\n",
        "  \n",
        "  return x_train_scaled, x_test_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bEFGBaARs8f"
      },
      "outputs": [],
      "source": [
        "x_train4, y_train4 = train_data4.drop(['Profit','State'], axis = 1), train_data4['Profit']\n",
        "x_test4, y_test4 = test_data4.drop(['Profit','State'],axis=1), test_data4['Profit']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYrmhhSBRs8g"
      },
      "outputs": [],
      "source": [
        "x_train_scaled_all4, x_test_scaled_all4= scale_datasets(x_train4, x_test4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geN7g4HVRs8h",
        "outputId": "4fe94572-e1cc-487e-b714-eb5f8dd7d5fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 3)\n",
            "(32,)\n",
            "(8, 3)\n"
          ]
        }
      ],
      "source": [
        "# Dividing scalled train data into train and validation data\n",
        "\n",
        "X_train4, X_val4, Y_train4, Y_val4 = train_test_split(x_train_scaled_all4, y_train4, test_size=0.20, random_state=40)\n",
        "print(X_train4.shape)\n",
        "print(Y_train4.shape)\n",
        "print(X_val4.shape)\n",
        "#validation data is not test data...\n",
        "#validation data is to check whether during training whether our training is happening successfully or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kkfs6dcFRs8i"
      },
      "outputs": [],
      "source": [
        "hidden_units1 = 100\n",
        "hidden_units2 = 80\n",
        "hidden_units3 = 70\n",
        "learning_rate = 0.0001\n",
        "# Creating model using the Sequential in tensorflow\n",
        "#Sequential() meaning each layer will be added one after another\n",
        "def build_model_using_sequential():\n",
        "  model4 = Sequential([\n",
        "    Dense(hidden_units1, input_dim=3, kernel_initializer='normal', activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(hidden_units2, kernel_initializer='normal', activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(hidden_units3, kernel_initializer='normal', activation='relu'),\n",
        "    #Dropout(0.2),\n",
        "\n",
        "    Dense(1, kernel_initializer='normal',activation='linear')\n",
        "  ])\n",
        "  return model4\n",
        "# build the model\n",
        "model4 = build_model_using_sequential()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsymJ6AKRs8j"
      },
      "outputs": [],
      "source": [
        "# loss function\n",
        "mse = MeanSquaredLogarithmicError()\n",
        "model4.compile(\n",
        "    loss=mse, \n",
        "    optimizer=SGD(learning_rate = learning_rate), \n",
        "    metrics=[mse]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yi0aKcXRs8j",
        "outputId": "123dbe27-7672-47fd-9a0f-d81866ac02ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "2/2 [==============================] - 1s 178ms/step - loss: 132.9253 - mean_squared_logarithmic_error: 132.9253 - val_loss: 138.7764 - val_mean_squared_logarithmic_error: 138.7764\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 132.9266 - mean_squared_logarithmic_error: 132.9266 - val_loss: 138.7764 - val_mean_squared_logarithmic_error: 138.7764\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 132.9262 - mean_squared_logarithmic_error: 132.9262 - val_loss: 138.7764 - val_mean_squared_logarithmic_error: 138.7764\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 132.9263 - mean_squared_logarithmic_error: 132.9263 - val_loss: 138.7764 - val_mean_squared_logarithmic_error: 138.7764\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 132.9254 - mean_squared_logarithmic_error: 132.9254 - val_loss: 138.7764 - val_mean_squared_logarithmic_error: 138.7764\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 132.9253 - mean_squared_logarithmic_error: 132.9253 - val_loss: 138.7762 - val_mean_squared_logarithmic_error: 138.7762\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 132.9243 - mean_squared_logarithmic_error: 132.9243 - val_loss: 138.7738 - val_mean_squared_logarithmic_error: 138.7738\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 132.9120 - mean_squared_logarithmic_error: 132.9120 - val_loss: 138.7542 - val_mean_squared_logarithmic_error: 138.7542\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 132.8728 - mean_squared_logarithmic_error: 132.8728 - val_loss: 138.6772 - val_mean_squared_logarithmic_error: 138.6772\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 132.7957 - mean_squared_logarithmic_error: 132.7957 - val_loss: 138.5700 - val_mean_squared_logarithmic_error: 138.5700\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 132.6913 - mean_squared_logarithmic_error: 132.6913 - val_loss: 138.4526 - val_mean_squared_logarithmic_error: 138.4526\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 132.5902 - mean_squared_logarithmic_error: 132.5902 - val_loss: 138.3369 - val_mean_squared_logarithmic_error: 138.3369\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 132.4749 - mean_squared_logarithmic_error: 132.4749 - val_loss: 138.2230 - val_mean_squared_logarithmic_error: 138.2230\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 132.3652 - mean_squared_logarithmic_error: 132.3652 - val_loss: 138.1102 - val_mean_squared_logarithmic_error: 138.1102\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 132.2435 - mean_squared_logarithmic_error: 132.2435 - val_loss: 137.9987 - val_mean_squared_logarithmic_error: 137.9987\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 132.1279 - mean_squared_logarithmic_error: 132.1279 - val_loss: 137.8886 - val_mean_squared_logarithmic_error: 137.8886\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 132.0408 - mean_squared_logarithmic_error: 132.0408 - val_loss: 137.7809 - val_mean_squared_logarithmic_error: 137.7809\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 131.9316 - mean_squared_logarithmic_error: 131.9316 - val_loss: 137.6735 - val_mean_squared_logarithmic_error: 137.6735\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 131.8215 - mean_squared_logarithmic_error: 131.8215 - val_loss: 137.5676 - val_mean_squared_logarithmic_error: 137.5676\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 131.7166 - mean_squared_logarithmic_error: 131.7166 - val_loss: 137.4628 - val_mean_squared_logarithmic_error: 137.4628\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 131.6155 - mean_squared_logarithmic_error: 131.6155 - val_loss: 137.3587 - val_mean_squared_logarithmic_error: 137.3587\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 131.5117 - mean_squared_logarithmic_error: 131.5117 - val_loss: 137.2558 - val_mean_squared_logarithmic_error: 137.2558\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 131.4241 - mean_squared_logarithmic_error: 131.4241 - val_loss: 137.1535 - val_mean_squared_logarithmic_error: 137.1535\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 131.3185 - mean_squared_logarithmic_error: 131.3185 - val_loss: 137.0525 - val_mean_squared_logarithmic_error: 137.0525\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 131.2113 - mean_squared_logarithmic_error: 131.2113 - val_loss: 136.9523 - val_mean_squared_logarithmic_error: 136.9523\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 131.1098 - mean_squared_logarithmic_error: 131.1098 - val_loss: 136.8533 - val_mean_squared_logarithmic_error: 136.8533\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 131.0201 - mean_squared_logarithmic_error: 131.0201 - val_loss: 136.7551 - val_mean_squared_logarithmic_error: 136.7551\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 130.9248 - mean_squared_logarithmic_error: 130.9248 - val_loss: 136.6577 - val_mean_squared_logarithmic_error: 136.6577\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 130.8322 - mean_squared_logarithmic_error: 130.8322 - val_loss: 136.5614 - val_mean_squared_logarithmic_error: 136.5614\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 130.7412 - mean_squared_logarithmic_error: 130.7412 - val_loss: 136.4659 - val_mean_squared_logarithmic_error: 136.4659\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 130.6381 - mean_squared_logarithmic_error: 130.6381 - val_loss: 136.3709 - val_mean_squared_logarithmic_error: 136.3709\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 130.5561 - mean_squared_logarithmic_error: 130.5561 - val_loss: 136.2770 - val_mean_squared_logarithmic_error: 136.2770\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 130.4680 - mean_squared_logarithmic_error: 130.4680 - val_loss: 136.1839 - val_mean_squared_logarithmic_error: 136.1839\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 130.3635 - mean_squared_logarithmic_error: 130.3635 - val_loss: 136.0916 - val_mean_squared_logarithmic_error: 136.0916\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 130.2812 - mean_squared_logarithmic_error: 130.2812 - val_loss: 136.0000 - val_mean_squared_logarithmic_error: 136.0000\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 130.1837 - mean_squared_logarithmic_error: 130.1837 - val_loss: 135.9094 - val_mean_squared_logarithmic_error: 135.9094\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 130.1129 - mean_squared_logarithmic_error: 130.1129 - val_loss: 135.8193 - val_mean_squared_logarithmic_error: 135.8193\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 130.0140 - mean_squared_logarithmic_error: 130.0140 - val_loss: 135.7301 - val_mean_squared_logarithmic_error: 135.7301\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 129.9196 - mean_squared_logarithmic_error: 129.9196 - val_loss: 135.6416 - val_mean_squared_logarithmic_error: 135.6416\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 129.8397 - mean_squared_logarithmic_error: 129.8397 - val_loss: 135.5538 - val_mean_squared_logarithmic_error: 135.5538\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 129.7537 - mean_squared_logarithmic_error: 129.7537 - val_loss: 135.4666 - val_mean_squared_logarithmic_error: 135.4666\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 129.6644 - mean_squared_logarithmic_error: 129.6644 - val_loss: 135.3800 - val_mean_squared_logarithmic_error: 135.3800\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 129.5838 - mean_squared_logarithmic_error: 129.5838 - val_loss: 135.2941 - val_mean_squared_logarithmic_error: 135.2941\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 129.4918 - mean_squared_logarithmic_error: 129.4918 - val_loss: 135.2088 - val_mean_squared_logarithmic_error: 135.2088\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 129.4119 - mean_squared_logarithmic_error: 129.4119 - val_loss: 135.1241 - val_mean_squared_logarithmic_error: 135.1241\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 129.3282 - mean_squared_logarithmic_error: 129.3282 - val_loss: 135.0398 - val_mean_squared_logarithmic_error: 135.0398\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 129.2559 - mean_squared_logarithmic_error: 129.2559 - val_loss: 134.9560 - val_mean_squared_logarithmic_error: 134.9560\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 129.1679 - mean_squared_logarithmic_error: 129.1679 - val_loss: 134.8727 - val_mean_squared_logarithmic_error: 134.8727\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 129.0854 - mean_squared_logarithmic_error: 129.0854 - val_loss: 134.7902 - val_mean_squared_logarithmic_error: 134.7902\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 128.9979 - mean_squared_logarithmic_error: 128.9979 - val_loss: 134.7084 - val_mean_squared_logarithmic_error: 134.7084\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 128.9212 - mean_squared_logarithmic_error: 128.9212 - val_loss: 134.6268 - val_mean_squared_logarithmic_error: 134.6268\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 128.8359 - mean_squared_logarithmic_error: 128.8359 - val_loss: 134.5458 - val_mean_squared_logarithmic_error: 134.5458\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 128.7503 - mean_squared_logarithmic_error: 128.7503 - val_loss: 134.4654 - val_mean_squared_logarithmic_error: 134.4654\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 128.6856 - mean_squared_logarithmic_error: 128.6856 - val_loss: 134.3856 - val_mean_squared_logarithmic_error: 134.3856\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 128.6064 - mean_squared_logarithmic_error: 128.6064 - val_loss: 134.3064 - val_mean_squared_logarithmic_error: 134.3064\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 128.5289 - mean_squared_logarithmic_error: 128.5289 - val_loss: 134.2276 - val_mean_squared_logarithmic_error: 134.2276\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 128.4564 - mean_squared_logarithmic_error: 128.4564 - val_loss: 134.1494 - val_mean_squared_logarithmic_error: 134.1494\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 128.3733 - mean_squared_logarithmic_error: 128.3733 - val_loss: 134.0717 - val_mean_squared_logarithmic_error: 134.0717\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 128.2978 - mean_squared_logarithmic_error: 128.2978 - val_loss: 133.9946 - val_mean_squared_logarithmic_error: 133.9946\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 128.2283 - mean_squared_logarithmic_error: 128.2283 - val_loss: 133.9177 - val_mean_squared_logarithmic_error: 133.9177\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 128.1574 - mean_squared_logarithmic_error: 128.1574 - val_loss: 133.8415 - val_mean_squared_logarithmic_error: 133.8415\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 128.0781 - mean_squared_logarithmic_error: 128.0781 - val_loss: 133.7659 - val_mean_squared_logarithmic_error: 133.7659\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 128.0027 - mean_squared_logarithmic_error: 128.0027 - val_loss: 133.6908 - val_mean_squared_logarithmic_error: 133.6908\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 127.9249 - mean_squared_logarithmic_error: 127.9249 - val_loss: 133.6159 - val_mean_squared_logarithmic_error: 133.6159\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 127.8481 - mean_squared_logarithmic_error: 127.8481 - val_loss: 133.5416 - val_mean_squared_logarithmic_error: 133.5416\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 127.7882 - mean_squared_logarithmic_error: 127.7882 - val_loss: 133.4677 - val_mean_squared_logarithmic_error: 133.4677\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 127.7080 - mean_squared_logarithmic_error: 127.7080 - val_loss: 133.3943 - val_mean_squared_logarithmic_error: 133.3943\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 127.6372 - mean_squared_logarithmic_error: 127.6372 - val_loss: 133.3214 - val_mean_squared_logarithmic_error: 133.3214\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 127.5684 - mean_squared_logarithmic_error: 127.5684 - val_loss: 133.2489 - val_mean_squared_logarithmic_error: 133.2489\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 127.4996 - mean_squared_logarithmic_error: 127.4996 - val_loss: 133.1770 - val_mean_squared_logarithmic_error: 133.1770\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 127.4291 - mean_squared_logarithmic_error: 127.4291 - val_loss: 133.1053 - val_mean_squared_logarithmic_error: 133.1053\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 127.3606 - mean_squared_logarithmic_error: 127.3606 - val_loss: 133.0343 - val_mean_squared_logarithmic_error: 133.0343\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 127.3007 - mean_squared_logarithmic_error: 127.3007 - val_loss: 132.9636 - val_mean_squared_logarithmic_error: 132.9636\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 127.2182 - mean_squared_logarithmic_error: 127.2182 - val_loss: 132.8933 - val_mean_squared_logarithmic_error: 132.8933\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 127.1599 - mean_squared_logarithmic_error: 127.1599 - val_loss: 132.8235 - val_mean_squared_logarithmic_error: 132.8235\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 127.0752 - mean_squared_logarithmic_error: 127.0752 - val_loss: 132.7541 - val_mean_squared_logarithmic_error: 132.7541\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 127.0161 - mean_squared_logarithmic_error: 127.0161 - val_loss: 132.6852 - val_mean_squared_logarithmic_error: 132.6852\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 126.9529 - mean_squared_logarithmic_error: 126.9529 - val_loss: 132.6167 - val_mean_squared_logarithmic_error: 132.6167\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 126.8787 - mean_squared_logarithmic_error: 126.8787 - val_loss: 132.5485 - val_mean_squared_logarithmic_error: 132.5485\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 126.8122 - mean_squared_logarithmic_error: 126.8122 - val_loss: 132.4809 - val_mean_squared_logarithmic_error: 132.4809\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 126.7476 - mean_squared_logarithmic_error: 126.7476 - val_loss: 132.4137 - val_mean_squared_logarithmic_error: 132.4137\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 126.6809 - mean_squared_logarithmic_error: 126.6809 - val_loss: 132.3468 - val_mean_squared_logarithmic_error: 132.3468\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 126.6157 - mean_squared_logarithmic_error: 126.6157 - val_loss: 132.2804 - val_mean_squared_logarithmic_error: 132.2804\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 126.5423 - mean_squared_logarithmic_error: 126.5423 - val_loss: 132.2144 - val_mean_squared_logarithmic_error: 132.2144\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 126.4956 - mean_squared_logarithmic_error: 126.4956 - val_loss: 132.1487 - val_mean_squared_logarithmic_error: 132.1487\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 126.4360 - mean_squared_logarithmic_error: 126.4360 - val_loss: 132.0835 - val_mean_squared_logarithmic_error: 132.0835\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 126.3591 - mean_squared_logarithmic_error: 126.3591 - val_loss: 132.0186 - val_mean_squared_logarithmic_error: 132.0186\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 126.2902 - mean_squared_logarithmic_error: 126.2902 - val_loss: 131.9541 - val_mean_squared_logarithmic_error: 131.9541\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 126.2338 - mean_squared_logarithmic_error: 126.2338 - val_loss: 131.8900 - val_mean_squared_logarithmic_error: 131.8900\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 126.1668 - mean_squared_logarithmic_error: 126.1668 - val_loss: 131.8262 - val_mean_squared_logarithmic_error: 131.8262\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 126.1175 - mean_squared_logarithmic_error: 126.1175 - val_loss: 131.7628 - val_mean_squared_logarithmic_error: 131.7628\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 126.0434 - mean_squared_logarithmic_error: 126.0434 - val_loss: 131.6997 - val_mean_squared_logarithmic_error: 131.6997\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 125.9887 - mean_squared_logarithmic_error: 125.9887 - val_loss: 131.6370 - val_mean_squared_logarithmic_error: 131.6370\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 125.9186 - mean_squared_logarithmic_error: 125.9186 - val_loss: 131.5745 - val_mean_squared_logarithmic_error: 131.5745\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 125.8565 - mean_squared_logarithmic_error: 125.8565 - val_loss: 131.5125 - val_mean_squared_logarithmic_error: 131.5125\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 125.7977 - mean_squared_logarithmic_error: 125.7977 - val_loss: 131.4507 - val_mean_squared_logarithmic_error: 131.4507\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 125.7458 - mean_squared_logarithmic_error: 125.7458 - val_loss: 131.3893 - val_mean_squared_logarithmic_error: 131.3893\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 125.6811 - mean_squared_logarithmic_error: 125.6811 - val_loss: 131.3282 - val_mean_squared_logarithmic_error: 131.3282\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 125.6193 - mean_squared_logarithmic_error: 125.6193 - val_loss: 131.2674 - val_mean_squared_logarithmic_error: 131.2674\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 125.5529 - mean_squared_logarithmic_error: 125.5529 - val_loss: 131.2070 - val_mean_squared_logarithmic_error: 131.2070\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 125.5041 - mean_squared_logarithmic_error: 125.5041 - val_loss: 131.1469 - val_mean_squared_logarithmic_error: 131.1469\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 125.4461 - mean_squared_logarithmic_error: 125.4461 - val_loss: 131.0871 - val_mean_squared_logarithmic_error: 131.0871\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 125.3879 - mean_squared_logarithmic_error: 125.3879 - val_loss: 131.0276 - val_mean_squared_logarithmic_error: 131.0276\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 125.3311 - mean_squared_logarithmic_error: 125.3311 - val_loss: 130.9684 - val_mean_squared_logarithmic_error: 130.9684\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 125.2628 - mean_squared_logarithmic_error: 125.2628 - val_loss: 130.9095 - val_mean_squared_logarithmic_error: 130.9095\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 125.2153 - mean_squared_logarithmic_error: 125.2153 - val_loss: 130.8509 - val_mean_squared_logarithmic_error: 130.8509\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 125.1575 - mean_squared_logarithmic_error: 125.1575 - val_loss: 130.7926 - val_mean_squared_logarithmic_error: 130.7926\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 125.0973 - mean_squared_logarithmic_error: 125.0973 - val_loss: 130.7347 - val_mean_squared_logarithmic_error: 130.7347\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 125.0442 - mean_squared_logarithmic_error: 125.0442 - val_loss: 130.6770 - val_mean_squared_logarithmic_error: 130.6770\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 124.9853 - mean_squared_logarithmic_error: 124.9853 - val_loss: 130.6195 - val_mean_squared_logarithmic_error: 130.6195\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 124.9297 - mean_squared_logarithmic_error: 124.9297 - val_loss: 130.5623 - val_mean_squared_logarithmic_error: 130.5623\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 124.8770 - mean_squared_logarithmic_error: 124.8770 - val_loss: 130.5055 - val_mean_squared_logarithmic_error: 130.5055\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 124.8195 - mean_squared_logarithmic_error: 124.8195 - val_loss: 130.4489 - val_mean_squared_logarithmic_error: 130.4489\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 124.7538 - mean_squared_logarithmic_error: 124.7538 - val_loss: 130.3927 - val_mean_squared_logarithmic_error: 130.3927\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 124.7049 - mean_squared_logarithmic_error: 124.7049 - val_loss: 130.3366 - val_mean_squared_logarithmic_error: 130.3366\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 124.6576 - mean_squared_logarithmic_error: 124.6576 - val_loss: 130.2809 - val_mean_squared_logarithmic_error: 130.2809\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 124.5990 - mean_squared_logarithmic_error: 124.5990 - val_loss: 130.2254 - val_mean_squared_logarithmic_error: 130.2254\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 124.5410 - mean_squared_logarithmic_error: 124.5410 - val_loss: 130.1702 - val_mean_squared_logarithmic_error: 130.1702\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 124.4946 - mean_squared_logarithmic_error: 124.4946 - val_loss: 130.1152 - val_mean_squared_logarithmic_error: 130.1152\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 124.4392 - mean_squared_logarithmic_error: 124.4392 - val_loss: 130.0605 - val_mean_squared_logarithmic_error: 130.0605\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 124.3779 - mean_squared_logarithmic_error: 124.3779 - val_loss: 130.0060 - val_mean_squared_logarithmic_error: 130.0060\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 124.3296 - mean_squared_logarithmic_error: 124.3296 - val_loss: 129.9518 - val_mean_squared_logarithmic_error: 129.9518\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 124.2816 - mean_squared_logarithmic_error: 124.2816 - val_loss: 129.8979 - val_mean_squared_logarithmic_error: 129.8979\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 124.2139 - mean_squared_logarithmic_error: 124.2139 - val_loss: 129.8442 - val_mean_squared_logarithmic_error: 129.8442\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 124.1767 - mean_squared_logarithmic_error: 124.1767 - val_loss: 129.7907 - val_mean_squared_logarithmic_error: 129.7907\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 124.1258 - mean_squared_logarithmic_error: 124.1258 - val_loss: 129.7375 - val_mean_squared_logarithmic_error: 129.7375\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 124.0621 - mean_squared_logarithmic_error: 124.0621 - val_loss: 129.6846 - val_mean_squared_logarithmic_error: 129.6846\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 124.0094 - mean_squared_logarithmic_error: 124.0094 - val_loss: 129.6319 - val_mean_squared_logarithmic_error: 129.6319\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 123.9562 - mean_squared_logarithmic_error: 123.9562 - val_loss: 129.5794 - val_mean_squared_logarithmic_error: 129.5794\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 123.9139 - mean_squared_logarithmic_error: 123.9139 - val_loss: 129.5271 - val_mean_squared_logarithmic_error: 129.5271\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 123.8593 - mean_squared_logarithmic_error: 123.8593 - val_loss: 129.4751 - val_mean_squared_logarithmic_error: 129.4751\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 123.8080 - mean_squared_logarithmic_error: 123.8080 - val_loss: 129.4233 - val_mean_squared_logarithmic_error: 129.4233\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 123.7626 - mean_squared_logarithmic_error: 123.7626 - val_loss: 129.3718 - val_mean_squared_logarithmic_error: 129.3718\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 123.7247 - mean_squared_logarithmic_error: 123.7247 - val_loss: 129.3204 - val_mean_squared_logarithmic_error: 129.3204\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 123.6558 - mean_squared_logarithmic_error: 123.6558 - val_loss: 129.2693 - val_mean_squared_logarithmic_error: 129.2693\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 123.5983 - mean_squared_logarithmic_error: 123.5983 - val_loss: 129.2184 - val_mean_squared_logarithmic_error: 129.2184\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 123.5559 - mean_squared_logarithmic_error: 123.5559 - val_loss: 129.1676 - val_mean_squared_logarithmic_error: 129.1676\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 123.4992 - mean_squared_logarithmic_error: 123.4992 - val_loss: 129.1171 - val_mean_squared_logarithmic_error: 129.1171\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 123.4634 - mean_squared_logarithmic_error: 123.4634 - val_loss: 129.0669 - val_mean_squared_logarithmic_error: 129.0669\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 123.4119 - mean_squared_logarithmic_error: 123.4119 - val_loss: 129.0168 - val_mean_squared_logarithmic_error: 129.0168\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 123.3574 - mean_squared_logarithmic_error: 123.3574 - val_loss: 128.9669 - val_mean_squared_logarithmic_error: 128.9669\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 123.3120 - mean_squared_logarithmic_error: 123.3120 - val_loss: 128.9172 - val_mean_squared_logarithmic_error: 128.9172\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 123.2642 - mean_squared_logarithmic_error: 123.2642 - val_loss: 128.8678 - val_mean_squared_logarithmic_error: 128.8678\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 123.2216 - mean_squared_logarithmic_error: 123.2216 - val_loss: 128.8186 - val_mean_squared_logarithmic_error: 128.8186\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 123.1694 - mean_squared_logarithmic_error: 123.1694 - val_loss: 128.7696 - val_mean_squared_logarithmic_error: 128.7696\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 123.1128 - mean_squared_logarithmic_error: 123.1128 - val_loss: 128.7207 - val_mean_squared_logarithmic_error: 128.7207\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 123.0682 - mean_squared_logarithmic_error: 123.0682 - val_loss: 128.6721 - val_mean_squared_logarithmic_error: 128.6721\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 123.0293 - mean_squared_logarithmic_error: 123.0293 - val_loss: 128.6236 - val_mean_squared_logarithmic_error: 128.6236\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 122.9740 - mean_squared_logarithmic_error: 122.9740 - val_loss: 128.5754 - val_mean_squared_logarithmic_error: 128.5754\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 122.9315 - mean_squared_logarithmic_error: 122.9315 - val_loss: 128.5273 - val_mean_squared_logarithmic_error: 128.5273\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 122.8856 - mean_squared_logarithmic_error: 122.8856 - val_loss: 128.4795 - val_mean_squared_logarithmic_error: 128.4795\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 122.8352 - mean_squared_logarithmic_error: 122.8352 - val_loss: 128.4318 - val_mean_squared_logarithmic_error: 128.4318\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 122.7891 - mean_squared_logarithmic_error: 122.7891 - val_loss: 128.3843 - val_mean_squared_logarithmic_error: 128.3843\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 122.7442 - mean_squared_logarithmic_error: 122.7442 - val_loss: 128.3370 - val_mean_squared_logarithmic_error: 128.3370\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 122.7029 - mean_squared_logarithmic_error: 122.7029 - val_loss: 128.2899 - val_mean_squared_logarithmic_error: 128.2899\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 122.6461 - mean_squared_logarithmic_error: 122.6461 - val_loss: 128.2430 - val_mean_squared_logarithmic_error: 128.2430\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 122.6056 - mean_squared_logarithmic_error: 122.6056 - val_loss: 128.1962 - val_mean_squared_logarithmic_error: 128.1962\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 122.5657 - mean_squared_logarithmic_error: 122.5657 - val_loss: 128.1496 - val_mean_squared_logarithmic_error: 128.1496\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 122.5170 - mean_squared_logarithmic_error: 122.5170 - val_loss: 128.1032 - val_mean_squared_logarithmic_error: 128.1032\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 122.4646 - mean_squared_logarithmic_error: 122.4646 - val_loss: 128.0570 - val_mean_squared_logarithmic_error: 128.0570\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 122.4151 - mean_squared_logarithmic_error: 122.4151 - val_loss: 128.0109 - val_mean_squared_logarithmic_error: 128.0109\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 122.3793 - mean_squared_logarithmic_error: 122.3793 - val_loss: 127.9650 - val_mean_squared_logarithmic_error: 127.9650\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 122.3287 - mean_squared_logarithmic_error: 122.3287 - val_loss: 127.9193 - val_mean_squared_logarithmic_error: 127.9193\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 122.2917 - mean_squared_logarithmic_error: 122.2917 - val_loss: 127.8737 - val_mean_squared_logarithmic_error: 127.8737\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 122.2330 - mean_squared_logarithmic_error: 122.2330 - val_loss: 127.8283 - val_mean_squared_logarithmic_error: 127.8283\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 122.1949 - mean_squared_logarithmic_error: 122.1949 - val_loss: 127.7831 - val_mean_squared_logarithmic_error: 127.7831\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 122.1537 - mean_squared_logarithmic_error: 122.1537 - val_loss: 127.7381 - val_mean_squared_logarithmic_error: 127.7381\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 122.1139 - mean_squared_logarithmic_error: 122.1139 - val_loss: 127.6932 - val_mean_squared_logarithmic_error: 127.6932\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 122.0656 - mean_squared_logarithmic_error: 122.0656 - val_loss: 127.6485 - val_mean_squared_logarithmic_error: 127.6485\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 122.0154 - mean_squared_logarithmic_error: 122.0154 - val_loss: 127.6040 - val_mean_squared_logarithmic_error: 127.6040\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 121.9827 - mean_squared_logarithmic_error: 121.9827 - val_loss: 127.5596 - val_mean_squared_logarithmic_error: 127.5596\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 121.9398 - mean_squared_logarithmic_error: 121.9398 - val_loss: 127.5153 - val_mean_squared_logarithmic_error: 127.5153\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 121.8946 - mean_squared_logarithmic_error: 121.8946 - val_loss: 127.4712 - val_mean_squared_logarithmic_error: 127.4712\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 121.8505 - mean_squared_logarithmic_error: 121.8505 - val_loss: 127.4273 - val_mean_squared_logarithmic_error: 127.4273\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 121.8086 - mean_squared_logarithmic_error: 121.8086 - val_loss: 127.3835 - val_mean_squared_logarithmic_error: 127.3835\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 121.7662 - mean_squared_logarithmic_error: 121.7662 - val_loss: 127.3399 - val_mean_squared_logarithmic_error: 127.3399\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 121.7170 - mean_squared_logarithmic_error: 121.7170 - val_loss: 127.2964 - val_mean_squared_logarithmic_error: 127.2964\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 121.6777 - mean_squared_logarithmic_error: 121.6777 - val_loss: 127.2531 - val_mean_squared_logarithmic_error: 127.2531\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 121.6309 - mean_squared_logarithmic_error: 121.6309 - val_loss: 127.2099 - val_mean_squared_logarithmic_error: 127.2099\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 121.6005 - mean_squared_logarithmic_error: 121.6005 - val_loss: 127.1669 - val_mean_squared_logarithmic_error: 127.1669\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 121.5559 - mean_squared_logarithmic_error: 121.5559 - val_loss: 127.1240 - val_mean_squared_logarithmic_error: 127.1240\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 121.5147 - mean_squared_logarithmic_error: 121.5147 - val_loss: 127.0813 - val_mean_squared_logarithmic_error: 127.0813\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 121.4601 - mean_squared_logarithmic_error: 121.4601 - val_loss: 127.0386 - val_mean_squared_logarithmic_error: 127.0386\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 121.4286 - mean_squared_logarithmic_error: 121.4286 - val_loss: 126.9961 - val_mean_squared_logarithmic_error: 126.9961\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 121.3705 - mean_squared_logarithmic_error: 121.3705 - val_loss: 126.9538 - val_mean_squared_logarithmic_error: 126.9538\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 121.3468 - mean_squared_logarithmic_error: 121.3468 - val_loss: 126.9116 - val_mean_squared_logarithmic_error: 126.9116\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 121.3027 - mean_squared_logarithmic_error: 121.3027 - val_loss: 126.8695 - val_mean_squared_logarithmic_error: 126.8695\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 121.2649 - mean_squared_logarithmic_error: 121.2649 - val_loss: 126.8276 - val_mean_squared_logarithmic_error: 126.8276\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 121.2214 - mean_squared_logarithmic_error: 121.2214 - val_loss: 126.7858 - val_mean_squared_logarithmic_error: 126.7858\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 121.1702 - mean_squared_logarithmic_error: 121.1702 - val_loss: 126.7441 - val_mean_squared_logarithmic_error: 126.7441\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 121.1345 - mean_squared_logarithmic_error: 121.1345 - val_loss: 126.7025 - val_mean_squared_logarithmic_error: 126.7025\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 121.0945 - mean_squared_logarithmic_error: 121.0945 - val_loss: 126.6611 - val_mean_squared_logarithmic_error: 126.6611\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 121.0530 - mean_squared_logarithmic_error: 121.0530 - val_loss: 126.6197 - val_mean_squared_logarithmic_error: 126.6197\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 121.0237 - mean_squared_logarithmic_error: 121.0237 - val_loss: 126.5785 - val_mean_squared_logarithmic_error: 126.5785\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 120.9724 - mean_squared_logarithmic_error: 120.9724 - val_loss: 126.5375 - val_mean_squared_logarithmic_error: 126.5375\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 120.9463 - mean_squared_logarithmic_error: 120.9463 - val_loss: 126.4965 - val_mean_squared_logarithmic_error: 126.4965\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 120.8949 - mean_squared_logarithmic_error: 120.8949 - val_loss: 126.4557 - val_mean_squared_logarithmic_error: 126.4557\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 120.8475 - mean_squared_logarithmic_error: 120.8475 - val_loss: 126.4150 - val_mean_squared_logarithmic_error: 126.4150\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 120.8188 - mean_squared_logarithmic_error: 120.8188 - val_loss: 126.3745 - val_mean_squared_logarithmic_error: 126.3745\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 120.7835 - mean_squared_logarithmic_error: 120.7835 - val_loss: 126.3341 - val_mean_squared_logarithmic_error: 126.3341\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "history = model4.fit(\n",
        "    X_train4, \n",
        "    Y_train4, \n",
        "    epochs=200, \n",
        "    batch_size=16,\n",
        "    validation_data = (X_val4,Y_val4)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "HBtO4DJTRs8l",
        "outputId": "9d8f6205-7e46-4a9a-daa6-72df5138fa5c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEGCAYAAACdJRn3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd1yWVf/A8c9hiVsUB6gMJ8p0750jd46WuRqmOdplv3rKynrqeXxaZpmVWmZZWpmZpubKbaCgqLhBxQGioKjs7++P6wYR2dw3N+O8X6/7BdfFNb7cd/nlnOuc81UigqZpmqYVhI21A9A0TdNKH508NE3TtALTyUPTNE0rMJ08NE3TtALTyUPTNE0rMDtrB1BcnJ2dxcPDw9phaJqmlSpBQUGXRaR21v3lJnl4eHgQGBho7TA0TdNKFaVURHb7dbeVpmmaVmA6eWiapmkFppOHpmmaVmDl5pmHVvokJydz7tw5EhISrB2KppV5jo6ONGjQAHt7+3wdr5OHVmKdO3eOqlWr4uHhgVLK2uFoWpklIsTExHDu3Dk8PT3zdY7uttJKrISEBGrVqqUTh6ZZmFKKWrVqFaiVr5OHVqLpxKFpxaOg/6/pbqu8HFwB0Uch441Vpu9N2+nf2zuCkyd4dgfHalYKVtM0rXjo5JGXwyvhyO/5P75SLej1f9D2sUwJR9M0rWzR3VZ5eeA7mBV3+/VGLLx+1fS6Av+KMV4vh8P41VCnJfzxPCwfD4nx1o5e0ywiPDwcHx+fHH++ZcsWBg8ebPE4Hn/8cQ4fPgzAu+++m+/48qNz585FOr+s08mjoJQCGxvTyxZs7YxXRSfw7Abjf4d+s43Wytf94Mppa0esafmWkpJi7RDyLTU1la+++oqWLVsCdyYPc9i5c6dZr5cu63uc3/e8pH02utvK3JSCztOhrjcsnwhf9oLRi6FRTysHVrq9+fshDp+/ZtZrtnStxhtDvHM9Jjw8nAEDBtCxY0d27txJu3btmDhxIm+88QZRUVEsXboUb29vpk+fTmhoKMnJycyaNYthw4YRHh7O2LFjuXHjBgCffvopnTt3ZsuWLcyaNQtnZ2dCQ0Np06YN3333XY4PLGfOnMmqVauws7OjX79+zJkzh9OnT/Pwww8THx/PsGHD+Oijj4iPj2fLli3MmTOH1atXAzBt2jTatm3LhAkTeOutt/j999+5desWnTt35osvvkApRc+ePQkICGD79u089NBD9OzZk+eee474+HicnZ1ZvHgxLi4uBAUF8eijjwLQr1+/fL/PV65c4dFHH+XUqVNUqlSJBQsW4OfnR3R0NA8//DDnz5+nU6dObNiwgaCgIJydnRk+fDhnz54lISGBp59+mkmTJgFQpUoVnnzySf766y/mzZvHa6+9xpw5c1ixYgW3bt0iICAAb29v3nnnHVJTU3niiSfYuXMn9evX57fffqNixYr07NmTVq1asW3bNm7cuMG3337Lv//9bw4ePMgDDzzA7NmzM+4VH2/0Hrz//vt899132NjYcO+99/Lee+9l+7uePHmSqVOnEh0dTaVKlfjyyy/x8vJiwoQJODo6sn//frp06cKVK1fu2B43bhyTJ0/m5s2bNG7cmIULF+Lk5HTXZ/P888/n+323NN3ysJTGvWHSZqhSD5aMgN3zQdeLL5VOnDjB888/T1hYGGFhYXz//fds376dOXPm8O677/LOO+/Qu3dv9u7dy+bNm3nxxRe5ceMGderUYcOGDezbt48ff/yRGTNmZFxz//79fPTRRxw+fJhTp06xY8eObO8dExPDr7/+yqFDhzhw4ACvvfYaAE8//TRTpkzh4MGDuLi45Ov3mDZtGv/88w+hoaHcunUrI8EAJCUlERgYyIwZM5g+fTorVqzISBavvvoqABMnTmTu3LmEhIQU6P174403aNWqFQcOHODdd99l3LhxALz55pv07t2bQ4cOMWrUKM6cOZNxzsKFCwkKCiIwMJBPPvmEmJgYAG7cuEGHDh0ICQmha9euGce/9957VKxYkeDgYJYuXQrA8ePHmTp1KocOHaJGjRr8/PPPGcc7ODgQGBjI5MmTGTZsGPPmzSM0NJTFixdn3Cvd2rVr+e2339izZw8hISG89NJLOf6ukyZNYu7cuQQFBTFnzhyeeuqpjJ+dO3eOnTt38sEHH9y1PW7cON5//30OHDiAr68vb7755l2fTUlKHKBbHpZVsxE8vgF+eRL+fBkuHYRBH4BdBWtHVurk1UKwJE9PT3x9fQHw9vamT58+KKXw9fUlPDycc+fOsWrVKubMmQMY81POnDmDq6sr06ZNIzg4GFtbW44dO5Zxzfbt29OgQQMAAgICCA8Pv+Mfw3TVq1fH0dGRxx57jMGDB2c8R9ixY0fGP4Zjx47l5ZdfzvP32Lx5M//5z3+4efMmV65cwdvbmyFDhgDwwAMPAHD06FFCQ0Pp27cvYHQNubi4EBsbS2xsLN27d8+459q1a/P1/m3fvj0j1t69exMTE8O1a9fYvn07v/76KwADBgzAyckp45xPPvkk42dnz57l+PHj1KpVC1tbW0aOHJmv+3p6ehIQEABAmzZtCA8Pz/jZ0KFDAfD19cXb2zsjATdq1IizZ89Sq1atjGP/+usvJk6cSKVKlQCoWbNmtveLj49n586djB49OmNfYmJixvejR4/G1tb2ru24uDhiY2Pp0aMHAOPHj7/jGumfTUmjk4elVahqPHTf+h5sfR+ij8EDS6BqPWtHpuVThQq3k72NjU3Gto2NDSkpKdja2vLzzz/TvHnzO86bNWsWdevWJSQkhLS0NBwdHbO9pq2tbY792XZ2duzdu5eNGzeyYsUKPv30UzZt2gRkPy7fzs6OtLS0jO30SV8JCQk89dRTBAYG0rBhQ2bNmnXHhLDKlSsDxkxjb29vdu3adcd1Y2Njc3mHzGvLli389ddf7Nq1i0qVKtGzZ8+MWB0dHe/4Bzg3Wd/jW7du3fWzzJ9n+nZhny2kpaVRo0YNgoODs/15+nuc03ZO8ntccdPdVsXBxsYYvjv6G7gUCgt6QWSQtaPSzKR///7MnTsXMXVL7t+/H4C4uDhcXFywsbFhyZIlpKamFvja8fHxxMXFMXDgQD788MOMLqMuXbqwbNkygIxuGgB3d3cOHz5MYmIisbGxbNy4EbidRJydnYmPj2fFihXZ3q958+ZER0dnJI/k5OSMbp8aNWqwffv2u+6Zl27dumUcv2XLFpydnalWrRpdunThp59+AmD9+vVcvXoVMN43JycnKlWqRFhYGLt3787Xfezt7UlOTs53XPnVt29fFi1axM2bNwHjGU52qlWrhqenJ8uXLweMRJyfLr7q1avj5OTEtm3bAFiyZElGK6Qk08mjOHkPh8fWg40dLLwX9n9n7Yg0M/jXv/5FcnIyfn5+eHt7869//QuAp556im+++QZ/f3/CwsIK9Rfk9evXGTx4MH5+fnTt2jWjv/zjjz9m3rx5+Pr6EhkZmXF8w4YNuf/++/Hx8eH++++nVatWANSoUYMnnngCHx8f+vfvT7t27bK9n4ODAytWrODll1/G39+fgICAjFFHixYtYurUqQQEBGQkyvyYNWsWQUFB+Pn5MXPmTL755hvAeBayfv16fHx8WL58OfXq1aNq1aoMGDCAlJQUWrRowcyZM+nYsWO+7jNp0iT8/PwYM2ZMvmPLjwEDBjB06FDatm1LQEBARvdkdpYuXcrXX3+Nv78/3t7e/Pbbb/m6xzfffMOLL76In58fwcHBvP766+YK32JUQf4jKM3atm0rJaaS4I3LsGIinP4bWo+He/9jzFDX7nDkyBFatGhh7TBKhcwjg0qLxMREbG1tsbOzY9euXUyZMiXHLh+teGT3/5xSKkhE2mY9Vj/zsIbKzjB2JWx+B7b9Dy4Ew/3fgpOHtSPTtGJz5swZ7r//ftLS0nBwcODLL7+0dkhaAVg0eSilFgKDgSgR8THtexsYBqQBUcAEETmvlHoRSG9v2gEtgNoiciXLNRcDPYA4064JIlL6/lyxsYU+r0ODdsZorC96wIgvoVn+x89rZct9993H6dN3Tip9//336d+/f57nWqvVsW7durtGenl6emaMlMpN06ZNM54PlTZTp069a3j1008/zcSJE60UUfGzaLeVUqo7EA98myl5VBORa6bvZwAtRWRylvOGAM+KSO9srrkYWC0i2T/xy0GJ6rbK6sop+GkcXDwI3V+CnjON5FLO6W4rTSteBem2sugDcxH5G7iSZV/macKVgeyy10PADxYMrWSp2Qge2wABj8Df/4Elw+H6RWtHpWmaliOrjLZSSr2jlDqL0U31epafVQIGAD9nd67JO0qpA0qpD5VSOc64U0pNUkoFKqUCo6OjzRK7xdhXhGGfwrB5cC4QPu8CJ/6ydlSapmnZskryEJFXRaQhsBSYluXHQ4AdWZ91ZPIK4AW0A2oCOU6tFZEFItJWRNrWrl3bDJFbmFLQ6hF4YjNUqQPfjYS/ZkGq+ceua5qmFYW153ksBbKuNfAguXRZicgFMSQCi4D2FozPOup4wROboM0E2P4hLBoIsWfyPE3TNK24FHvyUEo1zbQ5DAjL9LPqGCOpcpxZo5RyMX1VwHAg1DKRWpl9RRjyMYxaCFFHYH5XOLI67/M0q6pSpYq1QyiV8qr/sXjxYqZNy9pJYX4DBw7MWMfrs88+y3d8eTl//jyjRo0yR4glhkWTh1LqB2AX0FwpdU4p9RjwnlIqVCl1AOgHPJ3plPuA9SJyI8t11iilXE2bS5VSB4GDgDMw25K/g9X5jITJfxslbn8cA2teguT8F6nXNGsqzJIs1iAipKWlsWbNGmrUqHFX8igqV1fXHJeEKarC1gcp6meTr+ShlLJVSj1b0IuLyEMi4iIi9iLSQES+FpGRIuIjIn4iMkREIjMdv1hEHszmOgNF5Lzp+94i4mu6xiMiUrqm1RZGzUbGsiYdn4K9X8CCnnCxbDa4crR2JiwaZN7X2pl53nbmzJnMmzcvY3vWrFnMnj2bPn360Lp1a3x9ffO9BMWWLVvo0aMHw4YNo1GjRsycOZOlS5fSvn17fH19OXnyJADR0dGMHDmSdu3a0a5du4z5BHv37qVTp060atWKzp07c/ToUcD4q3zEiBEMGDCApk2b5rpkeGpqKhMmTMDHxwdfX18+/PBDAIKCgvD398ff358XX3wxowpf1r/4Bw8ezJYtWwCYMmUKbdu2xdvbmzfeeCPjGA8PD15++WVat27N8uXLWb9+PZ06daJ169aMHj06Y07Kn3/+iZeXF61bt+aXX37J13sIRo2V3r174+fnR58+fTKWcj958iQdO3bE19eX1157LaMVGB8fn+3nFR4eTvPmzRk3bhw+Pj6cPXsWDw8PLl++zMyZMzl58iQBAQG8+OKLGdcZNWoUXl5ejBkzJmOJFg8PD1555RUCAgJo27Yt+/bto3///jRu3Jj58+dn3Cv9PU1NTeWFF17Ax8cHPz8/5s6dm+PvGhQURI8ePWjTpg39+/fnwoULAPTs2ZNnnnmGtm3b8vHHH9+1vXHjRlq1aoWvry+PPvpoxuq+WT+bIhGRfL2Avfk9tiS+2rRpI2XCsQ0i/20q8pazyI65Iqmp1o7IYg4fPnx7Y83LIgsHmve15uU8Y9i3b5907949Y7tFixZy5swZiYuLExGR6Ohoady4saSlpYmISOXKlXO81ubNm6V69epy/vx5SUhIEFdXV3n99ddFROSjjz6Sp59+WkREHnroIdm2bZuIiERERIiXl5eIiMTFxUlycrKIiGzYsEFGjBghIiKLFi0ST09PiY2NlVu3bombm5ucOXMm2xgCAwPlnnvuydi+evWqiIj4+vrK1q1bRUTkhRdeEG9v74xrT506NeP4QYMGyebNm0VEJCYmRkREUlJSpEePHhISEiIiIu7u7vL+++9nvD/dunWT+Ph4ERF577335M0335Rbt25JgwYN5NixY5KWliajR4+WQYMG5fjeZY5j8ODBsnjxYhER+frrr2XYsGEZsX3//fciIvL5559nfBbJycnZfl6nT58WpZTs2rUr4z7u7u4SHR0tp0+fzngPRIzPrlq1anL27FlJTU2Vjh07ZnxG7u7u8tlnn4mIyDPPPCO+vr5y7do1iYqKkjp16oiI3HG9zz77TEaOHJnxWaa/j1klJSVJp06dJCoqSkREli1bJhMnThQRkR49esiUKVMyjs28nf7eHj16VERExo4dKx9++OFdn0127vh/zgQIlGz+TS3IDPMdSqlPgR+BjG4lEdlXtPSlFUjTe2DKTlg1A9a/CsfXw33zoZpr3ueWZvdmX7nN0lq1akVUVBTnz58nOjoaJycn6tWrx7PPPsvff/+NjY0NkZGRXLp0iXr18l5mv127dhm1Ixo3bpxRkc/X15fNmzcDRv2I9LrcANeuXctYXXf8+PEcP34cpdQdK8j26dOH6tWrA9CyZUsiIiJo2LDhXfdv1KgRp06dYvr06QwaNIh+/foVulbHTz/9xIIFC0hJSeHChQscPnwYPz8/4HYNit27d3P48GG6dOkCGIWNOnXqRFhYGJ6enjRtajwCfeSRR1iwYEGe9wTYtWtXRktl7NixGS2tXbt2sXLlSgAefvhhXnjhBcD4A/n//u//7vq8wFiFOL8LL+ZWgyVzfZD4+HiqVq1K1apVqVChwl3L2f/1119MnjwZOzvjn9+c6oPkVFslXdY6H5lrsnh6etKsWTPAqA8yb948nnnmmWzPK6yCJI8A09e3Mu0T4K5Z4JqFVXaGB5fCvm/gz1fgs07Gw3Xv4daOrEwaPXo0K1as4OLFizzwwAMsXbqU6OhogoKCsLe3x8PD447aGLnJqzYIGHUhdu/efUf9DzAqAfbq1Ytff/2V8PBwevbsme11c6sP4uTkREhICOvWrWP+/Pn89NNPGSv1Zien+iCnT59mzpw5/PPPPzg5OTFhwoQc64P07duXH364cwBlcS6AmNvnVZCVjnN7jy1RH0RyqK2Sztr1QfL9wFxEemXz0onDWpQyhvJO3m48E1k+HlY+BQnmrfOtGX+pLVu2jBUrVjB69Gji4uKoU6cO9vb2bN68mYiICLPer1+/fnf0g6f/QxsXF0f9+vUB41lEYVy+fJm0tDRGjhzJ7Nmz2bdvX661Ojw8PAgODiYtLY2zZ8+yd+9ewGgNVa5cmerVq3Pp0qUcWyodO3Zkx44dnDhxAjDKyB47dgwvLy/Cw8MznvNkTS656dy58x21TLp165Zxr/SKhek/Bwr1eVWtWpXr16/nO6aC6Nu3L1988UVGUsmpPkhOtVXy0rx5c8LDwzPec0vVB8l38lBKVVdKfZA+Y1sp9T/T0FrNmmo1Nh6md38JQn4wZqaf2mrtqMoUb29vrl+/Tv369XFxcWHMmDEEBgbi6+vLt99+i5eXl1nv98knnxAYGIifnx8tW7bMeOj60ksv8corr9CqVatC/zUbGRlJz549CQgI4JFHHuHf//43kHOtji5duuDp6UnLli2ZMWMGrVu3BsDf359WrVrh5eXFww8/nNEtlVXt2rVZvHgxDz30EH5+fhldVo6OjixYsIBBgwbRunVr6tSpk+/fYe7cuSxatAg/Pz+WLFnCxx9/DMBHH33EBx98gJ+fHydOnMjoxivM51WrVi26dOmCj49PxgNzc3n88cdxc3PDz88Pf39/vv/++2yPy622Sm4cHR1ZtGgRo0ePxtfXFxsbGyZPnpzneQWV74URlVI/Y8yp+Ma0ayzgLyIjzB6VBZTohRHN5cweWDkFrpyEdk/APbOgQumdd6AXRrSO8PBwBg8eTGho6RrRd/PmTSpWrIhSimXLlvHDDz/keyScZrBUPY/GIpJ5NvibSqnStxR6WebWwejG2vQ27P4cTmyAYZ+BR/Z/FWpaWRIUFMS0adMQEWrUqMHChQutHVKZVpDkcUsp1VVEtgMopboAt/I4RytuDpVgwL/BazD89hQsHggdphi1QxwqWTu6cuHgwYOMHTv2jn0VKlRgz549xRpHhw4dMsb3p1uyZAm+vr65nufh4WG1VseiRYsyuqHSdenS5Y65Njnp1q1bvmqGl0RFqeViLQXptvIHvgXSn3NcBcaLyAELxWZW5aLbKqukG8bCinsXGA/Vh38ObvkbllgSHDlyBC8vL4yVaDRNsyQRISwszLz1PJRStsBYEfEH/AA/EWlVWhJHueVQGQb+F8b/DmkpsHAArHsVkktHg9HR0ZGYmBjy+weOpmmFIyLExMTcNTw8N/nqthKRVKVUV9P3eixoaePZ3ZhYuOF12PUpHPvTmBfi0dXakeWqQYMGnDt3jhJfi0XTygBHR8eMSZD5UZBuq8+B+sBy7pxhnv9FaayoXHZbZefkZvj9aYiNMOaJ3PMmVKxh7ag0TSuhzFGG1hGIwZhRPsT0KvwaxZp1NO4FT+2CztNh37cwrwMcXmXtqDRNK2Xy1W1leuYRIyIvWDgerTg4VIZ+s43l3ldNh5/GGqOzBv637K+RpWmaWeSr5SEiqYCeLFDWuLYySt7e86ZRL31eB/jna8i0lpGmaVp2CtJtFayUWqWUGquUGpH+slhkWvGwtYeuzxgP1F0D4I/njLkh0UetHZmmaSWYfuahGWo1hnGrjBnpUUeMNbI2vl1qhvVqmla88j3aqrTTo60KID4K1r8GB36EGu7Gs5BmJXemq6ZpllPk0VZKqWZKqY1KqVDTtp9S6jVzBqmVEFXqwIgFMH412DnC9/fDsjEQe9bakWmaVkIUpNvqS+AVIBnANLv8rnrjmSmlFiqlotITjmnf20qpA0qpYKXUeqWUq2l/T6VUnGl/sFLq9Ryu6amU2qOUOqGU+lEp5VCA30ErCM9uxkKLfV6HExthXnvY8TGkJud9rqZpZVpBkkclEdmbZV9eRQUWAwOy7PuviPiJSACwGsicJLaJSIDp9RbZex/4UESaYKyv9Vj+wtcKxc4Buj0PU/eAZw9jlvr8bhC+w9qRaZpmRQVJHpeVUo0xSs+ilBoFXMjtBBH5G7iSZV/m5U0qp18vP5SxQl5vYIVp1zeArr1aHJzc4eFl8OAPkBRvjMj6dYrxfETTtHKnIMljKvAF4KWUigSeAQpVnkop9Y5S6iwwhjtbHp2UUiFKqbVKKe9sTq0FxIpIeovnHMaSKTndZ1J65UO9PpKZeA00WiFdn4WDP8HcNrBrnu7K0rRypiA1zE+JyD1AbcBLRLqKSEYxYKXU+AJc61URaQgsBaaZdu8D3E0r984FVub3erncZ4GItBWRtrVr1y7q5bR0DpWNKoVP7YYG7WDd/xlDe09usnZkmqYVk4K0PAAQkRsikl1l+KcLcf+lwEjTda+JSLzp+zWAvVLKOcvxMUANpVT6sioNgMhC3FczB+em8MjP8NAySE2EJfcZo7Kuhls7Mk3TLKzAySMX+arYo5RqmmlzGBBm2l/P9EwDpVR7U2wxmc8VY1LKZmCUadd4QBcptialoPm98NQe6P0vo/UxrwNsfheSblo7Ok3TLMScyeOuB99KqR+AXUBzpdQ5pdRjwHtKqVCl1AGgH7dbLKOAUKVUCPAJ8KApWaCUWpM+pBd4GXhOKXUC4xnI12b8HbTCsneE7i/AtEBjkcWt78On7eDQr1BOJqJqWnlithnmSqn9ItLKLBezAD3DvJhF7IQ1L8Glg+DeBfq/YyzEqGlaqWKOeh550QP/tdvcO8OTW2Hwh8Yiiwt6wi9PQpx+RKVpZUFBlid5VylVI9O2k1Jqdvq2iEzL/kyt3LKxhbaPwoz9xtDeQ78aQ3s3vQOJ8daOTtO0IihIy+NeEYlN3xCRq8BA84eklTmO1YyhvdP+MeaJ/P0fmNsa9i2BtFRrR6dpWiEUJHnYKqUqpG8opSoCFXI5XtPu5OQOoxbCY38Zq/WumgZf9IBTW6wdmaZpBVSQ5LEU2KiUesw0amoDxvIgmlYwDdvBY+th1CJIjINvh8H3D0D0MWtHpmlaPhVotJVS6l6gj2lzg4iss0hUFqBHW5VQyQmwZz5s+x8k3YDW46DnTKhaz9qRaZpGzqOtdDEorWSIj4a//wuBC8HGDjo9BV2eBsfq1o5M08q1Qg/VVUptN329rpS6lul1XSl1La/zNS1fqtSGgf8xHqq3GGy0RD72h52fGq0TTdNKlDyTh4h0NX2tKiLVMr2qikg1y4eolSs1PWHkV/Dk3+DaGta/Cp+2heDv9cgsTStBCjRJ0DS3w08p1Tr9ZanAtHLOxR/G/gLjVkFlZ1g5BeZ3haN/6uVONK0EsMv7EINS6m1gAnAKSDPtFoziTJpmGY16wBOb4fBK2Pg2/PAAuHUy5o24dbR2dJpWbuU7eQD3A41FJMlSwWhatpQC7/uMBRf3fWssuriwPzTpC71f1WtmaZoVFKTbKhSokedRmmYptvbQ7jFjuZN73oTIQGPNrGVj4NIha0enaeVKvofqKqXaYtTOCAUS0/eLyFDLhGZeeqhuGZRwzZgjsnMuJF43Wic9X4HazawdmaaVGUWe56GUOoRRw/wgt595ICJbzRWkJenkUYbdvAK7PoXd8yHlFvg9AD1eNkZuaZpWJOZIHv+ISDuzR1ZMCps8TkTFE3cr82Oe2wUTVabaiQ62NjhXqUDdahVQKl9FFTVzu3EZtn8I/3wFaSnQ6hHo/iJUb2DtyDSt1DJH8vgAo7tqFXd2W+0zV5CWVNjkMXlJEH8eupjv42tXrcA9LerwVM8mNKxZqcD308zg2gXY/gEELjIyfOvx0PUZnUQ0rRDMkTw2Z7NbRKRUDNUtbPI4dD6OmHij5ZH5ncr6viWmpBF1PZG9p6+w4fBF0tJgeu8mTO3VBBsb3RKxitgz8PccCF4KKKMl0vVZY3VfTdPyxSprWymlFgKDgSgR8THtexsYhvHcJAqYICLnlVJjMOqTK+A6MEVEQrK55mKgBxBn2jVBRILziqU4n3lcjEtg9h+HWX3gAj2a1eajBwJwquxQLPfWshF7BrZ/BPuXgKSB/0PQ7Tmo2cjakWlaiWeOlkcFYCTgQab5ISLyVi7ndAfigW8zJY9qInLN9P0MoKWITFZKdQaOiMhV0+q9s0SkQzbXXAysFpEV+QrcpJzGEawAACAASURBVLgfmIsI3+89w5urDuNcxYHPHmlDQEM90tmq4iJhx8cQtNh4JuJ3P3R7AZybWDsyTSuxzFHD/DeMFkMKcCPTK0ci8jdwJcu+zIspVsbUGyQiO03VCQF2A6W6g1opxZgO7qyY0gmlFKPn7+TbXeF3dXdpxah6fWPxxWcOQIfJcGglzGsHPz8OUWHWjk7TSpWCtDxC01sPBbqBUh4YLQWfTPveAcZhdD31EpHoLOe8AHiJyOPZXG8x0Anjof1GYKaIJGY9znTsJGASgJubW5uIiIiChm8WsTeTeO6nEDaFRfFA24a8PdwHB7sCLSumWUJ8lDFH5J+vIfkmeA+Hrs+Bi5+1I9O0EsMc3VYLgLkicrCAN/YgS/LI9LNXAEcReSPTvl7AZ0BXEYnJ5hwX4CLgACwATubWdZbO2vM80tKEj/46xiebTtDeoyafP9KaWlV0Fd8S4UYM7J4HexZA0nVj2ZOuz4J75zvHY2taOVTo5KGUOojRtWQHNMVYGDER48G2iEiuf6blkTzcgDWZnof4Ab8C94pInjVJlVI9gRdEZHBex1o7eaRbFXKeF5eHULtqBb4e347m9apaOyQt3a1YY47I7s/h5mVo2MFoiTTrr5OIVm4V5ZnHYGAIcC/QBOhn2k7fX9BAmmbaHAaEmfa7Ab8AY3NLHKaWB8qYiTccY7mUUmOovys/PtmJpJQ0Rny2gz9DL1g7JC1dxRrQ/QV45iDc+1+4dt5YxffzznDgJ0hNsXaEmlZiFKTbaomIjM1rX5af/wD0BJyBS8AbwECgOcZQ3QhgsohEKqW+whjNlf5gIiU92yml1gCPm4b0bgJqY7R8gk3nx+cVf0lpeaS7GJfApCWBHDgXxwDvesy+zwdn3Y1VsqQmQ+jPxqz16DCo4QadZxjzRewrWjs6TSsW5njmsU9EWmfatgUOikhL84VpOSUteQAkpaTx5bZTfLzxOE6V7Jn3cGvaetS0dlhaVmlpcGwtbPvAWMm3ch3oOBnaPgoVnawdnaZZVFFqmL+ilLoO+GWuX44xwe83C8RabjjY2TC1VxN+faozjva2PLBgN1/+fUoP5y1pbGzAaxA8/heMXw31fGDjW/CBN6ydCVetM4pP06ypIC2Pf4vIKxaOx2JKYssjs2sJyby4PIR1hy7R37su/xnlT/WK9tYOS8vJxYOw81MIXWHMWm85DDpPh/ptrB2ZpplVUUZbeYlIWE71ysv6wojFSUT4evtp3lsbRn2ninw2pjXertWtHZaWm7hI2PuFsQhj4jVw72Ikkab9jRaLppVyRUkeC0RkUnldGNEagiKuMHXpfq7cTGLWEG8eat9QL/Ne0iVcM9bO2v05xJ2FWk2h01Twf1A/XNdKtSI9MFdK2QCdRGSHJYIrDqUpeQDExCfyzI/BbDt+mUG+Lrw5zFuPxioNUlPg8Epj5vqFYKjkDO0nQbvHoXIta0enaQVmjtFW+0WkldkjKyalLXmAMSt9/t8n+XDDMSo52PHOfT4M9nO1dlhafohAxA4jiRz7E+wcwXc0dJwCdb2tHZ2m5Zs5ksccYBfwi5TC4UClMXmkOxF1nRdXHGD/mVge6ejGa4Na4mhva+2wtPyKPgq7P4OQH40yuR7djCTSbADY6M9RK9nMkTyuY6yCmwIkcHt5kmrmDNRSSnPyAEhOTWPOuqN88fcpvF2r8dmY1rjXqmztsLSCuHkF9n0De7+Ca+fAycPo0mr1CDjqgRFayWSVYlAlSWlPHun+OnyJ55eHkJYmvDfSj0F+LtYOSSuo1BQIWw175sOZXeBQBQIehvZP6toiWoljluShlHLCWBzRMX2fqWZHiVdWkgdAZOwtpn2/j/1nYhkW4MqsId66UmFpdX4/7PnCWAYlNclY0bfjZGjcRy/GqJUI5ui2ehx4GqNIUzDQEdilh+paR3JqGp9tPsmnm49TvaI9s4f7MMBHt0JKrfgoCFxo1Ba5EQXOzaDdE+D/gO7S0qzKHMnjINAO2C0iAUopL+BdERlh3lAto6wlj3RHLlzjxRUhhEZeY7CfC28O9dZ1QkqzlESjwuGe+XB+H9hXNsrltnvcWBZF04qZOZLHPyLSTikVDHQQkUSl1CERKRXjDstq8gCjFfLF1pN8vPE41RzteWuYj34WUhZEBhktkdCfISUB3DoZSaTFELDTfyBoxcMcyeNXYCLwDNAbuArYi8hAcwZqKWU5eaQ7evE6L64I4cC5OEa1acBbw7yp5GBn7bC0orp5BYKXGonk6mmoXBtaj4M2E6FGQ2tHp5VxZh1tpZTqAVQH/hSRJDPEZ3HlIXkApKSm8cmmE8zddJyGTpV4c5g3vZrXsXZYmjmkpcGpTUYSOfansa/ZAKM10qiXXktLswhztDyyKzRxXUSSixpccSgvySPd7lMxvPrrQU5G32CAdz1eH9IS1xp6jaUyI/YMBC2GoG+Mkrk1G0Hbx4whv5V0TRjNfMyRPMKBhhjdVQqoAVzEqBD4hIgEmS1aCyhvyQNuF5uau+k4DrY2zBntTz/vetYOSzOnlEQ48rtRe/3MLmMZlJbDoc0EcOuoh/tqRWaO5PElsEJE1pm2+2GUjV0EfCwiHcwYr9mVx+SRLiLmBtO+38/ByDiGBbjyfwNbULeaY94naqXLxYPGcN8DyyHpOjg3hzbjwf8h3RrRCs0sQ3VFxDfLvgMi4qeUChaRADPFahHlOXkAJKakMnfjCRb8fYoK9jbMHu7DsID61g5Ls4SkG3DoV6Nb69w/YOsALYYarRGPrro1ohVIocvQZnJBKfWyUsrd9HoJuGSqZZ6Wy40XKqWilFKhmfa9rZQ6oJQKVkqtV0q5mvYrpdQnSqkTpp9nW4BKKdVGKXXQdNwnShe7yFMFO1te6N+cdc92p2mdKjy9LJiHFuwmKOKqtUPTzM2hsrFe1uN/wZSdxqisExvgm8Ewtw3s+Bjio60dpVbKFaTl4Qy8AXQ17doBvAnEAW4iciKH87oD8cC3IuJj2ldNRK6Zvp8BtBSRyUqpgcB0YCDQgRy6w5RSe4EZwB5gDfCJiKzNLf7y3vLILCU1je92R/Dp5pPE3Ejk8a6ePN+vuV6ptyxLvgWHfzNaI2d2gY29UZe9zXjw7KlHamk5MttQXaVUVYzVdOMLcI4HsDo9eWT52SsYyWeKUuoLYIuI/GD62VGgp4hcyHS8C7BZRLxM2w+Zjnkytxh08rjbzaQU3l1zhO92n6F53ap8+EAALV1LxSLJWlFEHzVGaYV8D7euQg13Y96I/0NQXXdlancqcreVUspXKbUfCAUOKaWClFKFXi9BKfWOUuosMAZ43bS7PnA202HnTPsyq2/an9sx6feYpJQKVEoFRkfrZnpWlRzsmD3cl0UT23HlZhJDP93O7NWHuZ5QKkZfa4VVuzkMeBeeC4ORX0MNN9j0NnzkA9+NhNBfjFFcmpaLgrRVvwCeExF3EXEHngcWFPbGIvKqiDQElgLTCnudPO6xQETaikjb2rVrW+IWZUKv5nVY/0x3RrdtyNc7TtP7f1tZuT+S8rJcf7ll7wi+o2DCapixH7o9D1FhsGIi/K85rHkRLoRYO0qthCpI8qgsIpvTN0RkC0ZxqKJaijHkFyASYy5JugamfZlFmvbndoxWQE6VHfj3CF9WPtUF1xoVeebHYB75eg+nL9+wdmhacajZCHq/Bs8cgEd+gca9ja6tL7rD511h9+dwI8baUWolSEGSxyml1L+UUh6m12vAqcLcVCnVNNPmMCDM9P0qYJxp1FVHIC7z8w4A0/Y1pVRH0yirccBvhYlDu5t/wxr8OqUzs4f7cOBsHP0/+ptPNx0nKSXHAXVaWWJjC036wKiF8MJRGDgHbO3gz5lGa+THsXBsnVHQSivXCjLaygljdFX6aKttwCwRyXWsp1LqB6An4IwxG/0NjNFUzTGG+EYAk0Uk0pQMPgUGADeBiSISaLpOxlwSpVRbYDFQEVgLTM+rrrp+YF5wUdcSeHP1Yf44cIGmdaowe7gPHRrVsnZYmjVcOgT7l8KBZXAzBqrUA/8HIWAM1G5m7eg0C9JlaHXyKLTNYVG8tjKUyNhbDPZz4f8GttDrZJVXKUlwfB3s/w6ObwBJBdfWRiLxGQmVna0doWZmhU4eSqnfgRwPEpGhRQ/P8nTyKJpbSanM33qS+VtPohQ81bMJk7o30nNDyrPrl+DgcqM1cvEg2NgZZXT9HzRW+7XXS+CUBUVJHj1y+7mIbC1ibMVCJw/zOHf1Jv9eE8YfBy/QwKkirw1qQX/veuhJ/uXcpUMQsgwO/ATxF6FCdfAebswd0Qs0lmoW77ZSSv0sIiPzPtI6dPIwr50nL/PW74cJu3idzo1r8cYQb5rXq2rtsDRrS0uF01uNRHLkd0i+aUxC9H8Q/B6AWo2tHaFWQMWRPPaLSCuzXMwCdPIwv5TUNL7fe4b/rT9GfGIKYzu682zfZlSvaG/t0LSSIDHeSCAHlsGprYBAg/bg/wB4j9Ar/ZYSxZE89olItgsZlgQ6eVjO1RtJ/G/DUb7fcwanSg68NKA5o9s0xMZGd1VoJnGRxvORkGUQfcRYW6tZf2OSYrMBYK8HYJRUOnno5GFxoZFxzFp1iMCIq7R2q8Hs4b56rSztTiJw8YCRREJ/hvhL4FAVWgwGn1HQqKcxr0QrMXS3lU4exUJE+GVfJO+uOULsrWQmdPZgeu8m1KjkYO3QtJImLRXCtxktksO/Q2IcVHIG7/uMFkmD9nq13xKgOJJHPxFZb5aLWYBOHsUr9mYS/1l3lB/2nqGivS1jOrgxtZdOIloOUhKNeSMHl8OxPyElAaq7gc8I8B0Ndb31iC0rKcpQ3YPkPs/Dr+jhWZ5OHtZx9OJ15m89yW/BkVR1tGd67yaM6+SBg53+i1LLQcI1OLrGSCQnNxsTEWu3AN+RRtdWTU9rR1iuFCV5uJu+nWr6usT0dQyAiMw0V5CWpJOHdYVdvMY7fxxh2/HLuNeqxMsDvLjXR88P0fJw47JRUvfgCji729hXv63RGmk5DKq5WDe+csAcNczveqZR0h+SZ6aTR8mw9Vg07/5xhKOXrtPW3YlXB7WglZuTtcPSSoPYM8ZD9oMr4FIooMC9s/GMpMVQqFrX2hGWSeZIHsHAVBHZYdruDHyWvlhhSaeTR8mRmiYsDzzL/zYcI/p6IoP9XHipvxdutSpZOzSttIgKg8MrjcJVl48CCjy6GrPaWwyFKnWsHWGZYY7k0QZYCFQ37YoFHhWRfWaL0oJ08ih5biSm8MXWkyzYdoqUVOGh9m5M792EOtX0mkhaAUQdMbq2Qn+BmOOgbEyJxNQi0Ys1Fok5a5hXBxCRODPFVix08ii5Ll1L4JONx/nxn7PY2SomdPZkco9GemSWVjAiEHX4diK5chKULXh2MxKJ1xCorEsKFJQ5Wh51gXcBVxG5VynVEugkIl+bN1TL0Mmj5IuIucGHG47xW8h5qlSwY2qvJkzo7KFX7tUKTsR4LnLoV+N15ZSRSBr1MCWSwXp5lHwyR/JYCywCXhURf6WUHbBfRHzNG6pl6ORReoRdvMb7a8PYfDSa+jUq8kL/Zgzzr6+XO9EKJ31We3oiuRpuLB/v2QNaDjUSie7aypE5ksc/ItIu86irzNX9SjqdPEqfnScu8+7aI4RGXsOnfjWe79ucHs1q6ySiFZ4IXAg2JZKVEBthPCNx7wIthhivaq7WjrJEMUfy2AKMBDaISGtTjfH3RSTXeh8lhU4epVNamrAq5Dz/XXeUyNhbNKpdmaf7NGWIn6tOIlrRiBhFrI6sMlb/jQ4z9jdoZ0okQ/WERMyTPFoDcwEfIBSoDYwSkQO5nLMQGAxEiYiPad9/gSFAEnASo055rFJqDPBiptP9gNYiEpzlmrOAJ4Bo067/E5E1ecWvk0fplpSSxtrQC3y+5SRhF6/jW786M+/1oksT3d2gmUn0MVMiWQUXQox9dX2Nrq0WQ6F283K5REqRkodSyhaYgZE8mgMKOCoiyXmc1x2IB77NlDz6AZtEJEUp9T6AiLyc5TxfYKWI3FU5xpQ84kVkTp6BZ6KTR9mQliasDI7kf+uPERl7iw6eNZnRpymdG9fSs9U187kaYbRGjvwOZ/cAArWamhLJEHAJKDeJxBwtj70i0r4QN/YAVqcnjyw/uw+j9TImy/53ARGRV7M5ZxY6eZR7Ccmp/LD3DPO3nuTStUTauDsxvXcTejSrrZOIZl7XL95OJOHbjbW2argZrZEWQ4xuLpuyOyLQHMnjQ8Ae+BG4kb4/r0mCeSSP34EfReS7LPtPAsNEJDSbc2YBE4BrQCDwvIhczeHek4BJAG5ubm0iIiJyC1UrhRKSU1kedI75W04SGXsL/wbVmd67KX1a1NFJRDO/GzFwbC0cXgWnNkNqkrGMfPMBxqitRj3LXGErcySPzdnsFhHpncd5HmSTPJRSrwJtgRGSKQilVAfgq5yGAJvmm1zGWOn3bcBFRB7NK37d8ijbklLS+GXfOeZtOcHZK7do6VKN6b2b0N+7nn6wrllGwjU4sQHC1sDx9ZB4DewrQePe0HygUSGxDExKtHg9j1xu7EGW5KGUmgA8CfQRkZtZjv8QiBaRdwtz7Zzo5FE+JKem8VvweeZtPsHpyzdoVrcK03o3ZZCvC7Y6iWiWkpIEEduNRHJ0DVyLNIYAu3UyEonXQKjZyNpRFopZkodSahDgDWQsPiQib+VxjgeZ/oFXSg0APgB6iEh0lmNtgLNANxE5lcP1XETkgun7Z4EOIvJgXrHr5FG+pKYJqw+cZ+6mE5yIiqdJnSq8OdRbj87SLC99LknYGgj7A6IOGfvrtLydSFxalZoqiebotpoPVAJ6AV8Bo4C9IvJYLuf8APQEnIFLwBvAK0AFIMZ02G4RmWw6vifwnoh0zHKdr4D5IhKolFoCBGB0W4UDT6Ynk9zo5FE+paUJa0Mv8v6fYZy5cpMezWrzWFdPujV11s9EtOJx5TQcXWu0SCJ2gKRBVZfbicSjO9iV3HXczJE8DoiIX6avVYC1ItLN3MFagk4e5VtCcipfbz/Noh3hXI5PpGmdKjzZozHDA1yxsy0dfwFqZcDNK3BsHRz9A05shOSb4FAVmt4Dze6Fpn1L3Jpb5kgee0Skg1JqNzACo+VwSESamDdUy9DJQwNITElldcgFvtp+miMXruHpXJnpvZsw1F8nEa2YJd+CU1uNRHJ0LdyINp6TNOwAzfobD9xre1l9Pok5kse/MCYJ9gHmYXQbfSUi/zJnoJaik4eWmYiw/vAlPvrrOEcuXMO9ViWe7N6YkW3qU8Gu7I7Z10qotDQ4vx+O/Wm8LpoW7qjhbiSRZv2NGiV2FYo9NLOOtlJKVQAcS1NND508tOykpQnrD19k3uaTHIyMo07VCjzezZOHO7hTpYKdtcPTyqu4SDi+zujiOrUFUhLAvjI07gXN74Wm/YqtWqI5Wh7jstsvIt8WMbZioZOHlhsRYceJGD7feoIdJ2Ko5mjHkz0aM7GLB5UcdBLRrCjpJoRvM7VK1hnDgAHqt7ndKqnnZ7HuLXMkj7mZNh0xuq/2icgo84RoWTp5aPkVfDaWuRuPszEsisoOtvRpUZeH2rvRsVFNPUJLs670lYCPrTOSSWQQIFDV9fZzkkY9zDrL3eyTBJVSNYBlIjKgqMEVB508tILad+YqywPPsebgBeJuJRs1Rfo1p6deP0srKeKj4PgGI5Gc3ARJ8WBXETy7Q7N+RvdWDbci3cISycMeCBWR5kWKrJjo5KEVVkJyKiv3R2YsfdLW3Ynn+jajk17JVytJUhKNeSTprZKr4cb+2i1g5JdQr3BFX83RbfU7xggrABugJfCTiMwsVETFTCcPraiSUtL4KfAsn2w8TtR1Y67I2E7u3NeqPlUd7a0dnqbdJgIxJ4xEcnw9jF5c6Pkj5kgemSsGpgARInKuUNFYgU4emrkkJKeyKuQ8S3ZFcDAyjsoOtgxvVZ8nujXCw7mytcPTNLOy2sKIJYVOHpolhJyNZcnuCH4POU9qmjCqTQPGdnLH27W6tUPTNLMwR8vjOre7re74EcbS7NWKFqJl6eShWVLU9QTmbjzBj4FnSUpJo52HE0/1aqIfrmulnjmSx9vABWAJRsIYg1FL43VzBmopOnloxSHuZjIr9p3j622nOB+XQEuXakzq3oh7fevpmetaqWSO5BEiIv557SupdPLQilNSShq/BUcyf+tJTkbfwLmKAw+2c+PhDm641ihblea0si2n5FGQqbM3lFJjgGUY3VcPkakcraZptznY2TC6bUNGtm7AjpOX+XZXBJ9tOcHnW0/St0VdxnVy10N9tVKtIMnjYeBj00uAHaZ9mqblwMZG0a1pbbo1rc25qzdZuucMy/ae4c9DF2lSpwpjO7ozorUe6quVPnq0laYVs4TkVP44cIFvd4UTcs4Y6juidQPGdXKnad2q1g5P0+5gjmce/wFmA7eAPwE/4FkR+c6cgVqKTh5aSRR8NpZvd4Wz+sAFklLS6NSoFuM6udO3ZV1dX0QrEcyRPIJFJEApdR8wGHgO+Fs/MNe0ortyI4kf/znLd7sjiIy9Rb1qjozp4MaD7d2oXbX4azhoWrqckkdB/rRJfz4yCFien1oeSqmFSqkopVRopn3/VUqFKaUOKKV+NS2wiFLKQyl1SykVbHrNz+GaNZVSG5RSx01fnQrwO2haiVSzsgNTejbm75d68eW4tjStW4X/bThG5/c2MuOH/QRFXKG8dDFrpUNBWh7vAcMxuq3aAzWA1SLSIZdzugPxwLci4mPa1w/YJCIpSqn3AUTkZaWUh+l6PnnE8R/gioi8p5SaCTiJyMt5xa9bHlppcyo6niW7I1gReI7riSm0dKnGuE7uDAuoT0UHPWdEKx5mWZ5EKVUTiBORVKVUZaCqiFw0/ayviGzI5hwPckgKpi6wUSIypgDJ4yjQU0QuKKVcgC35WdlXJw+ttLqRmMLK4EiW7Iog7OJ1qlawY3ir+jzYvqFeBkWzOIuvbaWU2icirbPZ70HOyeN34EcR+c503CHgGHANeE1EtmVzTqyIpHd1KeBq+nZudPLQSjsR4Z/wq/yw9wx/HDQesPs3qM6D7d0Y4u+qy+ZqFlEcyWO/iLTKZr8H2SQPpdSrQFtghIiIqS56FRGJUUq1AVYC3iJyLct5sZmThVLqqohk+9xDKTUJmATg5ubWJiIioki/o6aVFHE3k/l1/zl+2HuWo5euU9nBlqEBrjzc3h3fBro1oplPiWp5KKUmAE8CfUTkZg7X2wK8ICKBWfbrbitNMxER9p+N5Yc9Z/j9wHkSktPwrV+dhzu4MdTflcq6NaIVkTlGW5krkAHAS8DQzIlDKVVbKWVr+r4R0BQ4lc0lVgHjTd+PB36zbMSaVnIppWjt5sR/R/uz99V7eGuYN8mpabzyy0Hav/MX//frQUIj8xwYqWkFZs6Wxy8iMiLLvh+AnoAzcAl4A3gFqADEmA7bLSKTlVIjgbeAZCANeENEfjdd5ytgvogEKqVqAT8BbkAEcL+IXMkrPt3y0MoLEWHfmVi+33OG1QfOk2h6NvJwB+PZSCUH3RrR8s9co606Ax5kWhNLRL41R4CWppOHVh7F3Uzml/3n+H7PGY5HxVPZwZYBPi6MbF2fjo1qYWOjF2bUcmeOGeZLgMZAMJBq2i0iMsNsUVqQTh5aeSYiBEZcZUXgOdYcvMD1xBQaOFVkQmcPRrRuQM3KDtYOUSuhzJE8jgAtpZROc9XJQ9MMCcmprD98ie92RbA3/Ap2NoquTZ0Z4ufKQF8XPQFRu4M5ksdyYIaIXDB3cMVBJw9Nu9uRC9dYFXKe30POc+7qLZyrODC2owe9vGrj41pdd2tpZkkem4EAYC+QmL5fRIaaK0hL0slD03ImIuw5fYV5m0+w7fhlAOrXqMjI1vV5sL2ufliemSN59Mhuv4hsLWJsxUInD03Ln6jrCWw7dpnfQs6z/Xg0SinuaVGHsR096NxYP2Qvbyw+SbCk08lD0wru7JWbfL/XqH549WYyDZwqMsTflb4t69KqYQ1dRrccMEfLoyMwF2gBOAC2wA0RqWbOQC1FJw9NK7yE5FTWHbrIiqBz7DoZQ0qa0Mi5MiPbNGBk6wbUq+5o7RA1CzFH8ggEHgSWY6xJNQ5oJiKvmDNQS9HJQ9PMI+5WMusPXWR50Dn2nr6CUhDQsAZ9vOrQt2U9mtfTpXTLErMkDxFpq5Q6ICJ+pn3ZLoZYEunkoWnmF375BqtCzrPxyCVCzhnLoHRuXIupvZrQuXEt3a1VBpgjefwN3AN8BVwELgATdBlaTdMAoq4l8FvweRZsO0X09UQa1a7MYD9XBvrWw6teqejd1rJhjuThjrE+lQPwLFAd+ExETpgzUEvRyUPTikdCcip/HLjAj/+cJTDiCmkCrdxq0K9lPbo3c6alSzXdIilFzLW2VUXATUSOmjO44qCTh6YVv5j4RFYGn2d54FnCLl4HoEmdKtzXqj5D/V1pWLOSlSPU8mKOlscQYA7gICKeSqkA4C09SVDTtPyIvp7IhsOXWBkcyd7TxkLY7TycGBZQn0G+Ljjp9bVKJHMkjyCgN0bxpVamfQdFxNeskVqITh6aVnKcu3qT34LPs3J/JMej4rGzUXRr6sxgP1f6etelmqO9tUPUTMyRPHaLSMfMI6wyj7wq6XTy0LSSR0Q4fOEaq4LPs/rABSJjb1HBzoZBfi6MbtOQ9p41sdUz2q0qp+RRkKowh5RSDwO2SqmmwAxgp7kC1DSt/FFK4e1aHW/X6sy814v9Z2P5OegcK/dH8su+SJyrVGCAT10G+rrQwbOWTiQlSEFaHpWAV4F+pl3rgLdFJDHns0oO3fLQtNLjZlIKm8KiWHPwApvCokhITsO5igP9vesxyE8nkuJkjm6rthjJw4PbLRbR3VaaplnSzaQUNodFZySSW8mptxOJrwvtPWtiZ2tjGzWKmgAADh9JREFU7TDLLHMkj6PAC0AoRo1xAEQkwlxBWpJOHppW+t1MSmHL0Wj+OHA7kdSs7EB/77rc6+NCp8a1sNeJxKzMkTy2i0jXAt50ITAYiBIRH9O+/wJDgCTgJDBRRGKVUn2B9zAmISYBL4rIpmyuOQt4Aog27fo/EVmTVyw6eWha2XIrKZWtx6L44+BFNh25xI2kVKpXtKdfS+MZSZcmzjjY6URSVOZIHn2Ah4CN3FkM6pdczun+/+3de3AV53nH8e+DQAJzB10QxpirEQgwpm4cpxAbsDGQOtRJprEn01zsGceZpE0mbWN73Gk90+YPJ03acZLW49xvtd1LEjMxmItNIZOCHcBcJHG/GEMkIQSywRChy9M/9pVZxDmgg7TnHMHvM3NGe96zZ/fRe1b76N3d8yxwGvhJLHksBF5191Yzeyos41EzuwWod/ffm9l0YKW7X59imU8Cp939n7sUeKDkIXL1+kNLG+v3NLCiqo41NfWcam5lcP++3D2tjCXTy5kzuZj+/XR73SvRE1dbfQaoAPpx/rCVA2mTh7uvN7NxndpWxZ5uBD4W2t+ItVcDA8ysqLeckBeR3Onfr4CFlaNYWDmK5tY2frvvOMt31LGquo5fbDnKoKK+LJhayuLp5dw5pUSJpAdkkjz+2N2n9PD6HwReSNH+UWDLJRLHF8zsk8Am4K/d/WSqmczsYeBhgLFjx/ZAuCKS74r6FjC/ooz5FWWcu28GGw40smJHLSur63hx6++5rrCAeRWlLJlezryKEq4rzGQ3KB0yOWz1Q+Dr7l6T0QqikcevOw5bxdqfILovyEc8FoSZVQLLgIXuvj/F8sqA40Sjnn8Eyt39wcvFocNWIte21rZ2Nh44wfKqWlZW1dH47jn69+vDvCmlLJ5RzvyKUgYVKZF01hPnPHYCE4GDROc8jC5cqpsqeZjZp4HPAgvc/UysfQzwKtFJ9N92IaaLlp2OkoeIdGhrd14/eIIVVbWsqKqj4VQzhX37cMdNJSyePooFFWUMvU4lUqBnznks6qFAFgFfAe7olDiGAS8Bj10qcZhZubvXhqf3EV06LCLSZQV9jNsnjuT2iSN58t5KNh8+yfIdtazYUcfqmnoK+hi3jR/B3dPKuHtaGWOGq/pvZxmVZM944WbPAXcCxUT3AvkH4HGgCGgMs21090fM7O/Ca3tji1jo7sfM7HvAM+6+ycx+CswiOmx1CPhsLJmkpZGHiFxOe7uz7UgTq2vqWVVTz75jpwGYOWYoS2ddz4KKUm4ced01dT+SHrmfR2+m5CEimTp4/F1WVdfx6+217Dga3WZ3zPABzJ1cwl1TS/mTSVf/JcBKHkoeItINh46/y2/2NrB+73E27G/kdHMrAwsLuLOilHsqRzFvSgmDr8JS8j1xzkNE5Jo1rngg44oH8he3j6O5tY0N+xtZWV3P6pp6XtpeS2FBHz4waST3VI7i7mllFA8qynXIidLIQ0SkG9ranS2HT7Kyqo6VNXW8deIsfQxuvXEECyujE+5jR/Te8yQ6bKXkISIJc3d21p5iZXUdK6vr3rtve/GgIuZOLmbJjHLm9rJSKUoeSh4ikmWHG8+wbs8xthxuYu3uYzSdaXmvVMqSGeXccVP+l0pR8lDyEJEcamlrZ8P+RpaHUiknz7QwsLCA+VPLWFQ5ipljhnL9sAH0ybObXCl5KHmISJ5oaWvntQMneCkkkhPvngOiw1sLKkq5a1oZcyYVM6Aw96MSJQ8lDxHJQ61t7Ww78ja76t5hw/5G1u1u4FRzK0V9+zB3cjF3TS1j/tRSSgf3z0l8Sh5KHiLSC5xrbef1gydYszO6DPho01kAZt0wjAUVpcyrKKVy9JCsXb2l5KHkISK9jLuzq+4Ua2rqWbOznm1Hom+5lw4uYt6UUuZVlDBnckmi1YCVPJQ8RKSXazjVzLo9DazddYz1e6LDW/0KjKnlQ7h5zDCWzCjntvEjevSku5KHkoeIXEVa2trZ/OZJ/nd3AzuONvHG4SbOnGtj1JD+fGhmObdPGMkf3Tic4QMLu7UeJQ8lDxG5ip0918aanfW8uPUo6/Y00NIW7dsnlgzkWw/MZtroIVe0XNW2EhG5ig0oLODem0dz782j+UNLG9veamLTmyfZ/OZJRg3t+Su1lDxERK4y/fsVcNuEkdw2YWRi6+iT2JJFROSqpeQhIiIZU/IQEZGMJZo8zOwHZnbMzKpibV83s11mtt3Mfmlmw2KvPW5m+8xst5ndk2aZ483stTDfC2bWvevQREQkY0mPPH4ELOrUthqY7u4zgT3A4wBmNg24H6gM7/k3M0tVFewp4F/cfRJwEngomdBFRCSdRJOHu68HTnRqW+XureHpRmBMmF4KPO/uze5+ENgHvC/+XouKucwH/js0/Rj4s4TCFxGRNHJ9zuNBYEWYvh54K/bakdAWNxJoiiWfVPOIiEjCcpY8zOwJoBX4eYLreNjMNpnZpoaGhqRWIyJyzcnJlwTN7NPAnwIL/Hx9lKPADbHZxoS2uEZgmJn1DaOPVPO8x92fBZ4N62wwszevMORi4PgVvjdJ+RoX5G9siisziitz+RrblcZ1Y6rGrCcPM1sEfAW4w93PxF5aBvyHmX0TGA1MBl6Pv9fd3czWAh8Dngc+BbzYlfW6e0k3Yt6UqrZLruVrXJC/sSmuzCiuzOVrbD0dV9KX6j4HbACmmNkRM3sI+DYwGFhtZlvN7BkAd68G/hOoAV4GPu/ubWE5y81sdFjso8CXzWwf0TmQ7yf5O4iIyMUSHXm4+wMpmtPu7N39q8BXU7QviU0foNNVWCIikl25vtqqt3g21wGkka9xQf7Gprgyo7gyl6+x9Whc18z9PEREpOdo5CEiIhlT8hARkYwpeVyGmS0KhRr3mdljOYzjBjNba2Y1ZlZtZl8M7U+a2dFw5dpWM1tyuWUlENshM9sR1r8ptI0ws9Vmtjf8HJ7lmKbE+mSrmb1jZl/KVX+lKRKaso8s8nTY5rab2ewsx5WyeKmZjTOzs7G+eybLcaX97LpSVDXBuF6IxXTIzLaG9mz2V7r9Q3LbmLvrkeYBFAD7gQlAIbANmJajWMqB2WF6MFFRyWnAk8Df5LifDgHFndq+BjwWph8Dnsrx51hH9GWnnPQX8EFgNlB1uT4ClhCV7THg/cBrWY5rIdA3TD8Vi2tcfL4c9FfKzy78HWwDioDx4W+2IFtxdXr9G8Df56C/0u0fEtvGNPK4tPcB+9z9gLufI/pi4tJcBOLute6+JUyfAnaS33W9lhIVroTcF7BcAOx39yutMNBtnqJIKOn7aCnwE49sJKqqUJ6tuDx98dKsSdNf6Vy2qGo24jIzA/4ceC6JdV/KJfYPiW1jSh6X1pVijVlnZuOAW4DXQtMXwtDzB9k+PBQ4sMrMNpvZw6GtzN1rw3QdUJaDuDrcz4V/0Lnurw7p+iiftrt48VKA8Wb2hpmtM7O5OYgn1WeXL/01F6h3972xtqz3V6f9Q2LbmJJHL2Nmg4D/Ab7k7u8A/w5MBGYBtUTD5myb4+6zgcXA583sg/EXPRon5+SacItuFvZh4L9CUz7010Vy2Ufp2MXFS2uBse5+C/BlonJCQ7IYUl5+djEPcOE/KVnvrxT7h/f09Dam5HFpXSnWmDVm1o9ow/i5u/8CwN3r3b3N3duB75KDb9+7+9Hw8xjwyxBDfccwOPw8lu24gsXAFnevDzHmvL9i0vVRzrc7O1+89BNhp0M4LNQYpjcTnVu4KVsxXeKzy4f+6gt8BHihoy3b/ZVq/0CC25iSx6X9Dphs0a1vC4kOfyzLRSDheOr3gZ3u/s1Ye/w45X1AVef3JhzXQDMb3DFNdLK1iqifPhVm63IBywRc8N9grvurk3R9tAz4ZLgi5v3A27FDD4mz88VLP+yx4qVmVmLh7p5mNoGoeOmBLMaV7rNbBtxvZkVmNp4URVWz4C5gl7sf6WjIZn+l2z+Q5DaWjSsBevOD6KqEPUT/NTyRwzjmEA05twNbw2MJ8FNgR2hfBpRnOa4JRFe6bAOqO/qIqGjlK8BeYA0wIgd9NpCojP/QWFtO+osogdUCLUTHlx9K10dEV8B8J2xzO4BbsxzXPqLj4R3b2TNh3o+Gz3grsAW4N8txpf3sgCdCf+0GFmczrtD+I+CRTvNms7/S7R8S28ZUnkRERDKmw1YiIpIxJQ8REcmYkoeIiGRMyUNERDKm5CEiIhlT8hDpBjNrswur9/ZY5eVQlTWX30MRSSvRe5iLXAPOuvusXAchkm0aeYgkINzX4WsW3efkdTObFNrHmdmrobjfK2Y2NrSXWXTvjG3h8YGwqAIz+264R8MqMxsQ5v+rcO+G7Wb2fI5+TbmGKXmIdM+AToetPh577W13nwF8G/jX0PYt4MfuPpOo4ODTof1pYJ2730x0v4jq0D4Z+I67VwJNRN9ahujeDLeE5TyS1C8nko6+YS7SDWZ22t0HpWg/BMx39wOhYF2du480s+NEZTVaQnutuxebWQMwxt2bY8sYB6x298nh+aNAP3f/JzN7GTgN/Ar4lbufTvhXFbmARh4iyfE005lojk23cf485YeIahPNBn4XqrqKZI2Sh0hyPh77uSFM/x9RdWaATwC/CdOvAJ8DMLMCMxuabqFm1ge4wd3XAo8CQ4GLRj8iSdJ/KyLdM8DMtsaev+zuHZfrDjez7USjhwdC218CPzSzvwUagM+E9i8Cz5rZQ0QjjM8RVW9NpQD4WUgwBjzt7k099huJdIHOeYgkIJzzuNXdj+c6FpEk6LCViIhkTCMPERHJmEYeIiKSMSUPERHJmJKHiIhkTMlDREQypuQhIiIZ+38GHWSpbLHkCAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "def plot_history(history, key):\n",
        "  plt.plot(history.history[key])\n",
        "  plt.plot(history.history['val_'+key])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(key)\n",
        "  plt.legend([key, 'val_'+key])\n",
        "  plt.show()\n",
        "# Plot the history\n",
        "plot_history(history, 'mean_squared_logarithmic_error')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2u4xmebETBm"
      },
      "source": [
        "**5.Initially you can try with only 2-3 layers ANN model and later try with around 30 layers to see its effect on performance.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14LTMnC9XC_s"
      },
      "outputs": [],
      "source": [
        "# Defining model for classification\n",
        "model5 = Sequential()\n",
        "model5.add(Dense(activation = \"relu\", input_dim = 13, units = 10, kernel_initializer = \"uniform\"))\n",
        "model5.add(Dense(activation = \"relu\", units = 9, kernel_initializer = \"uniform\"))\n",
        "model5.add(Dense(activation = \"relu\", units = 9, kernel_initializer = \"uniform\"))\n",
        "model5.add(Dense(activation = \"relu\", units = 9, kernel_initializer = \"uniform\"))\n",
        "model5.add(Dense(activation = \"relu\", units = 9, kernel_initializer = \"uniform\"))\n",
        "model5.add(Dense(activation = \"relu\", units = 9, kernel_initializer = \"uniform\"))\n",
        "model5.add(Dense(activation = \"relu\", units = 9, kernel_initializer = \"uniform\"))\n",
        "model5.add(Dense(activation = \"relu\", units = 9, kernel_initializer = \"uniform\"))\n",
        "model5.add(Dense(activation = \"relu\", units = 9, kernel_initializer = \"uniform\"))\n",
        "model5.add(Dense(activation = \"relu\", units = 9, kernel_initializer = \"uniform\"))\n",
        "model5.add(Dense(activation = \"relu\", units = 9, kernel_initializer = \"uniform\"))\n",
        "model5.add(Dense(activation = \"relu\", units = 9, kernel_initializer = \"uniform\"))\n",
        "model5.add(Dense(activation = \"relu\", units = 9, kernel_initializer = \"uniform\"))\n",
        "model5.add(Dense(activation = \"relu\", units = 9, kernel_initializer = \"uniform\"))\n",
        "model5.add(Dense(activation = \"relu\", units = 9, kernel_initializer = \"uniform\"))\n",
        "model5.add(Dense(activation = \"relu\", units = 9, kernel_initializer = \"uniform\"))\n",
        "model5.add(Dense(activation = \"relu\", units = 9, kernel_initializer = \"uniform\"))\n",
        "model5.add(Dense(activation = \"relu\", units = 9, kernel_initializer = \"uniform\"))\n",
        "model5.add(Dense(activation = \"relu\", units = 9, kernel_initializer = \"uniform\"))\n",
        "model5.add(Dense(activation = \"relu\", units = 9, kernel_initializer = \"uniform\"))\n",
        "model5.add(Dense(activation = \"relu\", units = 9, kernel_initializer = \"uniform\"))\n",
        "model5.add(Dense(activation = \"relu\", units = 9, kernel_initializer = \"uniform\"))\n",
        "model5.add(Dense(activation = \"relu\", units = 9, kernel_initializer = \"uniform\"))\n",
        "model5.add(Dense(activation = \"relu\", units = 9, kernel_initializer = \"uniform\"))\n",
        "model5.add(Dense(activation = \"relu\", units = 9, kernel_initializer = \"uniform\"))\n",
        "model5.add(Dense(activation = \"relu\", units = 9, kernel_initializer = \"uniform\"))\n",
        "model5.add(Dense(activation = \"relu\", units = 9, kernel_initializer = \"uniform\"))\n",
        "model5.add(Dense(activation = \"relu\", units = 9, kernel_initializer = \"uniform\"))\n",
        "model5.add(Dense(activation = \"relu\", units = 9, kernel_initializer = \"uniform\"))\n",
        "model5.add(Dense(activation = \"relu\", units = 9, kernel_initializer = \"uniform\"))\n",
        "model5.add(Dense(activation = \"softmax\", units = 2, kernel_initializer = \"uniform\"))\n",
        "model5.compile(optimizer = 'adam' , loss = 'categorical_crossentropy', \n",
        "                   metrics = ['accuracy'] )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model5.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58dfee0c-00c1-4cba-e88d-ad9360e7e6c2",
        "id": "z52BVCXzXC_u"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_44 (Dense)            (None, 10)                140       \n",
            "                                                                 \n",
            " dense_45 (Dense)            (None, 9)                 99        \n",
            "                                                                 \n",
            " dense_46 (Dense)            (None, 9)                 90        \n",
            "                                                                 \n",
            " dense_47 (Dense)            (None, 9)                 90        \n",
            "                                                                 \n",
            " dense_48 (Dense)            (None, 9)                 90        \n",
            "                                                                 \n",
            " dense_49 (Dense)            (None, 9)                 90        \n",
            "                                                                 \n",
            " dense_50 (Dense)            (None, 9)                 90        \n",
            "                                                                 \n",
            " dense_51 (Dense)            (None, 9)                 90        \n",
            "                                                                 \n",
            " dense_52 (Dense)            (None, 9)                 90        \n",
            "                                                                 \n",
            " dense_53 (Dense)            (None, 9)                 90        \n",
            "                                                                 \n",
            " dense_54 (Dense)            (None, 9)                 90        \n",
            "                                                                 \n",
            " dense_55 (Dense)            (None, 9)                 90        \n",
            "                                                                 \n",
            " dense_56 (Dense)            (None, 9)                 90        \n",
            "                                                                 \n",
            " dense_57 (Dense)            (None, 9)                 90        \n",
            "                                                                 \n",
            " dense_58 (Dense)            (None, 9)                 90        \n",
            "                                                                 \n",
            " dense_59 (Dense)            (None, 9)                 90        \n",
            "                                                                 \n",
            " dense_60 (Dense)            (None, 9)                 90        \n",
            "                                                                 \n",
            " dense_61 (Dense)            (None, 9)                 90        \n",
            "                                                                 \n",
            " dense_62 (Dense)            (None, 9)                 90        \n",
            "                                                                 \n",
            " dense_63 (Dense)            (None, 9)                 90        \n",
            "                                                                 \n",
            " dense_64 (Dense)            (None, 9)                 90        \n",
            "                                                                 \n",
            " dense_65 (Dense)            (None, 9)                 90        \n",
            "                                                                 \n",
            " dense_66 (Dense)            (None, 9)                 90        \n",
            "                                                                 \n",
            " dense_67 (Dense)            (None, 9)                 90        \n",
            "                                                                 \n",
            " dense_68 (Dense)            (None, 9)                 90        \n",
            "                                                                 \n",
            " dense_69 (Dense)            (None, 9)                 90        \n",
            "                                                                 \n",
            " dense_70 (Dense)            (None, 9)                 90        \n",
            "                                                                 \n",
            " dense_71 (Dense)            (None, 9)                 90        \n",
            "                                                                 \n",
            " dense_72 (Dense)            (None, 9)                 90        \n",
            "                                                                 \n",
            " dense_73 (Dense)            (None, 9)                 90        \n",
            "                                                                 \n",
            " dense_74 (Dense)            (None, 2)                 20        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,779\n",
            "Trainable params: 2,779\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34df646e-c2da-45d2-a722-9b80bc1bd968",
        "id": "uw9zmLvKXC_u"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "25/25 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5492\n",
            "Epoch 2/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6918 - accuracy: 0.5596\n",
            "Epoch 3/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6908 - accuracy: 0.5596\n",
            "Epoch 4/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6901 - accuracy: 0.5596\n",
            "Epoch 5/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6893 - accuracy: 0.5596\n",
            "Epoch 6/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6893 - accuracy: 0.5596\n",
            "Epoch 7/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6885 - accuracy: 0.5596\n",
            "Epoch 8/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6881 - accuracy: 0.5596\n",
            "Epoch 9/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6878 - accuracy: 0.5596\n",
            "Epoch 10/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6873 - accuracy: 0.5596\n",
            "Epoch 11/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6872 - accuracy: 0.5596\n",
            "Epoch 12/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6868 - accuracy: 0.5596\n",
            "Epoch 13/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6865 - accuracy: 0.5596\n",
            "Epoch 14/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6865 - accuracy: 0.5596\n",
            "Epoch 15/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6867 - accuracy: 0.5596\n",
            "Epoch 16/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6864 - accuracy: 0.5596\n",
            "Epoch 17/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6863 - accuracy: 0.5596\n",
            "Epoch 18/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6867 - accuracy: 0.5596\n",
            "Epoch 19/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 20/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6863 - accuracy: 0.5596\n",
            "Epoch 21/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6863 - accuracy: 0.5596\n",
            "Epoch 22/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 23/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6860 - accuracy: 0.5596\n",
            "Epoch 24/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 25/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 26/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6863 - accuracy: 0.5596\n",
            "Epoch 27/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6860 - accuracy: 0.5596\n",
            "Epoch 28/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 29/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 30/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 31/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6865 - accuracy: 0.5596\n",
            "Epoch 32/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 33/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 34/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 35/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 36/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6864 - accuracy: 0.5596\n",
            "Epoch 37/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6860 - accuracy: 0.5596\n",
            "Epoch 38/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 39/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 40/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 41/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6860 - accuracy: 0.5596\n",
            "Epoch 42/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 43/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6866 - accuracy: 0.5596\n",
            "Epoch 44/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 45/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6865 - accuracy: 0.5596\n",
            "Epoch 46/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 47/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6864 - accuracy: 0.5596\n",
            "Epoch 48/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6863 - accuracy: 0.5596\n",
            "Epoch 49/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 50/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6863 - accuracy: 0.5596\n",
            "Epoch 51/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 52/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 53/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6863 - accuracy: 0.5596\n",
            "Epoch 54/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 55/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 56/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 57/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6864 - accuracy: 0.5596\n",
            "Epoch 58/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 59/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6865 - accuracy: 0.5596\n",
            "Epoch 60/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 61/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 62/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6863 - accuracy: 0.5596\n",
            "Epoch 63/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 64/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6864 - accuracy: 0.5596\n",
            "Epoch 65/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6865 - accuracy: 0.5596\n",
            "Epoch 66/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6864 - accuracy: 0.5596\n",
            "Epoch 67/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6869 - accuracy: 0.5596\n",
            "Epoch 68/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6872 - accuracy: 0.5596\n",
            "Epoch 69/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 70/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6865 - accuracy: 0.5596\n",
            "Epoch 71/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6865 - accuracy: 0.5596\n",
            "Epoch 72/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 73/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 74/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6863 - accuracy: 0.5596\n",
            "Epoch 75/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6864 - accuracy: 0.5596\n",
            "Epoch 76/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6863 - accuracy: 0.5596\n",
            "Epoch 77/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6863 - accuracy: 0.5596\n",
            "Epoch 78/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6860 - accuracy: 0.5596\n",
            "Epoch 79/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 80/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 81/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 82/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 83/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6863 - accuracy: 0.5596\n",
            "Epoch 84/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 85/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 86/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 87/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6860 - accuracy: 0.5596\n",
            "Epoch 88/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 89/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 90/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 91/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 92/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 93/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 94/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 95/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 96/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 97/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 98/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 99/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 100/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 101/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 102/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 103/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6863 - accuracy: 0.5596\n",
            "Epoch 104/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 105/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6864 - accuracy: 0.5596\n",
            "Epoch 106/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6863 - accuracy: 0.5596\n",
            "Epoch 107/200\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.6864 - accuracy: 0.5596\n",
            "Epoch 108/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 109/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 110/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 111/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6863 - accuracy: 0.5596\n",
            "Epoch 112/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 113/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6863 - accuracy: 0.5596\n",
            "Epoch 114/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 115/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 116/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 117/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 118/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 119/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6863 - accuracy: 0.5596\n",
            "Epoch 120/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6863 - accuracy: 0.5596\n",
            "Epoch 121/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 122/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 123/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 124/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6863 - accuracy: 0.5596\n",
            "Epoch 125/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 126/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 127/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6866 - accuracy: 0.5596\n",
            "Epoch 128/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6864 - accuracy: 0.5596\n",
            "Epoch 129/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 130/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6863 - accuracy: 0.5596\n",
            "Epoch 131/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 132/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 133/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 134/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 135/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 136/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 137/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 138/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 139/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 140/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 141/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 142/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 143/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 144/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 145/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 146/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6863 - accuracy: 0.5596\n",
            "Epoch 147/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6863 - accuracy: 0.5596\n",
            "Epoch 148/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 149/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 150/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 151/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 152/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 153/200\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 154/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6860 - accuracy: 0.5596\n",
            "Epoch 155/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 156/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 157/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6864 - accuracy: 0.5596\n",
            "Epoch 158/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 159/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6863 - accuracy: 0.5596\n",
            "Epoch 160/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 161/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 162/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 163/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6868 - accuracy: 0.5596\n",
            "Epoch 164/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 165/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6863 - accuracy: 0.5596\n",
            "Epoch 166/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 167/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 168/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6866 - accuracy: 0.5596\n",
            "Epoch 169/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6860 - accuracy: 0.5596\n",
            "Epoch 170/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 171/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 172/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6860 - accuracy: 0.5596\n",
            "Epoch 173/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 174/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6864 - accuracy: 0.5596\n",
            "Epoch 175/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6863 - accuracy: 0.5596\n",
            "Epoch 176/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 177/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6863 - accuracy: 0.5596\n",
            "Epoch 178/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 179/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 180/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 181/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 182/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 183/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 184/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6866 - accuracy: 0.5596\n",
            "Epoch 185/200\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.6864 - accuracy: 0.5596\n",
            "Epoch 186/200\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 187/200\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.6863 - accuracy: 0.5596\n",
            "Epoch 188/200\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.6863 - accuracy: 0.5596\n",
            "Epoch 189/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 190/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 191/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 192/200\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.6864 - accuracy: 0.5596\n",
            "Epoch 193/200\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 194/200\n",
            "25/25 [==============================] - 0s 15ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 195/200\n",
            "25/25 [==============================] - 0s 14ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 196/200\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.6861 - accuracy: 0.5596\n",
            "Epoch 197/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n",
            "Epoch 198/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6860 - accuracy: 0.5596\n",
            "Epoch 199/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6863 - accuracy: 0.5596\n",
            "Epoch 200/200\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5596\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7883b6a190>"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "model5.fit(X_train_scaled1 , Y_train1 , batch_size = 8 ,epochs = 200 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08a29c00-d29d-4b03-abe6-c43dcf224de1",
        "id": "sVr_HdAKXC_v"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on training data: 55.95855116844177% \n",
            "Error on training data: 44.04144883155823\n"
          ]
        }
      ],
      "source": [
        "# prediction on scalled train data\n",
        "pred_train1= model5.predict(X_train_scaled1)\n",
        "scores = model5.evaluate(X_train_scaled1, Y_train1, verbose=0)\n",
        "print('Accuracy on training data: {}% \\nError on training data: {}'.format(scores[1]*100, (1 - scores[1])*100))   \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction on scalled test data\n",
        "pred_test1= model1.predict(x_test_scaled_all1)\n",
        "\n",
        "scores1 = model1.evaluate(x_test_scaled_all1, y_test1, verbose=0)\n",
        "print('Accuracy on test data: {}% \\nError on test data: {}'.format(scores1[1]*100, (1 - scores1[1])*100))    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2018eb7e-8799-4ce2-99fa-072a88dee7fd",
        "id": "qDVFk7WNXC_w"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test data: 57.37704634666443% \n",
            "Error on test data: 42.62295365333557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting predictions to label(reversing onehot encoding operation)\n",
        "pred1 = list()\n",
        "for i in range(len(pred_test1)):\n",
        "    pred1.append(np.argmax(pred_test1[i])) # returns index of maximum value"
      ],
      "metadata": {
        "id": "OH2K6nKdXC_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting predictions to label(reversing onehot encoding operation) \n",
        "test1 = list()\n",
        "for i in range(len(y_test1)):\n",
        "    test1.append(np.argmax(y_test1[i])) # returns index of maximum value\n"
      ],
      "metadata": {
        "id": "lxPPUYw9XC_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#comparing test and prediction\n",
        "pd.DataFrame({'Test':test1,'Pred':pred1}).head(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "outputId": "0e2413e9-ebbf-43de-c776-9f72a86aed63",
        "id": "qU5KEbzAXC_y"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Test  Pred\n",
              "0      0     1\n",
              "1      1     0\n",
              "2      1     0\n",
              "3      0     1\n",
              "4      1     1\n",
              "5      1     0\n",
              "6      0     1\n",
              "7      0     1\n",
              "8      1     0\n",
              "9      1     1\n",
              "10     0     0\n",
              "11     1     1\n",
              "12     0     0\n",
              "13     0     0\n",
              "14     0     0\n",
              "15     1     0\n",
              "16     0     1\n",
              "17     0     1\n",
              "18     1     0\n",
              "19     1     1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-88138140-0403-449e-a606-1e9c63ed3956\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Test</th>\n",
              "      <th>Pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88138140-0403-449e-a606-1e9c63ed3956')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-88138140-0403-449e-a606-1e9c63ed3956 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-88138140-0403-449e-a606-1e9c63ed3956');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DL_Assignment 3_202111010.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}